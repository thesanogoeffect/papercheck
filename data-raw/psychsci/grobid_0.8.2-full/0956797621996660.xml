<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Search for the Unknown: Guidance of Visual Search in the Absence of an Active Template</title>
				<funder ref="#_tYsVZTm">
					<orgName type="full">Israel Science Foundation</orgName>
				</funder>
				<funder ref="#_xYgMgHG">
					<orgName type="full">German Israel Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oryah</forename><forename type="middle">C</forename><surname>Lancry-Dayan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Gamer</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Würzburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoni</forename><surname>Pertzov</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Search for the Unknown: Guidance of Visual Search in the Absence of an Active Template</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BFA537B36D2CD74128B69355866A528E</idno>
					<idno type="DOI">10.1177/0956797621996660</idno>
					<note type="submission">Received 6/22/20; Revision accepted 1/15/21</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-06-03T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>eye movements</term>
					<term>memory</term>
					<term>visual search</term>
					<term>template</term>
					<term>open data</term>
					<term>open materials</term>
					<term>preregistered</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Can you efficiently look for something even without knowing what it looks like? According to theories of visual search, the answer is no: A template of the search target must be maintained in an active state to guide search for potential locations of the target. Here, we tested the need for an active template by assessing a case in which this template is improbable: the search for a familiar face among unfamiliar ones when the identity of the target face is unknown. Because people are familiar with hundreds of faces, an active guiding template seems unlikely in this case. Nevertheless, participants (35 Israelis and 33 Germans) were able to guide their search as long as extrafoveal processing of the target features was possible. These results challenge current theories of visual search by showing that guidance can rely on long-term memory and extrafoveal processing rather than on an active search template.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We spend approximately 10 min a day searching for things, which adds up to roughly 153 days of our lifetime <ref type="bibr" target="#b27">(Miller, 2012)</ref>. Since visual search is such a central task, it comes as no surprise that the neurocognitive research community has been investing substantial effort to understanding the mechanisms that enable it to function efficiently. Emerging from the seminal ideas of feature-integration theory <ref type="bibr" target="#b38">(Treisman &amp; Gelade, 1980)</ref> and guided search <ref type="bibr" target="#b43">(Wolfe et al., 1989)</ref>, current theoretical views are predicated on the need for a guiding template <ref type="bibr" target="#b42">(Wolfe, 2020)</ref> for visual search to be efficient. As an active representation of the properties of the search target, the guiding template resides in visual working memory and guides search by being compared with the visual input and directing search to the probable whereabouts of the target. Indeed, studies have shown that gaze is attracted toward locations that share the target's low-level characteristics <ref type="bibr" target="#b0">( Alexander et al., 2019;</ref><ref type="bibr" target="#b11">Findlay, 1997;</ref><ref type="bibr" target="#b22">Luria &amp; Strauss, 1975;</ref><ref type="bibr" target="#b28">Motter &amp; Belky, 1998)</ref>, even if the target is not present <ref type="bibr" target="#b36">( Tavassoli et al., 2009)</ref>. Search can be efficiently executed even when the guiding template is specified by an altered depiction of the target <ref type="bibr" target="#b4">(Bravo &amp; Farid, 2009;</ref><ref type="bibr" target="#b18">Hout &amp; Goldinger, 2015;</ref><ref type="bibr" target="#b39">Vickery et al., 2005)</ref>, a name cue <ref type="bibr" target="#b24">( Malcolm &amp; Henderson, 2009;</ref><ref type="bibr" target="#b33">Spotorno et al., 2014;</ref><ref type="bibr" target="#b44">Yang &amp; Zelinsky, 2009;</ref><ref type="bibr" target="#b47">Zelinsky et al., 2013)</ref>, or the target's function <ref type="bibr" target="#b8">(Castelhano &amp; Witherspoon, 2016;</ref><ref type="bibr" target="#b20">Humphreys &amp; Riddoch, 2001)</ref>. All of these studies suggest that the active template can vary in its level of abstraction but is nevertheless crucial for guiding search toward plausible locations.</p><p>However, one everyday experience completely challenges the notion that an active guiding template is a prerequisite for efficient search. Imagine that you go to a conference and decide to look for someone familiar to sit next to. Suddenly, you notice a colleague whom you did not expect to see. Clearly, you could not have had an active guiding template of this colleague's face in mind because you were not expecting to see them. In addition, because this search involved looking for a familiar face among many other faces, even unspecific templates (such as category-based templates) could not help you distinguish the target face from other face distractors. Therefore, in this type of search, people do not have an active guiding template but, rather, a very large range of representations in long-term memory (LTM) that are target candidates. Given that statistical models <ref type="bibr" target="#b13">(Gelman, 2013;</ref><ref type="bibr" target="#b14">Gurevitch, 1961;</ref><ref type="bibr" target="#b26">McCormick et al., 2010)</ref> estimate that a person knows hundreds of people, it is hard to believe that this massive number of representations in LTM can be combined into one or a handful of active templates-that is, to depict a set of visual properties that are compared with the visual input until the target is found. If an active guidingtemplate mechanism is not probable in this case, what are the cognitive processes underpinning this type of search?</p><p>Obviously, it could be argued that a search for a familiar face can be achieved only through unguided search, in which the person must serially scan the people at the conference until someone familiar is found. Although this is possible, some of us feel that a familiar face can attract our attention and simply "jump" into view. This intuitive feeling suggests that despite not having an active guiding template, search can still be guided by LTM representations. That is, the familiar stimulus could be processed to some extent (whether fully recognized or only eliciting a familiarity signal) and thus direct the search to the whereabouts of the target. This implies that the matching between an active guiding template and the visual input is not the sole way to conduct a guided search. In the current study, we suggest an alternative mechanism to guided search that relies on the ability to process LTM representations through extrafoveal vision. For that purpose, we tracked Israeli and German participants' eye movements during a visual search task in which they were asked to look for the face of a familiar person from among five faces, without knowing whom they were searching for. The findings provide compelling evidence that search for an unexpected familiar face can successfully be guided, thus suggesting that a revision of current theoretical visual search frameworks would be warranted and highlighting the importance of LTM-based extrafoveal processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The experimental procedure was preregistered at <ref type="url" target="https://osf.io/kxpdg">https://osf.io/kxpdg</ref> and <ref type="url" target="https://osf.io/wf5k2">https://osf.io/wf5k2</ref> for the Israeli and German groups, respectively. This study was approved by the Ethics Committee of the Faculty of Social Sciences at The Hebrew University of Jerusalem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Thirty-six and 41 participants were recruited for the Israeli and German groups, respectively. We eliminated participants with more than 50% disqualified trials (see exclusion criteria below), resulting in a final sample of 35 Israelis (10 men; M = 24.8 years) and 33 Germans (six men; M = 28.5 years). Prior to the experiment, we conducted a power analysis based on the null hypothesis for the mean ordinal number in the revealed block (for more details regarding the analysis, see below). This power analysis indicated that to obtain a power of at least .8 for detecting a medium effect size (d = 0.5), approximately 32 participants were needed (for more information, see the preregistrations).</p><p>All participants had normal or corrected-to-normal vision and took part in the experiment in return for course credits or payment (Israelis: 30 shekels; Germans: 7.5 euros). To encourage correct performance, we told participants that they would receive a bonus payment (Israelis: 10 shekels; Germans: 2.5 euros) if they started the search on time, found the familiar face quickly, and correctly recognized which face was the familiar one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statement of Relevance</head><p>Searching for something, whether it is your keys or a familiar face, is a frequent everyday activity. Under some circumstances, such as in security settings, it even carries life-saving implications. Until now, it was widely believed that in order to find what they are looking for, people need to know at least some aspects of what they are trying to find. However, this assumption is inconsistent with common human experiences, such as suddenly finding a friend in a crowd although there was no prior expectation of seeing them. Here, we demonstrate that despite the many people that each person knows, we can all find a familiar face embedded in unfamiliar ones, even without knowing the identity of that face in advance. This calls for a modification of current theories and makes clear that the cognitive system can utilize information from a large area of the visual environment to guide search, even if it is unclear what the search target looks like.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus and stimuli</head><p>The stimuli were displayed on a 24-in. monitor, with a 1,920 × 1,080 pixel screen resolution and a 120-Hz (Israelis) or 144-Hz (Germans) refresh rate. Monocular gaze position was tracked at 1,000 Hz with a towermounted EyeLink 1000+ eye tracker <ref type="bibr">(SR Research, Kanata, Ontario, Canada)</ref>. The distance from the center of the screen to the participants' eyes was 53 cm.</p><p>After collection of face stimuli from Google images, all images were cropped, resized, and transformed to black and white. Then, the images were divided into 100 sets; each set consisted of five faces that included one face of an Israeli celebrity and one face of a German celebrity. We used MATLAB (The MathWorks, Natick, MA) code to make sure that the mean luminance of all the images in each set was equal. In addition, we used Amazon Mechanical Turk (MTurk) questionnaires to ensure that (a) the familiar faces did not differ from the unfamiliar faces in a trait other than familiarity, (b) the familiarity status of the faces was correct (i.e., no familiar face was mistakenly tagged as unfamiliar), and (c) the familiar Israeli and German faces were not universally familiar. For further details on the processing of the stimuli and the MTurk questionnaires, see the Supplemental Material available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design</head><p>Participants completed a visual search task in which they were asked to look for a familiar face among five faces (see Fig. <ref type="figure">1</ref>). The array of faces included one Israeli celebrity, one German celebrity, and three nonfamous faces. The area of each face was scaled to a circle with a radius of 4 degrees of visual angle (DVA). The center of each face was at an equal distance of 9 DVA from the center of the screen. The distance between the center of each pair of adjacent faces and each pair of nonadjacent faces was 10.6 DVA and 17.1 DVA, respectively. Before each trial, a drift correction was carried out, allowing for a deviation of only 2 DVA between the predicted gaze position and the actual fixation point. After the drift-correction procedure, a cross appeared in the center of the screen (for 500 or 1,500 ms), and participants were asked to fixate on the cross and start the search only when it disappeared. In instances in which they shifted their gaze from the cross before it disappeared, an error beep was played and the trial continued as usual (although these trials were later excluded from analysis). Participants were asked to find the target and press the space bar when they were looking at it. If participants pressed the space bar without fixating on the target, a message appeared on Participants were asked to look for a singleton color (color block) or a familiar face (revealed and gaze-contingent blocks). Each search array was displayed around a central cross (left column). Participants were asked to look at the cross and initiate search after it disappeared (middle column). When participants found the target, they were asked to respond by pressing the space bar while fixating on the target (right column). In the color and revealed blocks, participants were exposed to all stimuli throughout the course of the trial; in contrast, in the gaze-contingent block, each face was revealed only when the participant had fixated directly on it. Black bars have been added to the faces to preserve these individuals' privacy (participants saw the unmodified images).</p><p>the screen informing the participants that they had not looked at the target when they responded.</p><p>This task was carried out in two separate blocks. In the revealed block (100 trials), all five faces were visible from the start of the search task. In contrast, in the gaze-contingent block (10 trials), participants saw five circles serving as placeholders for the faces. The face was displayed only when participants shifted their gaze to one of these placeholders. Obviously, in this condition, no stimulus was presented in the periphery of the visual field, and thus no guidance of gaze by familiar faces was expected. Accordingly, our aim in this block was to confirm that the pattern of results in the revealed block was due to the processing of familiar faces by extrafoveal vision. Each face in the revealed condition appeared only once throughout the experiment. The faces in the gaze-contingent block had already appeared in the preceding revealed block.</p><p>To familiarize participants with the task and to provide an example of a task that would be strongly based on extrafoveal vison, we administered a color block prior to the main task. In this block, the five stimuli were not faces but, rather, four circles in one color and a single circle in another color (colors switched across trials). This block was composed of 10 trials that were repeated until participants completed more than 70% of them correctly (i.e., they started the search only after the cross had disappeared and responded only when looking at the target). Beyond this training, participants also engaged in four training trials with faces, prior to the initiation of the revealed block. At the beginning of the experiment (prior to the color block), each participant went through the standard 9-point calibration and validation procedure provided with the eye tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-exclusion criteria</head><p>Trials were excluded when one or more of the following conditions occurred: (a) Participants shifted their gaze before the cross in the center had disappeared, (b) participants pressed the space bar without looking at the familiar face, and (c) eye tracking was of bad quality (more than 20% of the eye samples were missing). This procedure led to the removal of 21% and 36% of the trials in the Israeli and German samples, respectively. In addition, we excluded participants when more than 50% of their trials were disqualified according to these criteria. To ensure that the results are robust and representative even without these strict exclusion criteria, we ran the same analysis without removing any trials. The results of this analysis were qualitatively similar to the results described below and are further discussed in the Supplemental Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The ordinal number</head><p>The spatiotemporal dynamics of gaze behavior during the visual search task provided information on the order in which the faces were scanned during the trial, thus making it possible to assign an ordinal number for each face. If guidance was present, the mean ordinal number of the familiar faces should be lower than (a) the expected value under the null hypothesis, (b) the mean ordinal number of the familiar faces of the other group, and (c) the mean ordinal number of the familiar faces in the gaze-contingent block. Below, we elaborate on each of these comparisons. For further information on the computation of the ordinal number, see the Supplemental Material.</p><p>Under the null hypothesis (i.e., the search could not be guided toward the familiar face), we would expect that the familiar face would not be privileged in terms of scanning order. Thus, each ordinal number of the familiar face would be equally probable, implying that the ordinal number should follow a discrete uniform distribution with a probability of .2 for each possibility and an expected value of 3. However, if participants could guide their search and direct their gaze according to face familiarity, we would expect them to direct their gaze toward the familiar face earlier in the trial, resulting in a mean ordinal number of less than 3. Our analysis showed exactly this pattern, where the mean ordinal number of the familiar face was less than 3 in both groups-Israeli sample: t(34) = -5.26, p &lt; .001, d = 0.89, 95% confidence interval (CI) of d = [0.5, 1.29]; German sample: t(32) = -6.83, p &lt; .001, d = 1.19, 95% CI = [0.75, 1.66] (see Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>Moreover, to control for visual distinctiveness of the famous faces, we compared the ordinal number of the familiar face of one national group with the ordinal number of the familiar face of the other national group. That is, for the Israeli observers, we compared the ordinal number of the Israeli celebrity with the ordinal number of the German celebrity, and vice versa for the German participants. This analysis enabled us to compare the same sets of faces, where familiarity was the only factor differentiating these sets between the groups. The ordinal number of the familiar face was lower than the ordinal number of the familiar face of the other group in both groups-Israeli sample: t p d 34 4 55 001 0 77 95 039 1 16</p><formula xml:id="formula_0">( ) = - &lt; = = . ,</formula><p>. , . , % [ . , . ] CI ; German sample: t(32) = -3.34, p &lt; .001, d = 0.58, 95% CI = [0.21, 0.96] (see Fig. <ref type="figure" target="#fig_1">2</ref>). Finally, we also compared the ordinal number of the familiar face in the revealed block with the ordinal number of the familiar face in the gaze-contingent block. This comparison showed that in the gaze-contingent block, when extrafoveal vision was not available, the ordinal number of the familiar face was higher in comparison with the revealed block for both groups-Israeli sample: t(32) = 2.15, p = .019, d = 0.36, 95% CI = [0.02, 0.71]; German sample: t(32) = 2.08, p = .023, d = 0.36, 95% CI = [0.01, 0.72]. Note that the color block provided a case in which task completion could easily be achieved by extrafoveal vision. Indeed, the mean ordinal number of the target in this block was very close to 1 (1.03 in both samples).</p><p>To further probe what caused the decrease in the ordinal number of the familiar face, we examined the distribution of the ordinal numbers across all participants. This analysis demonstrated that the distribution of the ordinal numbers departed from a uniform distribution in both groups-Israeli sample: χ 2 (4, N = 2,754) = 51.84, p &lt; .001; German sample:</p><formula xml:id="formula_1">χ 2 4 8 0 1 001 , = 2,283 N ( )= &lt; . , . p</formula><p>. The discrepancy from the uniform distribution was mainly caused by the high proportion of trials in which the ordinal number of the familiar face was 1. Consistent with this analysis, results showed that the mean proportion of trials in which the ordinal number of the familiar face was 1 was significantly higher than .2, which is the expected mean proportion under the null hypothesis-Israeli sample: t(34) = 4.54, p &lt; .001, d = 0.77, 95% CI = [0.39, 1.16]; German sample: t(32) = 7.43, p &lt; .001, d = 1.29, 95% CI = [0.84, 1.78] (see Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>Next, we examined whether more time to process the extrafoveal information would improve participants' search performance. To test this assumption, we validated that, as instructed, participants started their search only 500 ms or 1,500 ms after the search array appeared. The results did not show a substantial improvement associated with additional display time before the search began. Participants fixated on the familiar face earlier when they had more time before they were allowed to move their gaze, but this effect was not statistically significant in either group-Israeli sample: t(34) = -1.51, p = .070, d = 0.26, 95% CI = [-0.08, 0.6]; German sample: t(32) = -1.34, p = .095, d = 0.23, 95% CI = [-0.12, 0.59]. Error bars indicate 95% confidence intervals. Asterisks indicate significant differences between face nationalities (top row) and significant differences compared with chance (bottom row; p &lt; .001 for paired-samples t-test comparisons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scanning pattern</head><p>The ordinal-number analysis (see Fig. <ref type="figure" target="#fig_1">2</ref>) indicated a high proportion of trials in which participants fixated on the familiar face before all other faces. To explore whether there was guidance by familiarity when participants did not find the familiar face right away and scanned the array more extensively, we further analyzed the scanning pattern after the first fixation on the faces. First, we characterized the scanning pattern and investigated whether participants scanned the array of faces in a systematic manner (i.e., clockwise or counterclockwise). For that purpose, we analyzed the gaze transitions between faces and scored a transition as 1 if it adhered to a clockwise or counterclockwise scanning pattern (see Fig. <ref type="figure">3a</ref>). Then, we summed these scores separately in two counters for clockwise and counterclockwise patterns. The final scanning score was the maximum value between these two sums divided by the total number of transitions. Thus, the scanning score reflected the proportion of transitions that followed a consistent clockwise or counterclockwise scanning pattern. A permutation procedure was used to quantify the chance level of the scanning scores (for additional details, see the Supplemental Material). As can be seen in Figure <ref type="figure">3a</ref>, both groups had scanning scores above chance level-Israeli sample: = 34.26, p &lt; .001, d = 5.79, 95% CI = [4.44, 7.27]; German sample: t(32) = 30.67, p &lt; .001, d = 5.34, 95% CI = [4.05, 6.75]. Interestingly, there was also a cultural difference between the two samples, in that the German participants had a more systematic pattern of scanning than the Israeli sample, t(66) = -6.05, p &lt; .001, d = 1.47, 95% CI = [0.93, 2].</p><p>The strong tendency to scan the array in a clockwise or counterclockwise manner implies that when participants reached the first face, they usually shifted their gaze between adjacent faces. However, shifts between nonadjacent faces were still present. These "long-distance" shifts (see Fig. <ref type="figure">3b</ref>) toward a familiar face are a valid indicator of guidance of search because they show that the participant deviated from his or her typical scanning pattern, even during active search. Accordingly, we examined whether these long-distance shifts were more probable toward the familiar face than toward the familiar face of the other group. For that purpose, we computed the percentage of trials in which long-distance shifts were made toward the familiar face and the familiar face of the other group. In line with our guidance hypothesis, results showed a higher percentage of trials with long-distance shifts toward the familiar face (Israeli sample: M = 5.1%, SD = 3.9%; German sample: M = 6.8%, SD = 5.3%) in comparison with the familiar face of the other group (Israeli sample: M = 3.8%, SD = 3.6%; German sample: M = 5.5%, SD = 4%). These differences were significant-Israeli sample: t(34) = 2.54, p = .008, d = 0.43, 95% CI = [0.08, 0.79]; German sample: t(32) = 1.74, p = .046, d = 0.3, 95% CI = [-0.05, 0.66].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saccade amplitude</head><p>The higher proportion of trials with long-distance shifts toward the familiar face suggests that guidance toward the familiar face would also manifest in the amplitude of the saccade (i.e., the length of the movement of the eye between two relatively stable fixations). Specifically, if gaze was shifted toward familiar faces from longer distances than to unfamiliar faces, we would predict that the amplitude of the saccades preceding the first fixation on the familiar faces would be, on average, longer than the amplitude of the saccades preceding the first fixation to the unfamiliar face. As predicted, the analysis confirmed this pattern of gaze behavior (see Fig. <ref type="figure">4a</ref>): Larger saccade amplitudes preceded the first fixation on the familiar face than the first fixation on the face that was familiar to the other nationality-Israeli sample: t(34) = 3.22, p = .001, d = 0.54, 95% CI = [0.19, 0.91]; German sample: t(32) = 3.4, p &lt; .001, d = 0.59, 95% CI = [0.22, 0.97]. Because the distance between the central cross and the center of the faces (9 DVA) was smaller than the distance between the centers of the adjacent faces (10.6 DVA), we also verified that this effect could not be attributed to the first saccade from the central cross to the first face. For that purpose, we excluded the saccade that preceded the fixation on the first face and repeated the analysis. The same pattern of results was obtained: Longer saccades preceded the fixation on the familiar face in comparison with the familiar face of the other group-Israeli sample: t(34) = 2.7, p = .005, d = 0.46, 95% CI = [0.11, 0.81]; German sample: t(32) = 2.17, p = .019, d = 0.38, 95% CI = [0.02, 0.74].</p><p>To better characterize the source of this effect, we compared the distributions of saccades preceding the familiar face with the saccades preceding the familiar face of the other group. We plotted the quantiles of one distribution against the quantiles of the other distribution (i.e., quantile-quantile [QQ] plots). QQ plots provide an intuitive visualization of the comparison between distributions according to their quantiles; if the distributions are similar, the values of the quantiles of the two distributions should be approximately equal. Therefore, when we plot the values of the quantiles against each other, they should lie on the cardinal diagonal (i.e., y = x). Any deviation from that diagonal line reflects a difference between the two distributions. As highlighted in Figure <ref type="figure">4b</ref>, the QQ plots of the two groups diverged from the diagonal line in the range of approximately 12.5 to 17 DVA. Specifically, in this range, the values that corresponded to the quantiles of the distribution of the saccades preceding the familiar face were larger. This finding indicates that in this range, the saccades that preceded the familiar face were longer in comparison with the saccades that preceded the familiar face of the other group. Because the distance between the centers of two adjacent faces was 10.5 DVA (the distance between the near and far edges was 2.5 and 18.5 DVA, respectively; see the Method section), having longer saccades in the range of 12.5 to 17 DVA constituted another indication for saccades between pairs of nonadjacent faces or between relatively distant locations in adjacent faces. Accordingly, the discrepancy between the two distributions of saccades specifically in the described range provides further support for shifts in gaze toward the familiar face based on extrafoveal processing of familiarity (because the fovea lies within a radius of approximately 1 DVA from the center of the retina; <ref type="bibr" target="#b34">Strasburger et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Two groups of participants, in two different labs and two different continents, were exposed to the same sets of faces and were asked to look for the familiar face in each set. The familiar face of one group was an unfamiliar face for the other group. Nevertheless, gaze behavior was impressively similar in the two groups: Participants fixated on the familiar face earlier than expected by chance and typically before gaze was directed toward the familiar face of the other group. This effect was mainly driven by a higher probability that the familiar face would be the first face selected during the trial. However, guidance was also evident when participants did not find the familiar face on their first attempt. During the active search, there was a higher probability for the participants to shift their gaze toward the familiar face from nonadjacent faces. Gaze shifts before the first fixation on the familiar face were significantly larger, on average, than the shifts targeting the familiar face of the other group. These indices of guidance were found despite participants' tendency to systematically scan the face array in a clockwise or counterclockwise manner. Importantly, the design of the task (in which participants saw the same faces but with different familiarity tagging) makes it possible to distill the effect of familiarity on gaze behavior and rule out alternative explanations for these results (e.g., visual saliency). Moreover, because familiarity differentiated the target from the distractors and because of the large number of faces that are familiar to each individual, it seems improbable that any active template was available to guide search in this case. In addition, each face was displayed only once, hampering the construction of an active guiding template throughout the experiment. Therefore, our findings suggest that long-term familiarity was sufficient to guide gaze toward the target, even in the absence of an active search template. It should be emphasized that our findings not only are consistent with the ability to process familiarity through extrafoveal vision but also provide strong evidence that this ability can be further used by the cognitive system to facilitate the deployment of visual attention during search.</p><p>How could the search for the familiar face be guided even though the participants did not know whom they were looking for? This seemingly paradoxical situation led previous researchers to assume that an active guiding template is mandatory for successful completion of a search <ref type="bibr" target="#b4">(Bravo &amp; Farid, 2009</ref><ref type="bibr">, 2014;</ref><ref type="bibr" target="#b18">Hout &amp; Goldinger, 2015;</ref><ref type="bibr" target="#b23">Madrid et al., 2019;</ref><ref type="bibr" target="#b24">Malcolm &amp; Henderson, 2009;</ref><ref type="bibr" target="#b32">Reeder et al., 2015;</ref><ref type="bibr" target="#b39">Vickery et al., 2005;</ref><ref type="bibr" target="#b42">Wolfe, 2020;</ref><ref type="bibr" target="#b44">Yang &amp; Zelinsky, 2009;</ref><ref type="bibr" target="#b50">Zelinsky et al., 2006)</ref>. This assumption is apparently inconsistent with visual search tasks in which participants search for the odd-one-out target, sometimes without knowing in advance what the target is <ref type="bibr" target="#b12">(Friedman-Hill &amp; Wolfe, 1995;</ref><ref type="bibr" target="#b25">Maljkovic &amp; Nakayama, 1994;</ref><ref type="bibr" target="#b29">Müller et al., 1995;</ref><ref type="bibr" target="#b40">Wang et al., 1994)</ref>. However, in these tasks, the odd-one-out target is defined by the visual features of the distractors (e.g., color and orientation), so that an active guiding template can easily be constructed after exposure to two distractors (e.g., two green distractors mean that the target color is different). Accordingly, although such tasks might start without an active template, it can be constructed rapidly after the onset of the trial. Importantly, although searching for a familiar face among unfamiliar ones may be conceptualized as a search for the odd-one-out target, the exposure to the distractors (i.e., the unfamiliar faces) is not informative in any way as to the target in question (i.e., the familiar face). Thus, although searching for the familiar face resembles the odd-one-out search in some ways, it differs from it on a crucial point: When participants search for the familiar face, no active guiding template could be generated by processing the distractors.</p><p>A recent theoretical review <ref type="bibr" target="#b19">(Hulleman &amp; Olivers, 2017)</ref> suggests that the centrality of the search template in previous studies and frameworks derives from theories that emphasize individual items as the unit of search. According to these item-based theories, search proceeds on the basis of selection of individual items that are either rejected as distractors or recognized as the target. Therefore, it is reasonable to view search as a process in which items are compared with a template specifying the target. In contrast, Hulleman and Olivers put forward an alternative framework of visual search in which fixations (and not individual items) are the central unit of search. Specifically, during visual search, a functional viewing field is defined around each fixation according to the area of the visual field that can be processed through this fixation. The functional viewing field is not fixed but changes as a function of processing difficulty; that is, the more easily a target can be discriminated from distractors through extrafoveal vision, the larger the functional viewing field should be and the faster the target can be found. The reverse would be true when the target is more difficult to discriminate. In line with this framework, having an active guiding template could make the target more discriminable and facilitate search, but it is not indispensable to the search process; other features of the target may also enhance its discriminability. Specifically, our results demonstrate that familiarity is one such feature because we observed guidance of search toward the familiar face even when participants did not know whom they were looking for. Importantly, when participants could not benefit from extrafoveal vision (the gaze-contingent condition in the current study), this type of guidance vanished.</p><p>When an active guiding template does exist, search is presumed to be guided to places in the visual field that generate a match to the template <ref type="bibr" target="#b5">(Bravo &amp; Farid, 2014)</ref>. When an active template is not available, search must be guided by multiple LTM representations rather than a few active ones. Computational models of memory suggest that encoding and retrieval processes (including face recognition) may be performed by attractor networks <ref type="bibr" target="#b1">(Amit, 1992;</ref><ref type="bibr" target="#b10">Farah et al., 1993;</ref><ref type="bibr" target="#b35">Tank &amp; Hopfield, 1987)</ref>. These models are composed of several units that are linked through weighted connections and are updated during encoding until the network incorporates several stable states (i.e., attractors) that represent memories. Subsequent to a new input, the network settles on one of the attractors on the basis of the input and the weights. Notably, even a partial input can activate the correct memory, as suggested by theories of pattern-completion processes and empirical findings on the robustness of face recognition despite substantial changes to the visual input <ref type="bibr" target="#b6">(Bruce et al., 2001;</ref><ref type="bibr" target="#b7">Bruck et al., 1991;</ref><ref type="bibr" target="#b30">Ramon, 2015)</ref>. The attractor network may be a plausible model underlying the guidance of search based on LTM, as observed in our study. In some cases, the information perceived through extrafoveal vision was sufficient to lead the network toward an established attractor, thus guiding search toward the target without necessarily matching the input to an active template.</p><p>Early studies of visual search <ref type="bibr" target="#b38">(Treisman &amp; Gelade, 1980;</ref><ref type="bibr" target="#b43">Wolfe et al., 1989</ref>) used search arrays with different set sizes to examine how the slope of the reaction times (i.e., time until the target is detected) changes as a function of the number of distractors. This slope estimates the cost in search time for additional distractors and was conceptualized as an indication of how efficiently attention can be guided toward the target. Interestingly, certain findings indicated that the overall number of fixations during the search closely mirrored the reaction time <ref type="bibr" target="#b3">(Binello et al., 1995;</ref><ref type="bibr" target="#b45">Young &amp; Hulleman, 2013;</ref><ref type="bibr" target="#b48">Zelinsky &amp; Sheinberg, 1995</ref><ref type="bibr" target="#b11">, 1997)</ref>. Moreover, there is considerable evidence that gaze shifts are preceded by covert shifts of attention toward the same locations <ref type="bibr" target="#b9">(Deubel &amp; Schneider, 1996;</ref><ref type="bibr" target="#b15">Henderson, 1992;</ref><ref type="bibr" target="#b16">Henderson et al., 1989;</ref><ref type="bibr" target="#b17">Hoffman &amp; Subramaniam, 1995;</ref><ref type="bibr" target="#b21">Kowler et al., 1995)</ref>. Taken together, gaze shifts may be another, presumably more overt, indicator of attentional guidance. Therefore, despite some accounts that view eye movements as an epiphenomenon of visual search <ref type="bibr" target="#b37">(Treisman, 1982;</ref><ref type="bibr" target="#b41">Wolfe, 1998)</ref>, eye movements have become increasingly more common and crucial to visual search studies <ref type="bibr" target="#b2">(Beutter et al., 2003;</ref><ref type="bibr" target="#b9">Deubel &amp; Schneider, 1996;</ref><ref type="bibr" target="#b18">Hout &amp; Goldinger, 2015;</ref><ref type="bibr" target="#b31">Rao et al., 2002;</ref><ref type="bibr" target="#b36">Tavassoli et al., 2009;</ref><ref type="bibr" target="#b45">Young &amp; Hulleman, 2013;</ref><ref type="bibr" target="#b47">Zelinsky et al., 2013)</ref> and theories <ref type="bibr" target="#b46">(Zelinsky, 2008;</ref><ref type="bibr" target="#b50">Zelinsky et al., 2006)</ref>. Thus, in the current study, we used eye movements rather than reaction time slopes to shed light on the dynamics of attention in visual search.</p><p>It is no wonder that visual search, being one of the most common human activities, has become one of the most frequently investigated topics in the cognitive sciences. In different frameworks, the active search template tends to be considered essential for guiding search. In the current study, we showed that search can be guided even in the absence of an active visual template through extrafoveal processing of familiarity. These findings corroborate new theoretical accounts <ref type="bibr" target="#b19">(Hulleman &amp; Olivers, 2017)</ref> that claim that search performance is determined by the discriminability of the target through extrafoveal vision. Hence, we believe that to decipher the underlying mechanisms of visual search, researchers should focus their efforts on understanding the processing of target information in extrafoveal vision rather than the ability to maintain an active guiding template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency</head><p>Action Editor: Krishnankutty Sathian Editor: Patricia J. Bauer</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Fig.1. Illustration of the visual search paradigm. Participants were asked to look for a singleton color (color block) or a familiar face (revealed and gaze-contingent blocks). Each search array was displayed around a central cross (left column). Participants were asked to look at the cross and initiate search after it disappeared (middle column). When participants found the target, they were asked to respond by pressing the space bar while fixating on the target (right column). In the color and revealed blocks, participants were exposed to all stimuli throughout the course of the trial; in contrast, in the gaze-contingent block, each face was revealed only when the participant had fixated directly on it. Black bars have been added to the faces to preserve these individuals' privacy (participants saw the unmodified images).</figDesc><graphic coords="3,151.60,243.30,80.06,58.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Summary of the ordinal-number analysis for the Israeli (left column) and German (right column) participants. The mean ordinal number of direct fixations on Israeli faces (bars with blue and white stripes) and German faces (bars with black, red, and yellow stripes) in the revealed and gaze-contingent blocks is shown in the top row. The mean proportion of trials for each ordinal number is shown in the bottom row. The dashed lines in all graphs represent the expected chance level when search was not guided (uniform distribution).Error bars indicate 95% confidence intervals. Asterisks indicate significant differences between face nationalities (top row) and significant differences compared with chance (bottom row; p &lt; .001 for paired-samples t-test comparisons).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Summary of scanning-pattern analysis. The distribution of average scanning scores across all trials (a) is shown separately for the Israeli and German participants. The gray area is the distribution of scores following permutations of the order of faces and captures the chance level of scanning scores across both groups. The vertical line represents the mean of each distribution. On the left, we display the scanning patterns that were considered systematic scanning. The percentage of trials in which long-distance shifts were made toward the familiar Israeli faces (bars with blue and white stripes) and the familiar German faces (bars with black, red, and yellow stripes) is shown in (b), separately for Israeli and German participants. Error bars indicate 95% confidence intervals. Asterisks indicate significant differences between whether faces were famous in participants' native country or the comparison country (*p &lt; .05, **p &lt; .01 for paired-</figDesc><graphic coords="7,307.10,618.85,79.94,96.62" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Chana Berelejis</rs> and <rs type="person">Aki Schumacher</rs> for assisting in data collection.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This research was supported by an <rs type="funder">Israel Science Foundation</rs> grant (<rs type="grantNumber">2414/20</rs>) to <rs type="person">Y. Pertzov</rs> and a <rs type="funder">German Israel Foundation</rs> grant (<rs type="grantNumber">I-2416-105.4/2016</rs>) to <rs type="person">Y. Pertzov</rs>.</p></div>
<div><head>Open Practices</head><p>All data, analysis code, and materials have been made publicly available via OSF and can be accessed at <ref type="url" target="https://osf.io/bkuwt">https://  osf.io/bkuwt</ref>. The design and analysis plans for the Israeli and German groups were preregistered at <ref type="url" target="https://osf.io/kxpdg">https://osf.io/  kxpdg</ref> and <ref type="url" target="https://osf.io/wf5k2">https://osf.io/wf5k2</ref>, respectively. This article has received the badges for Open Data, Open Materials,</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tYsVZTm">
					<idno type="grant-number">2414/20</idno>
				</org>
				<org type="funding" xml:id="_xYgMgHG">
					<idno type="grant-number">I-2416-105.4/2016</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and Preregistration. More information about the Open Practices badges can be found at <ref type="url" target="http://www.psychologicalscience.org/publications/badges">http://www.psycho  logicalscience.org/publications/badges</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>All the authors developed the study concept and contributed to the design and the required analysis methods. O. C. Lancry-Dayan analyzed and interpreted the data under the supervision of Y. Pertzov and M. Gamer. All the authors wrote the manuscript and approved the final version for submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Additional supporting information can be found at <ref type="url" target="http://journals.sagepub.com/doi/suppl/10.1177/0956797621996660">http://  journals.sagepub.com/doi/suppl/10.1177/0956797621996660</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Specifying the precision of guiding features for visual search</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Nahvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1248" to="1264" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Modeling brain function: The world of attractor neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Amit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Saccadic and perceptual performance in visual search tasks. I. Contrast detection and discrimination</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Beutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1341" to="1355" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The characteristics of eye movements made during visual search with multi-element stimuli</title>
		<author>
			<persName><forename type="first">A</forename><surname>Binello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Ruddock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="362" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The specificity of the search template</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<idno type="DOI">10.1167/9.1.34</idno>
		<ptr target="https://doi.org/10.1167/9.1.34" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Informative cues can slow search: The cost of matching a specific template</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="39" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Matching identities of familiar and unfamiliar faces caught on CCTV images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fortysomething: Recognizing faces at one&apos;s 25th reunion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cavanagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Ceci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="228" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How you use it matters: Object function guides attention during visual search in scenes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Witherspoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="606" to="621" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Saccade target selection and object recognition: Evidence for a common attentional mechanism</title>
		<author>
			<persName><forename type="first">H</forename><surname>Deubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1827" to="1838" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dissociated overt and covert recognition as an emergent property of a lesioned neural network</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Farah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>O'reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Vecera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="571" to="588" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Saccade target selection during visual search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Findlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="617" to="631" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Second-order parallel processing: Visual search for the odd item in a subset</title>
		<author>
			<persName><forename type="first">S</forename><surname>Friedman-Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="531" to="551" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The average American knows how many people? The New York Times</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2013/02/19/science/the-average-american-knows-how-many-people.html#:~:text=The%20average%20American%20knows%20about%20600%20people" />
		<imprint>
			<date type="published" when="2013-02-19">2013, February 19</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The social structure of acquaintanceship networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gurevitch</surname></persName>
		</author>
		<ptr target="https://dspace.mit.edu/handle/1721.1/11312" />
		<imprint>
			<date type="published" when="1961">1961</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Doctoral thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual attention and eye movement control during reading and picture viewing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eye movements and visual cognition</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Rayner</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="260" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Covert visual attention and extrafoveal information use during object identification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pollatsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="196" to="208" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The role of visual attention in saccadic eye movements</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Subramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="787" to="795" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Target templates: The precision of mental representations affects attentional guidance and decision-making in visual search</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Goldinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="149" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The impending demise of the item in visual search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hulleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Olivers</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X15002794</idno>
		<ptr target="https://doi.org/10.1017/S0140525X15002794" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detection by action: Neuropsychological evidence for action-defined templates in search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Riddoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="88" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The role of attention in the programming of saccades</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dosher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blaser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1897" to="1916" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eye movements during search for coded and uncoded targets</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Luria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="308" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">You&apos;re looking for what? Comparing search for familiar, nameable objects to search for unfamiliar, novel objects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Madrid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="20" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The effects of target template specificity on visual search in real-world scenes: Evidence from eye movements</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Malcolm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.1167/9.11.8</idno>
		<ptr target="https://doi.org/10.1167/9.11.8" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Priming of pop-out: I. Role of features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Maljkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="657" to="672" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How many people do you know? Efficiently estimating personal network size</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Salganik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">489</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="https://www.dailymail.co.uk/news/article-2117987/Lost-today-Misplaced-items-cost-minutes-day.html" />
	</analytic>
	<monogr>
		<title level="m">Lost something already today? Misplaced items cost us ten minutes a day</title>
		<imprint>
			<date type="published" when="2012-03-20">2012, March 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The guidance of eye movements during active visual search</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Motter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Belky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1805" to="1815" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual search for singleton feature targets within and across feature dimensions</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Perception of global facial geometry is modulated through experience</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ramon</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.850</idno>
		<ptr target="https://doi.org/10.7717/peerj.850" />
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">850</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Eye movements in iconic visual search</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Hayhoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1447" to="1463" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Involuntary attentional capture by task-irrelevant objects that match the search template for category detection in natural scenes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Reeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Zoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Peelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1070" to="1080" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How context information and target information guide the eyes from the first epoch of search in real-world scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Spotorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Malcolm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Tatler</surname></persName>
		</author>
		<idno type="DOI">10.1167/14.2.7</idno>
		<ptr target="https://doi.org/10.1167/14.2.7" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Peripheral vision and pattern recognition: A review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strasburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rentschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jüttner</surname></persName>
		</author>
		<idno type="DOI">10.1167/11.5.13</idno>
		<ptr target="https://doi.org/10.1167/11.5.13" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Collective computation in neuronlike circuits</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="104" to="115" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Eye movements selective for spatial frequency and orientation during active visual search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tavassoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Van Der Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="181" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Perceptual grouping and attention in visual search for features and for objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="214" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Setting up the target template in visual search</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Vickery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1167/5.1.8</idno>
		<ptr target="https://doi.org/10.1167/5.1" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Familiarity and pop-out in visual search</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cavanagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="495" to="500" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Visual search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<editor>H. Pashler</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Psychology Press</publisher>
			<biblScope unit="page" from="13" to="74" />
		</imprint>
	</monogr>
	<note>Attention</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual search: How do we find what we are looking for?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Vision Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="539" to="562" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Guided search: An alternative to the feature integration model for visual search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Franzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="433" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visual search is guided to categorically defined targets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2095" to="2103" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Eye movements reveal how task difficulty moulds visual search</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hulleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="168" to="190" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A theory of eye movements during target acquisition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="787" to="835" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Eye can read your mind: Decoding gaze fixations to reveal categorical search targets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<idno type="DOI">10.1167/13.14.10</idno>
		<ptr target="https://doi.org/10.1167/13.14.10" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Why some search tasks take longer than others: Using eye movements to redefine reaction times</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Sheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eye movement research: Mechanisms, processes and applications</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Findlay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Walker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kentridge</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="325" to="336" />
			<date type="published" when="1995">1995</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
	<note>Studies in visual information processing</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Eye movements during parallel-serial visual search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Sheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="244" to="262" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The role of top-down and bottom-up processes in guiding eye movements during visual search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1569" to="1576" />
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
