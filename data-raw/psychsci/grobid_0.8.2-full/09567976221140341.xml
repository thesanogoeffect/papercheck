<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predictive Processing of Scene Layout Depends on Naturalistic Depth of Field</title>
				<funder ref="#_Y9vn9nr #_XZ7mtyr">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_x3EdQE2">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Marco</forename><surname>Gandolfo</surname></persName>
							<email>marco.gandolfo@donders.ru.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="department" key="dep2">Donders Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="institution" key="instit1">Radboud University</orgName>
								<orgName type="institution" key="instit2">Radboud University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="department" key="dep2">Donders Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="institution" key="instit1">Radboud University</orgName>
								<orgName type="institution" key="instit2">Radboud University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="department" key="dep2">Donders Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="institution" key="instit1">Radboud University</orgName>
								<orgName type="institution" key="instit2">Radboud University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Donders</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="department" key="dep2">Donders Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="institution" key="instit1">Radboud University</orgName>
								<orgName type="institution" key="instit2">Radboud University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hendrik</forename><surname>Nägele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="department" key="dep2">Donders Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="institution" key="instit1">Radboud University</orgName>
								<orgName type="institution" key="instit2">Radboud University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marius</forename><forename type="middle">V</forename><surname>Peelen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="department" key="dep2">Donders Institute for Brain, Cognition, and Behaviour</orgName>
								<orgName type="institution" key="instit1">Radboud University</orgName>
								<orgName type="institution" key="instit2">Radboud University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Predictive Processing of Scene Layout Depends on Naturalistic Depth of Field</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">04B18478EE5E7AEDE3DC266B09825A58</idno>
					<idno type="DOI">10.1177/09567976221140341</idno>
					<note type="submission">Received 5/25/22; Revision accepted 10/31/22</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-06-03T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>boundary extension</term>
					<term>natural vision</term>
					<term>predictive processing</term>
					<term>scene memory</term>
					<term>visual perception</term>
					<term>open data</term>
					<term>open materials</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Boundary extension is a classic memory illusion in which observers remember more of a scene than was presented. According to predictive-processing accounts, boundary extension reflects the integration of visual input and expectations of what is beyond a scene's boundaries. According to normalization accounts, boundary extension rather reflects one end of a normalization process toward a scene's typically experienced viewing distance, such that close-up views give boundary extension but distant views give boundary contraction. Here, across four experiments (N = 125 adults), we found that boundary extension strongly depends on depth of field, as determined by the aperture settings on a camera. Photographs with naturalistic depth of field led to larger boundary extension than photographs with unnaturalistic depth of field, even when distant views were shown. We propose that boundary extension reflects a predictive mechanism with adaptive value that is strongest for naturalistic views of scenes. The current findings indicate that depth of field is an important variable to consider in the study of scene perception and memory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Boundary extension is a classic memory illusion in which observers remember more of a scene than was actually presented <ref type="bibr" target="#b24">(Intraub &amp; Richardson, 1989)</ref>. For example, when participants draw a previously observed scene, they typically include information not present in the photograph, expanding the scene beyond its boundaries <ref type="bibr" target="#b24">(Intraub &amp; Richardson, 1989)</ref>. Similarly, when rating the viewpoint of a second probe photograph relative to a remembered target photograph, participants rate the exact same view as closer than the original image, indicating that they remembered more of the scene than was presented <ref type="bibr" target="#b1">(Bainbridge &amp; Baker, 2020;</ref><ref type="bibr" target="#b4">Bertamini et al., 2005;</ref><ref type="bibr">Dickinson &amp; Intraub, 2008;</ref><ref type="bibr">Intraub &amp; Dickinson, 2008)</ref>.</p><p>Why does this illusion occur? One possibility is that it reflects a predictive mechanism in which scene percepts do not correspond solely to the visual input but are rather constructed through integration of visual input and expectations of what is beyond a scene's boundaries <ref type="bibr" target="#b16">(Intraub, 2010;</ref><ref type="bibr" target="#b27">Maguire &amp; Mullally, 2013)</ref>. The discrete views sampled by the human eye may thus be supplemented with anticipatory representations of the scene, providing a continuous visual experience.</p><p>Alternatively, boundary extension may reflect one end of a generic normalization process in memory <ref type="bibr" target="#b3">(Bartlett, 1932)</ref>, where the perceived scene viewpoint regresses toward a prototypical viewing distance: A view that is close up compared with the observer's average viewing distance of that scene will be remembered as farther away (yielding boundary extension), whereas faraway views will be remembered as being closer, generating the opposite error, boundary contraction. On this account, boundary extension and boundary contraction are in principle equally common, depending only on whether the presented photographs are taken from a relatively close or far distance.</p><p>Previous work has shown that boundary extension occurs more frequently than boundary contraction <ref type="bibr" target="#b15">(Hubbard et al., 2010;</ref><ref type="bibr" target="#b18">Intraub, 2020)</ref>. Furthermore, boundary extension occurs for scenes but not for single, isolated objects <ref type="bibr" target="#b11">(Gottesman &amp; Intraub, 2002;</ref><ref type="bibr" target="#b21">Intraub et al., 1998)</ref>, unlike normalization, which also occurs for isolated objects <ref type="bibr" target="#b5">(Brady et al., 2011;</ref><ref type="bibr" target="#b21">Intraub et al., 1998;</ref><ref type="bibr" target="#b25">Konkle &amp; Oliva, 2007)</ref>. However, although these results appear to show that boundary extension cannot be entirely accounted for by normalization, this conclusion relies on the strong assumption that the scene photographs used in previous work were representative of our daily-life visual experience, particularly with respect to viewing distance. If the photographs used in boundary extension studies were not representativefor example, if they primarily showed views that were closer than usually experienced-boundary extension may be entirely explained by a normalization account.</p><p>Indeed, using a large and diverse stimulus set (1,000 photographs), a recent study showed that boundary extension was equally as common as boundary contraction <ref type="bibr" target="#b1">(Bainbridge &amp; Baker, 2020)</ref>. Moreover, the direction of these memory errors was correlated with ratings of subjective image distance: In line with the normalization account, subjectively close scenes tended to extend, and subjectively far scenes tended to contract. These results suggest that boundary extension may be one side of a regression toward the typical sceneviewing distance.</p><p>The above discussion highlights that approximating daily-life visual experience is critical for both accounts: for the predictive-processing account because it proposes that boundary extension reflects expectations based on real-world experience and for the normalization account because normalization is relative to our prior experience. If boundary extension reflects expectations, it should be observed more prominently (and be more common than boundary contraction) when views are representative of our visual experience. Importantly, this is not automatically achieved through the use of large and diverse image sets. Even if these are representative of pictorial views of scenes (i.e., of how scene photographs are taken), they are not necessarily representative of how an observer perceives their surroundings during day-to-day visual experience.</p><p>A striking example is that a camera can produce pictures with a deeper depth of field than the eye can <ref type="bibr" target="#b0">(Artal, 2014;</ref><ref type="bibr" target="#b30">Middleton, 1957)</ref>. The depth of field represents the distance range over which an object may be moved without causing a sharpness reduction. On a camera, depth of field can be controlled by the lens aperture, typically specified by an f-number (focal length divided by aperture diameter). The eye's depth of field similarly depends on the aperture of the pupil.</p><p>The larger the aperture, the shallower the depth of field, resulting in a smaller distance range within which objects are in focus. The aperture range of most cameras is much larger than that of the eye, which ranges between f/2.1 and f/8. Specifically, photographs often have very deep depth of field (e.g., f/22): Everything in the picture is sharp, leading to views of scenes that are highly unnatural.</p><p>Here, we found that boundary extension strongly depends on depth of field, as determined by the aperture settings on a camera. Reanalyzing the 1,000-image dataset <ref type="bibr" target="#b1">(Bainbridge &amp; Baker, 2020)</ref>, we found that boundary extension was much more common than boundary contraction for photographs with depths of field within the range of human vision. Subsequently, in three experiments, we demonstrated the specific influence of depth of field on boundary extension while controlling for factors that may naturally covary with depth of field in scene photographs, including viewing distance, number and type of objects, and image statistics. Again, boundary extension was strongest for the relatively shallow depth of field that characterizes human vision and was observed even for distant views of scenes. Altogether, our results are most in line with views of boundary extension as a predictive mechanism, with boundary extension reliably observed for naturalistic views of scenes to which the brain has adapted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statement of Relevance</head><p>In daily life, we experience a rich and continuous visual world in spite of the capacity limits of the visual system. We may compensate for such limits with our memory, by filling in the visual input with anticipatory representations of upcoming views. The boundary extension illusion provides a tool to investigate this phenomenon. For example, not all images equally lead to boundary extension. In this set of experiments, we found that memory extrapolation beyond scene boundaries was strongest for images resembling human visual experience, showing depth of field in the range of human vision. On the basis of these findings, we propose that predicting upcoming views is conditional to a scene being naturalistic. More generally, the strong reliance of a cognitive effect, such as boundary extension, on naturalistic image properties indicates that it is imperative to use image sets that are ecologically representative when studying the cognitive, computational, and neural mechanisms of scene processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Practices Statement</head><p>All data and stimuli for these experiments have been made publicly available via OSF and can be accessed at <ref type="url" target="https://osf.io/7btfq/">https://osf.io/7btfq/</ref>. The design and analysis plans for the experiments were not preregistered. Video demonstrations of the experimental procedures are available at <ref type="url" target="https://sites.google.com/view/marcogandolfo/online-experiments/boundary-extension-and-depth-of-field?pli=1">https://sites.google.com/view/marcogandolfo/  online-experiments/boundary-extension-and-depth- of-field?pli=1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1</head><p>Our goal in Experiment 1 was to test for a relationship between the perceived depth of field of a photograph and its boundary transformation scores (boundary extension or boundary contraction) in the large image set used by <ref type="bibr" target="#b1">Bainbridge and Baker (2020)</ref>. To this end, we collected aperture ratings (as judged from depth of field) for each of the 1,000 scenes and related these ratings to the boundary transformation scores made available in the original study. On the basis of the predictive-processing account, we hypothesized that the more an image would be rated as having been shot with a large aperture-resembling natural vision-the more strongly it would elicit boundary extension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. Twenty-six participants (22 females, age: M = 24.5, SD = 5.84) signed up for the online scene-rating experiment. Participants were recruited through Radboud University's participant panel (Sona Systems) in return for course credits and through social media. The experiment was advertised as being for people who knew the basics of photography. To further ensure participants' expertise, prior to the scene ratings, we administered a photography quiz. First, participants were asked to define their photography experience with a multiplechoice question: "How would you define your experience with photography? 1) Only took pictures with my smartphone; 2) Some knowledge, but no hands-on experience with Single-Lens-Reflex (SLR) cameras; 3) Some experience using SLR cameras; 4) (Semi-) Professional photographer." Second, participants had to complete a quiz with five multiple-choice questions. Four of these questions asked how a photograph would look if shot with a large aperture (i.e., shallow depth of field, lots of light entering the camera) and a small aperture (i.e., deep depth of field, little light entering the camera). Finally, one question asked whether a lens aperture of f/5.6 was larger or smaller than f/22.0, testing their knowledge of f-stop values. On the basis of the above questions, we excluded six participants because they declared they had no experience with single-lens-reflex photography (n = 3) or because they failed three or more questions out of five in the photography quiz (two participants had four errors, and one participant had five errors). Among the 20 included participants, only five had one or two errors (two participants had one error, and three participants had two errors). Quiz data for all the participants are available at <ref type="url" target="https://osf.io/7btfq/">https://osf.io/7btfq/</ref>. This experiment was approved by the Ethics Committee of Radboud University's Faculty of Social Sciences (ECSW2017-2306).</p><p>Stimuli and apparatus. Stimuli were taken from the study by <ref type="bibr" target="#b1">Bainbridge and Baker (2020)</ref> and consisted of 1,000 images from their rapid serial visual presentation (RSVP) experiments. These were downloaded from the OSF link (<ref type="url" target="https://osf.io/28nzt/">https://osf.io/28nzt/</ref>). Further, for the practice trials, 24 images were downloaded from Shutterdial (<ref type="url" target="https://www.shutterdial.com">https://www.shutterdial.com</ref>). This website allows users to search for photos according to their metadata (including aperture) from the flickr application programming interface (API). We selected 12 pictures with large apertures (f ≤ 4) and 12 pictures with small apertures (f ≥ 22.0) so the difference between shallow and deep depth of field would be salient. The experiment was conducted online and coded in JavaScript using the jsPsych toolbox (Version 6.2.0; de <ref type="bibr" target="#b7">Leeuw, 2015)</ref>. The data were saved on the Pavlovia.org servers (<ref type="url" target="http://www.pavlovia.org">www.pavlovia.org</ref>).</p><p>Procedure. After the photography quiz, participants were shown the task instructions. Here, the concept of aperture was briefly reexplained. Afterward, each participant was presented with six randomly selected practice images (three with large apertures and three with small apertures). In the practice session, participants simply had to report whether an image was shot with a large or small aperture. They received feedback after each trial. When the practice session was completed, more detailed example images were presented, this time stressing the continuity of the aperture effect over the depth of field of the pictures. This was achieved by showing images of the same photograph shot at different levels of aperture (i.e., large, middle, small). Finally, participants were informed that they would make the upcoming ratings on a sliding scale from 0 (large aperture) to 100 (small aperture). Each rating trial began with a fixation cross with a random duration of 1 s, 1.25 s, or 1.5 s. Following fixation, participants saw an image appear on screen concurrently with the slider positioned in the middle (aperture = 50). To continue to the next trial, participants had to touch the slider on the scale and press the button "continue" placed below the image and the slider. After every 50 ratings, participants were invited to take a break.</p><p>Design. Each participant completed one of four versions of the experiment. Each version included 250 images from the study by <ref type="bibr" target="#b1">Bainbridge and Baker (2020)</ref>, equally divided between those originally taken from the SUN database <ref type="bibr" target="#b37">(Xiao et al., 2016)</ref> and the Google Open Images database <ref type="bibr" target="#b26">(Kuznetsova et al., 2020)</ref>. The images were presented in random order to each participant. In total, we obtained five aperture ratings for each of the 1,000 stimuli of Bainbridge and Baker (2020), matching the number of subjective distance ratings collected by <ref type="bibr" target="#b1">Bainbridge and Baker (2020)</ref>.</p><p>Analysis. All the analyses were conducted using R (Version 4.0.1; R Core Team, 2020). We calculated the mean aperture rating for each of the 1,000 images. We also split these ratings (from 0 to 100) in five bins of 20-extra large (XL): 0-20, large (L): 21-40, medium (M): 41-60, small (S): 61-80; extra small (XS): 81-100. Afterward, we downloaded the raw data from the 1,000-image RSVP experiments (i.e., Experiments 2 and 3 by <ref type="bibr" target="#b1">Bainbridge &amp; Baker, 2020)</ref>. We rescored the boundary transformation scores to -1 (closer), 0 (same, only for Experiment 3), and 1 (farther) and calculated the mean boundary transformation score per image. With these two measures, we (a) ran the correlation between mean aperture ratings and the boundary transformation scores and (b) computed the average boundary rating across images in each categorical aperture rating bin (see Figs. <ref type="figure" target="#fig_0">1a</ref> and <ref type="figure" target="#fig_0">1b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>There was a strong and reliable correlation between rated aperture and boundary transformation score (Fig. <ref type="figure" target="#fig_0">1a</ref>; ρ = 0.58, p &lt; .001). Images with a shallow depth of field (large aperture) led to the largest extension effects, whereas images with deep depth of field (small aperture) showed less extension or even contraction. To further inspect this relationship, we created five bins, each representing an interval of 20 on the aperture rating scale (XL: 0-20, L: 21-40, M: 41-60, S: 61-80, XS: 81-100). Compellingly, the bar plot in Figure <ref type="figure" target="#fig_0">1b</ref> shows that contraction was reliable, on average, only for images rated as having been shot with a small aperture-t test vs. 0 on the values of the XS bin, t(121) = 2.56, p = .01, d = 0.23, M = 0.03, SE = 0.01, 95% confidence interval (CI) = [0.01, 0.06]). These aperture values are reachable by a camera but not by the human eye <ref type="bibr" target="#b14">(Hecht, 1987;</ref><ref type="bibr" target="#b30">Middleton, 1957</ref>; see the Supplemental Material available online for information about how we estimated the f-stop value corresponding to each of the five bins shown in Fig. <ref type="figure" target="#fig_0">1</ref>). <ref type="bibr" target="#b1">Bainbridge and Baker (2020)</ref> showed that subjective ratings of viewing distance were correlated with boundary transformation scores, in line with findings of other studies <ref type="bibr" target="#b12">(Hafri et al., 2022;</ref><ref type="bibr" target="#b19">Intraub et al., 1992;</ref><ref type="bibr" target="#b31">J. Park et al., 2021)</ref>. Images rated as having a close view led to larger extension than images rated as having a faraway view. It is possible that the relationship between rated depth of field and boundary scores observed here was entirely absorbed by the relationship between subjective distance and boundary scores. To address this, we partialed out ratings of subjective distance. The correlation between aperture and boundary scores remained highly reliable (ρ = .29, p &lt; .001), even when the shared variance between depth-of-field ratings and subjective distance ratings was regressed out (see the Supplemental Material for the same analyses splitting by image set and partialing out other available image properties from <ref type="bibr" target="#b1">Bainbridge &amp; Baker, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments 2 and 3</head><p>In Experiment 1, we showed that boundary transformation scores and perceived depth of field were related to each other across a large and diverse set of photographs, thus revealing that a previously unexplored image property affects memory for scene boundaries. However, as is the case for viewing distance <ref type="bibr" target="#b1">(Bainbridge &amp; Baker, 2020)</ref>, the depth of field of an image may covary with numerous other visual and semantic properties. For example, images with deep depth of field are more likely to show an outdoor environment. The correlational approach used in Experiment 1 is thus unable to unambiguously ascribe the effects to depth of field (or, indeed, viewing distance). Therefore, to firmly establish an effect of depth of field on boundary extension, we needed to manipulate depth of field while keeping all other properties equal. To do so, we shot the same 32 pictures of outdoor environments with two different camera apertures (f/5.6 and f/22). This factor was crossed with the camera's focus distance: either on a nearby or on a faraway object in the scene (Fig. <ref type="figure" target="#fig_1">2</ref>). If the size of memory distortions depends on how the depth of field of an image resembles natural viewing conditions, then images with shallow depth of field (f/5.6) should show larger extension than images with deep depth of field (f/22), independently of focus distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. The sample size was determined through a power analysis based on the data of a pilot experiment (reported in the Supplemental Material). This analysis revealed that a sample size of 35 participants was required to obtain 80% power to detect a between-conditions difference at least as large as the one observed between the large-and small-aperture conditions in the pilot experiment (reported in the Supplemental Material), on the basis of a paired-samples t contrast. In Experiment 2, 39 participants were recruited (16 females; age: M = 24.66, SD = 4.10) to arrive at a final sample of 35 after exclusions based on missing data or low accuracy (see Analyses). In Experiment 3, 36 participants were recruited (11 females, age: M = 24.36, SD = 4.66). Because of server issues, we obtained only 32 complete data files in Experiment 3 (see Analyses). Participants were recruited via Prolific (<ref type="url" target="www.prolific.co">www.prolific.co</ref>) in return for monetary compensation of £6.31 per hour. Both experiments were approved by the Ethics Committee of Radboud University's Faculty of Social Sciences (ECSW2017-2306).</p><p>Stimuli and apparatus. The stimulus set consisted of six photographs of 32 unique outdoor scenes <ref type="bibr">(192 photographs)</ref>. Each scene featured one or more objects in the foreground at a distance of approximately 2 m and one or more objects in the background at a distance of at least 5 m. For each scene, six photographs were taken using a digital single-lens reflex (DSLR) camera with a 50-mm fixed focal-length lens placed on a tripod. Photos were shot at three aperture levels-f/5.6 (large), f/11.0 (medium), and f/22 (small)-each one either manually focusing on the foreground objects or on the background objects (see Fig. <ref type="figure" target="#fig_1">2</ref>). The medium aperture level was used only in the pilot experiment (see the Supplemental Material). The International Organization for Standardization (ISO) number (light sensitivity of the sensor) was set manually in each scene (but kept constant across aperture and focus variations of that scene), and the shutter speed was automated to achieve similar lighting for the six photographs. We then generated close-view scenes through resizing (without resampling) the original pictures. This was achieved by enlarging the image (i.e., zooming in) using a random percentage value between 17% to 24% (in steps of 0.5%) of the image's surface area and then cropping the image to its original size. Each scene was enlarged by the same percentage across the scene's aperture and focus distance levels. All the pictures were then converted to gray scale and resized to 750 × 500 pixels. The images appeared on the screen at this resolution and therefore varied in size depending on the resolution the participants visualized them on their screen. Further, eight different masks were generated by computing the average image of all the photos in the stimulus set and iterating image scrambling in 20 pixel × 20 pixel blocks (randomly scrambling on the x-axis and then on the y-axis, or vice versa). Image processing was performed using the R package imager <ref type="bibr" target="#b2">(Barthelmé &amp; Tschumperlé, 2019)</ref>.</p><p>In Experiment 3, we matched the stimuli for low-level properties. Specifically, we used the SHINE toolbox <ref type="bibr" target="#b36">(Willenbockel et al., 2010)</ref> running on Octave (Version 5.1.0; <ref type="bibr" target="#b9">Eaton et al., 1997)</ref>. Using the sfMatch and Hist-Match functions, we matched the spatial frequency and histograms of each scene across its focus and aperture levels. Therefore, for each scene, four images were matched for their low-level properties-the small and large aperture images in both near and far focus levels. This procedure ensured that any difference between conditions in the experiment could not be due to covarying low-level properties emerging from the changes in focus distance and/or lens aperture. For both experiments, there were 128 images-32 for each combination of aperture and focus distance. Further, we generated an additional 128 close-view images belonging to the same conditions through resizing. Finally, the same eight masks of Experiment 2 were used. Both Experiments 2 and 3 were conducted online and coded in JavaScript using the jsPsych toolbox (Version 6.2.0; de <ref type="bibr" target="#b7">Leeuw, 2015)</ref>. The data were saved on the Pavlovia.org servers (<ref type="url" target="http://www.pavlovia.org)">www.pavlovia.org</ref>).</p><p>Procedure. In each trial (Fig. <ref type="figure" target="#fig_3">3a</ref>), after a fixation cross with a random duration between 1 s and 2 s, participants viewed a scene for 250 ms, followed by a 1-s dynamic mask (changing every 250 ms; four masks randomly selected from the eight available masks in each trial). Then a closer or wider version of the same scene was shown for 250 ms. Participants were asked to respond as quickly and as accurately as possible by pressing the "A" key if the second image looked closer or the "S" key if the image looked farther compared with the first image. Responses were given with the index ("S") and middle ("A") finger of the left hand. Immediately after response, or after 3.5 s, participants had to rate how sure they were of their response on a sliding scale from 0 (not sure) to 100 (sure). In the instructions, participants were told that the study concerned scene memory. Further, they were shown a self-paced example of the trial and performed a 10-trial practice block in which they were given feedback about their performance and got used to the task speed. Every 32 trials, there was a self-paced break. The above procedure applies to both Experiments 2 and 3.</p><p>Design. The experiment consisted of eight blocks of 32 trials each. In each block, we ensured that the same scene was not presented more than once. In every block, there were four scene images for each of the experimental conditions (i.e., two views: close to wide vs. wide to close; two focus levels: far vs. near; two apertures: large vs. small). The full design thus included 256 trials in total.</p><p>Analyses. In both experiments, we excluded participants who did not have a complete data file (one participant for Experiment 2; four participants for Experiment 3) or who did not respond on more than 50% of trials (one participant in Experiment 2; no participants in Experiment 3). Further, participants were considered outliers when their response time (RT) or accuracy was 2.5 standard deviations above (RT) or below (accuracy) the group mean across conditions (two participants in Experiment 2, no participants in Experiment 3). The analyses for Experiment 2 thus included data from 35 participants, and those for Experiment 3 included data from 32 participants. Finally, for each experiment, we ran a 2 (view: close to wide vs. wide to close) × 2 (aperture: large vs. small) × 2 (focus: foreground vs. background) repeated measures analysis of variance (ANOVA) on accuracy (correct detection of the second picture as being closer or wider). For purposes of visualization, we then computed a viewasymmetry index by subtracting accuracy on wide-toclose trials from accuracy on close-to-wide trials (Fig. <ref type="figure" target="#fig_3">3b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For this index, positive values indicate boundary contraction, and negative values indicate boundary extension.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We used a paradigm that quantifies boundary extension via a view-change-detection asymmetry <ref type="bibr">(Intraub &amp; Dickinson, 2008;</ref><ref type="bibr" target="#b24">Intraub &amp; Richardson, 1989;</ref><ref type="bibr" target="#b28">McDunn et al., 2014;</ref><ref type="bibr" target="#b32">S. Park et al., 2007)</ref>. In this paradigm (Fig. <ref type="figure" target="#fig_3">3a</ref>), all trials showed an actual change of view from the first to the second image, either changing from a wide to a close view or vice versa. Participants indicated whether the second image was closer or farther than the first image and rated their confidence on a sliding scale. If the boundaries of the first image were extended in memory, participants should have been worse at detecting changes from close to wide (vs. changes from wide to close) because the first (close) image was remembered as wider than it was. The Supplemental Material reports an experiment showing similar results when employing the same paradigm used to obtain the boundary extension scores employed in Experiment 1 <ref type="bibr" target="#b1">(Bainbridge &amp; Baker, 2020)</ref>.</p><p>In both experiments, we found a main effect of view-Experiment 2: F( <ref type="formula">1</ref> Importantly, in both experiments we also found a significant View × Aperture interaction (Fig. <ref type="figure" target="#fig_3">3b</ref> Photographs with large apertures led to larger boundary extensions-Experiment 2: M = -0.21, SE = 0.3, 95% CI = [-0.27, -0.16]; t(34) = -7.75, p &lt; .001, d = -1.31; Experiment 3: M = -0.20, SE = 0.3, 95% CI = [-0.26, -0.13]; t(31) = -6.17, p &lt; .001, d = -1.09-than pictures with small apertures-Experiment 2: M = -0.12, SE = 0.02, 95% CI = [-0.17, -0.07]; t(34) = -4.83, p &lt; .001, d = -0.82; Experiment 3: M = -0.13, SE = 0.03, 95% CI = [-0.20, -0.06]; t(31) = -3.74, p &lt; .001, d = -0.66. View change did not interact with focus distance (Fig. <ref type="figure" target="#fig_3">3b</ref>), Experiment 2: F(1, 34) = 2.24, p = .14, η p 2 = .06; Experiment 3: F(1, 31) = 0.48, p = .49, η p 2 = .01, and there was no three-way interaction, Experiment 2: F(1, 34) = 0.05, p = .82, η p 2 = .01; Experiment 3: F(1, 31) = 0.02, p = .88, η p 2 = .01. The main effect of aperture, Experiment 2: F(1, 34) = 4.12, p = .05, η p 2 = .10; Experiment 3: F(1, 31) = 0.04, p = .85, η p 2 = .01; the main effect of focus distance, Experiment 2: F(1, 34) = 0.23, p = .63, η p 2 = .01; Experiment 3: F(1, 31) = 0.22, p = .64, η p 2 = .01; and the interaction between aperture and focus distance, Experiment 2: F(1, 34) = 0.00, p = .96, η p 2 = .00; Experiment 3: F(1, 31) = 2.86, p = .10, η p 2 = .08, did not reach significance. A similar pattern of results was observed for inverse efficiency scores (RT/% correct), confirming that results were not due to speed/ accuracy trade-offs (see the Supplemental Material). Finally, confidence ratings mirrored the accuracy results (see the Supplemental Material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 4</head><p>In Experiments 2 and 3, we observed extension at all aperture levels, even when the depth of field of the images was deep (and therefore did not resemble human vision) and the focus was on the background (for which normalization may be expected). This could reflect the relatively close-up views of these photographs, with the background generally being within 50 m of the camera. To test whether depth of field also influences boundary extension for very distant views of scenes (for which normalization accounts predict boundary contraction; <ref type="bibr" target="#b1">Bainbridge &amp; Baker, 2020)</ref>, in Experiment 4, we used a new set of photographs depicting distant views of scenes, again with deep or shallow depth of field (small vs. large apertures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. We tested 38 participants (25 females, two other; age: M = 28 years, SD = 5) to arrive at the desired sample size of 35 (based on a power analysis; see Method of Experiments 2 and 3). Participants were recruited via Prolific (<ref type="url" target="www.prolific.co">www.prolific.co</ref>) in return for monetary compensation of £6.31 per hour. This experiment was approved by the Ethics Committee of Radboud University's Faculty of Social Sciences (ECSW2017-2306).</p><p>Stimuli and apparatus. We shot two photographs of 28 unique outdoor scenes (56 images). Each scene featured a landscape in the background, often containing a path or a house at a faraway distance (Fig. <ref type="figure" target="#fig_4">4a</ref>). For each scene, we shot two photographs using a DSLR camera with a 45-to 55-mm focal-length lens placed on a tripod. Photos were shot at two aperture levels-f/5.6 (large) and f/22 (small)-each one focusing on the foreground plane (stimuli can be downloaded from <ref type="url" target="https://osf.io/7btfq/">https://osf  .io/7btfq/</ref>). The ISO number was set manually in each scene (but was held constant across aperture levels), and the shutter speed was automated to achieve similar lighting for the two photographs. All photographs were then converted to gray scale and matched for luminance using the SHINE toolbox. Specifically, we matched the histogram of each pair of unique scene photographs across their aperture levels. We then resized the images to 750 pixels × 500 pixels.</p><p>Finally, we generated close-view scenes using the same procedure as in Experiments 2 and 3. Like before, the same scene was resized by an equal percentage across their aperture levels. In total, we had 112 images: 28 for each aperture level in their wide and close views. The masks were generated using the same procedure as in Experiments 2 and 3. The size at which the images were presented was controlled by asking participants to place a real-world object of a standard size (a credit card) on the screen and use it to resize a rectangle displayed on it. On the basis of the ratio between the rectangle image width (in pixels) and the physical width of the card (in millimeters), we scaled the images so that they would appear 15 cm wide and 7.5 cm long on every screen, independently of screen resolution and size. The experiment was conducted online and coded in JavaScript using the jsPsych toolbox (Version 6.2.0; de <ref type="bibr" target="#b7">Leeuw, 2015)</ref>. The data were saved on the Pavlovia.org servers (<ref type="url" target="http://www.pavlovia.org)">www.pavlovia.org</ref>).</p><p>Design. The experiment consisted of two blocks of 112 trials each. Each block was further divided in four miniblocks of 28 trials each. Within each miniblock, the same scene was not presented more than once. In each miniblock, there were seven scene images for each of the experimental conditions (i.e., two views: close to wide vs. wide to close; two apertures: large vs. small). The second 112-trial block had the same structure as the first block, but the order of the miniblocks was reshuffled. The full design included 224 trials in total.</p><p>Procedure. The procedure was the same as in Experiments 2 and 3.</p><p>Analyses. The same exclusion criteria as in Experiments 2 and 3 were used. On the basis of these criteria, we excluded two participants because they did not respond on more than 50% of the trials and one participant because their accuracy was 2.5 standard deviations below the group mean across conditions. The analyses included data from 35 participants. As our main analysis, we ran a 2 (view: close to wide vs. wide to close view) × 2 (aperture: large vs. small) repeated measures ANOVA on accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We found a significant View × Aperture interaction, F(1, 34) = 19.73, p &lt; .001, η p 2 = .37 (Fig. <ref type="figure" target="#fig_4">4b</ref>). Boundary extension was reliable for the large-aperture condition (M = -0.08, SE = 0.04, 95% CI = [-0.15, -0.01]), t(34) = -2.23, p = .03, d = -0.38, but was no longer observed for the small-aperture condition (M = -0.002, SE = 0.04, 95% CI = [-0.08, 0.07]), t(34) = -0.06, p = .96, d = -0.01, Bayes Factor favoring the null over the alternative hypothesis (BF 01 ) = 5.51. We did not observe a main effect of aperture, F(1, 34) = 1.42, p = .24, η p 2 = .04, or a main effect of view change, F(1, 34) = 1.38, p = .25, η p 2 = .04. A similar pattern was observed for inverse efficiency scores (RT/% correct), confirming that results were not due to speed/accuracy trade-offs, and for confidence ratings (see the Supplemental Material).</p><p>These results generalized the effects of depth of field on boundary extension to a new stimulus set consisting of distant scene views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>In four experiments, we found that boundary extension depends on a photograph's depth of field. In Experiment 1, across a large and variable image set, depth of field was a strong predictor of boundary extension, with boundary extension being largest for images with shallow depth of field, resembling human vision. By contrast, boundary contraction was only reliably observed for images with deep, unnaturalistic depth of field. Three controlled experiments showed that depth of field modulated boundary extension even when other properties of the scene were kept constant across conditions. Altogether, these results demonstrate that boundary extension is reliably observed for ecologically representative stimuli.</p><p>Because depth of field may covary with perceived distance across photographs, we ensured that the relationship between depth of field and boundary extension observed here could not be explained by differences in perceived distance between shallow and deep depth of field. First, in Experiment 1, the relationship between depth of field and boundary extension remained reliable after we regressed out subjective distance ratings. Second, in Experiments 2 and 3, focus distance did not interact with depth of field, even though for large apertures, focus distance was clearly visible (Fig. <ref type="figure" target="#fig_1">2</ref>). It should be noted, however, that previous work showed that boundary extension is larger for scenes that show closer compared with faraway views <ref type="bibr" target="#b1">(Bainbridge &amp; Baker, 2020;</ref><ref type="bibr" target="#b4">Bertamini et al., 2005;</ref><ref type="bibr" target="#b12">Hafri et al., 2022;</ref><ref type="bibr" target="#b19">Intraub et al., 1992;</ref><ref type="bibr">Intraub &amp; Dickinson, 2008;</ref><ref type="bibr" target="#b31">Park et al., 2021)</ref>. These results suggest an additional role for normalization processes. This more generic normalization process (also observed for objects) could exist independently of the more scene-specific predictive mechanism driving boundary extension <ref type="bibr" target="#b22">(Intraub et al., 1996</ref><ref type="bibr" target="#b21">(Intraub et al., , 1998))</ref>. Experiment 4 supports this idea, showing no boundary extension for distant scene views with deep depth of field, suggesting that the opposite effects of normalization (leading to boundary contraction) and predictive (leading to boundary extension) processes cancelled each other out for this condition. Importantly, predictive processes were strong for naturalistic (shallow) depth of field, leading to boundary extension even for distant scene views. When integrating our results with previous findings, we see that the largest boundary extension is observed for photographs with a shallow depth of field taken from a relatively close distance and with the main object seen from a central vantage point <ref type="bibr" target="#b10">(Gagnier et al., 2011)</ref>. What do all these aspects have in common? One possibility is that images shot under these conditions best resemble how we naturally sample the visual world. Indeed, images with these characteristics show a more partial view of the scene than images with a deep depth of field, wide view, and lateral vantage point. These partial views are more likely to require integration of visual input: When the image is less complete, observers may need to rely more on top-down expectations of scene layout, drawing on sources other than the visual input to complete the percept.</p><p>Another aspect that photographs with deep depth of field, shot from a great distance, and shot from a lateral vantage point have in common is that they typically contain many visible objects. To perceptually encode and remember such scenes requires greater attentional resources. This may lead to a loss of peripheral image content, resulting in boundary contraction. Furthermore, fewer resources will be available for predicting what is beyond the scene's boundaries, thus reducing boundary extension. Nevertheless, our results suggest that the depth-of-field effects on boundary extension observed here cannot be fully accounted for by image complexity. First, the relationship between depth of field and boundary scores remained reliable even when we regressed out the number of objects in the images (see the Supplemental Material). Second, in Experiments 2 and 3, depth of field affected boundary extension regardless of whether the camera's focus was on the background (showing a larger number of objects) or on the foreground.</p><p>Given our results, we propose that boundary extension reflects a constructive mechanism with adaptive value that is conditional to a scene being naturalistic. An image shown at a plausible distance from the observer, with a depth of field resembling the day-today perceptual experience, will likely lead to extrapolation of scene layout from memory and, therefore, boundary extension. When one or more of these properties are removed, boundary extension is attenuated. Future research is needed to identify those conditions of the external visual input that allow the mental representation of scenes, stored in memory, to complete external percepts.</p><p>By showing stronger boundary extension for images with more naturalistic depth of field, our results support the view that boundary extension is a scene-perception phenomenon rather than a phenomenon specific to photographs <ref type="bibr" target="#b17">(Intraub, 2012)</ref>. This is in line with previous evidence showing that boundary extension is independent of photographical artifacts (e.g., image magnification; <ref type="bibr" target="#b4">Bertamini et al., 2005)</ref> and that it occurs across modalities <ref type="bibr" target="#b23">(Intraub et al., 2015)</ref>. Nevertheless, the viewing conditions in our experiments differed substantially from our real-world visual experience. Our stimuli were brief static views, did not cover the full visual field, and lacked binocular depth cues, and focus distance was not yoked to the observer's fixation in the scene. Future studies are needed to test boundary extension for more naturalistic viewing conditions, for example using virtual reality <ref type="bibr" target="#b34">(Snow &amp; Culham, 2021)</ref>. We predict that boundary extension will be reliably observed under those conditions.</p><p>Finally, our results have broad implications for researchers aiming to study natural vision. Many recent psychological, neuroscientific, and computer vision studies implement large stimulus sets to achieve higher ecological validity (e.g., <ref type="bibr" target="#b1">Bainbridge &amp; Baker, 2020;</ref><ref type="bibr" target="#b6">Chang et al., 2019;</ref><ref type="bibr" target="#b13">Hebart et al., 2020;</ref><ref type="bibr" target="#b29">Mehrer et al., 2021)</ref>. Our findings indicate that such large image sets are not necessarily representative of how observers perceive their visuospatial world. Indeed, there is evidence that decreasing a scene's realism impacts visual memory <ref type="bibr" target="#b35">(Tatler &amp; Melcher, 2007)</ref>. By showing that depth of field can drastically change the influence of top-down knowledge on visual processing, our results imply that it is important to use images with naturalistic depth of field in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency</head><p>Action Editor: Sachiko Kinoshita Editor: Patricia J. Bauer</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Results from Experiment 1. The scatterplot (a; with best-fitting linear regression line) shows the relation between aperture ratings and boundary transformation scores across the two 1,000-image experiments of Bainbridge and Baker (2020). Images that participants rated as having been shot with large apertures led to strong boundary extension. Images that participants rated as having been shot with small apertures led to less boundary extension or even contraction. The yellow error band around the regression line represents the 95% confidence interval. The bar plot (b) shows average boundary transformation scores across images for each of five aperture bins. Professional photographers judged images in the small (S) and extra small (XS) bins to have been shot with apertures between f/8 and f/22, as computed from comparisons with aperture judgments of pictures with known aperture levels (see the Supplemental Material available online). These apertures are mostly impossible for the human eye. Each data point represents the mean boundary transformation value per image. Error bars represent standard errors of the mean. XL = extra large, L = large, M = medium.</figDesc><graphic coords="4,57.84,73.66,251.78,251.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of the experimental conditions in Experiments 2 and 3, illustrated by a zoomed-in portion of one of the scene photographs. Each outdoor scene image was shot focusing either on a foreground object or on the background at two levels of aperture: f/5.6 (shallow depth of field [DOF]) and f/22 (deep DOF). The full stimulus set, including the images equated for spatial frequency and histogram used in Experiment 3, is publicly available at https://osf.io/7btfq/.</figDesc><graphic coords="5,315.75,69.00,234.00,317.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>, 34) = 44.20, p &lt; .001, η p 2 = .57; Experiment 3: F(1, 31) = 26.69, p &lt; .001, η p 2 = .46. View changes from wide to close (Experiment 2: M = 0.80, SE = 0.02; Experiment 3: M = 0.81, SE = 0.02) were detected more accurately than view changes from close to wide (Experiment 2: M = 0.63, SE = 0.02; Experiment 3: M = 0.65, SE = 0.02). This effect indicates that boundaries were extended in memory when averaging across conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Schematic representation of the procedure of Experiments 2, 3, and 4 (a) and results from Experiments 2 and 3 (b). The example trial sequence (a) shows a wide-to-close trial from Experiment 3. The second (probe) picture shows a closer view than the first picture. Dyn = dynamic. For the bar plot (b), the view-asymmetry index was computed as the difference in change-detection accuracy for view changes (close to wide vs. wide to close). Negative values represent higher accuracy in detecting a change from wide to close, indicative of boundary extension. In both experiments, there was an effect of aperture across both levels of focus distance. Error bars represent standard errors of the mean. DOF = depth of field.</figDesc><graphic coords="7,358.17,239.46,94.10,70.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of two scenes from Experiment 4 with shallow and deep depth of field (DOF; a) and results from Experiment 4 (b). For the bar plot (b), the view-asymmetry index was computed as the difference in change-detection accuracy for view changes (close to wide vs. wide to close). Negative values represent higher accuracy in detecting a change from wide to close, indicative of boundary extension. This effect was present for the shallow depth of field and absent for the deep depth of field photographs. Error bars represent standard errors of the mean.</figDesc><graphic coords="9,466.74,213.96,69.62,130.10" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Surya Gayet</rs>, <rs type="person">Simen Hagen</rs>, <rs type="person">Genevieve Quek</rs>, and <rs type="person">Lu-Chun Yeh</rs> for feedback on earlier versions of this article and the other <rs type="institution">Peelen Lab</rs> members for helpful discussions and feedback during lab meetings.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This project has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="programName">Marie Skłodowska-Curie</rs> (grant agreement No. <rs type="grantNumber">101033489</rs>), and <rs type="funder">European Research Council (ERC)</rs> under the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> (grant agreement No <rs type="grantNumber">725970</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Y9vn9nr">
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_x3EdQE2">
					<idno type="grant-number">101033489</idno>
					<orgName type="program" subtype="full">Marie Skłodowska-Curie</orgName>
				</org>
				<org type="funding" xml:id="_XZ7mtyr">
					<idno type="grant-number">725970</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>Marco Gandolfo: conceptualization, data curation, formal analysis, investigation, methodology, resources, software, visualization, writing -original draft, writing -review &amp; editing.</p><p>Hendrik Nägele: resources. Marius V. Peelen: conceptualization, funding acquisition, methodology, supervision, writing -review &amp; editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Additional supporting information can be found at <ref type="url" target="http://journals.sagepub.com/doi/suppl/10.1177/09567976221140341">http://  journals.sagepub.com/doi/suppl/10.1177/09567976221140341</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optics of the eye and its impact in vision: A tutorial</title>
		<author>
			<persName><forename type="first">P</forename><surname>Artal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Optics and Photonics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="340" to="367" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boundaries extend and contract in scene memory depending on image properties</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="537" to="543" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">imager: An R package for image processing based on CImg</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barthelmé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tschumperlé</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.01012</idno>
		<ptr target="https://doi.org/10.21105/joss.01012" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page">1012</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Bartlett</surname></persName>
		</author>
		<title level="m">Remembering: A study in experimental and social psychology</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1932">1932</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boundary extension: The role of magnification, object size, context, and binocular information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spooner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1288" to="1307" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A review of visual memory capacity: Beyond individual items and toward structured representations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Konkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="DOI">10.1167/11.5.4</idno>
		<ptr target="https://doi.org/10.1167/11.5" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BOLD5000, a public fMRI dataset while viewing 5000 visual images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Pyles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Tarr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Aminoff</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-019-0052-3</idno>
		<ptr target="https://doi.org/10.1038/s41597-019-0052-3" />
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">jsPsych: A JavaScript library for creating behavioral experiments in a Web browser</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>De Leeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transsaccadic representation of layout: What is the time course of boundary extension?</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="543" to="555" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gnu Octave (Version 5.1.0)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<ptr target="https://octave.org/" />
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Computer software</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Why does vantage point affect boundary extension?</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Gagnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="257" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Surface construal and the mental representation of scenes</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Gottesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="589" to="599" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceived distance alters memory for scene boundaries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hafri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Bonner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2040" to="2058" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revealing the multidimensional mental representations of natural objects underlying human similarity judgements</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Hebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Hecht</surname></persName>
		</author>
		<title level="m">Optics</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boundary extension: Findings and theories</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Courtney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1467" to="1494" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rethinking scene perception: A multisource model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The psychology of learning and motivation</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Ross</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="231" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking visual scene perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Searching for boundary extension</title>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="1463" to="R1464" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Looking at pictures but remembering scenes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Mangels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">180</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">False memory 1/20th of a second later: What the early onset of boundary extension reveals about perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1007" to="1014" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effects of perceiving and imagining scenes on memory for pictures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Gottesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="186" to="201" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Boundary extension for briefly glimpsed photographs: Do common perceptual processes result in unexpected memory distortions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Gottesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Willey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Zuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="118" to="134" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual, haptic and bimodal scene perception: Evidence for a unitary representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Gagnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="132" to="147" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wide-angle memories of close-up scenes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="187" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Normative representation of objects: Evidence for an ecological bias in object perception and memory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Konkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="407" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Open Images dataset V4</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The hippocampus: A manifesto for change</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Mullally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Seeking the boundary of boundary extension</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Mcdunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="370" to="375" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An ecologically motivated image dataset for deep learning yields better models of human vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Spoerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Kietzmann</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2011417118</idno>
		<ptr target="https://doi.org/10.1073/pnas.2011417118" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2011417118</biblScope>
			<date type="published" when="2021">2021</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vision through the atmosphere</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E K</forename><surname>Middleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geophysik II [Geophysics II</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Bartels</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1957">1957</date>
			<biblScope unit="page" from="254" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Systematic transition from boundary extension to contraction along an object-to-scene continuum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Josephs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Konkle</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/84exs</idno>
		<ptr target="https://doi.org/10.31234/osf.io/84exs" />
	</analytic>
	<monogr>
		<title level="j">Psyarxiv Preprints</title>
		<imprint>
			<date type="published" when="2021-01-26">2021, January 26</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond the edges of a view: Boundary extension in human scene-selective visual cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Intraub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Widders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="342" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Version 4.0.1 Computer software</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The treachery of images: How realism influences brain and behavior</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Culham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="506" to="519" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pictures in mind: Initial encoding of object properties varies with the realism of the scene stimulus</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Tatler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Melcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1715" to="1729" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Controlling low-level image properties: The SHINE toolbox</title>
		<author>
			<persName><forename type="first">V</forename><surname>Willenbockel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sadr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fiset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">O</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="671" to="684" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SUN database: Exploring a large collection of scene categories</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
