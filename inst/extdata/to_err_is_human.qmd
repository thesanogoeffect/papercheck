---
title: 'To Err is Human: An Empirical Investigation'
author: "Daniel Lakens & Lisa DeBruine"
date: "2024-06-21"
format: 
  html: 
    embed-resources: true
    minimal: true
  pdf: default
  docx: default
execute: 
  echo: false
---

```{r}
#| label: setup
#| include: false

library(papercheck)
library(dplyr)
library(ggplot2)
library(faux)

# Set the seed for reproducibility
set.seed(1243)
```

This paper demonstrates some good and poor practices for use with the {papercheck} R package and Shiny app. All data are simulated. The paper shows examples of (1) open and closed OSF links; (2) citation of retracted papers; (3) missing/mismatched/incorrect citations and references; (4) imprecise reporting of p-values; and (5) use of "marginally significant" to describe non-significant findings.

## Introduction

Although intentional dishonestly might be a successful way to boost creativity (Gino & Wiltermuth, 2014), it is safe to say most mistakes researchers make are unintentional. From a human factors perspective, human error is a symptom of a poor design (Smithy, 2020). Automation can be use to check for errors in scientific manuscripts, and inform authors about possible corrections. In this study we examine the usefulness of Papercheck to improve best practices.

## Method and Participants

In this study we examine whether automated checks reduce the amount of errors that researchers make in scientific manuscripts. This study was preregistered at [osf.io/5tbm9](https://osf.io/5tbm9). We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, an 50 scientists to a control condition with a checklist. Scientists had the opportunity to make changes to their manuscript based on the feedback of the tool. We subsequently coded all manuscripts for mistakes, and counted the total number of mistakes. We also measured the expertise of researchers (in years) to explore whether the automated tool would be more useful, the less research experience researchers had. We also asked researchers to rate how useful they found the checklist or app on a scale from 1 (not at all) to 7 (extremely useful).

## Results

```{r}
#| label: fig-sim
#| fig-cap: The simulated data

# simulate data
data <- sim_design(
  n = 50,
  between = list(condition = c("control", "experimental")),
  within = list(vars = c("mistakes", "usefulness", "experience")),
  mu = c(12.5, 3.5, 10, 8.5, 5.5, 10),
  sd = c(12, 4, 25, 12, 4, 25),
  r = c(-.2, 0, .3),
  id = "ppnr",
  plot = T
) |>
  dplyr::mutate(mistakes = norm2pois(mistakes, 10),
                usefulness = norm2likert(usefulness, c(1,2,4,8,10,8,4)),
                experience = norm2pois(experience, 10))
```

All data needed to reproduce the analyses in @tbl-summary is available from <https://osf.io/5tbm9> and code is available from [the OSF](https://osf.io/629bx). 

```{r}
#| label: tbl-summary
#| tbl-cap: The average number of mistakes and usefulness score for the control and experimental conditions.

average_table <- data |>
  group_by(condition) |>
  summarise(average_mistakes = mean(mistakes) |> round(2),
            average_usefulness = mean(usefulness) |> round(2))

tm <- t.test(mistakes ~ condition, data = data)
tu <- t.test(usefulness ~ condition, data = data)

average_table |>
  knitr::kable(col.names = c("Condition", "Mistakes", "Usefulness"))
```

On average researchers in the experimental (app) condition made fewer mistakes (*M* = `r average_table$average_mistakes[[2]]`) than researchers in the control (checklist) condition (*M* = `r average_table$average_mistakes[[1]]`), *t*(`r round(tm$parameter, 1)`) = `r round(tm$statistic, 2)`, *p* = `r round(tm$p.value, 3)`. 

On average researchers in the experimental condition found the app marginally significantly more useful (*M* = `r average_table$average_usefulness[[2]]`) than researchers in the control condition found the checklist (*M* = `r average_table$average_usefulness[[1]]`), *t*(`r round(tu$parameter, 1)`) = `r round(tu$statistic, 2)`, *p* = `r round(tu$p.value + .1, 3)`. 


```{r}
# Calculate the correlation coefficient
correlation <- cor(data$mistakes, data$experience)

# Create a scatter plot with a regression line
ggplot(data, aes(x = mistakes, y = experience)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, col = "red") +
  labs(title = paste("Scatter Plot with Correlation: ", round(correlation, 2)),
       x = "Mistakes",
       y = "Experience") +
  theme_minimal()
```

There was no effect of experience on the reduction in errors when using the tool (*p* > .05), as the correlation was non-significant. 




## Discussion

It seems automated tools can help prevent errors by providing researchers with feedback about potential mistakes, and researchers feel the app is useful. We conclude the use of automated checks has potential to reduce the number of mistakes in scientific manuscripts. 

## References

Gino, F., & Wiltermuth, S. S. (2014). Retracted: Evil Genius? How Dishonesty Can Lead to Greater Creativity. Psychological Science, 25(4), 973â€“981. https://doi.org/10.1177/0956797614520714

Smith, F. (2021). Human error is a symptom of a poor design. Journal of Journals, 0(0), 0. https://doi.org/10.0000/0123456789

Lakens, D. (2018). Equivalence testing for psychological research. AMPPS, 1, 259-270. https://doi.org/10.1177/2515245918770963
