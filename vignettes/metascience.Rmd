---
title: "Using Papercheck for MetaScience"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using Papercheck for MetaScience}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>"
)

mytable <- function(tbl) {
  DT::datatable(tbl[, c("id", "text")], 
                rownames = FALSE,
                filter = "none",
                options = list(pageLength = 5, dom = "tip")
  )
}
```

```{r}
#| label: setup
#| message: false
library(papercheck)
library(readr) # reading and writing CSV files
library(dplyr) # for data wrangling
library(tidyr) # for data wrangling
library(ggplot2) # for dataviz
library(tidytext) 
```

## Initial Text Search

See the [batch processing vignette](batch.html) for information on how to load multiple PDFs. Here, we will load 250 open access papers from Psychological Science, which have been previously converted to XML by grobid and read in to papercheck. 

```{r}
papers <- psychsci
```

### Fixed Terms

Let's start with a fixed search term: "power analysis". We'll keep track of our iteratively developed search terms by naming the resulting table `text_#`. 

```{r}
text_1 <- search_text(papers, pattern = "power analysis")
```

Here we have `r nrow(text_1)` results. We'll just show the paper id and text columns of the returned table, but the table also provides the section type, header, and section, paragraph, and sentence numbers (div, p, and s).

```{r}
#| echo: false
mytable(text_1)
```

We caught a lot of sentences with that term, but are probably missing a few. Let's try a more general fixed search term: "power".

```{r}
text_2 <- search_text(papers, pattern = "power")
```

Here we have `r nrow(text_2)` results. Inspect them to see if there are any false positives.

```{r}
#| echo: false
mytable(text_2)
```

### Regex

After a quick skim through the `r nrow(text_2)` results, we can see that words like "powerful" are never reporting a power analysis, so we should try to exclude them.

We can use regex to make our text search a bit more specific. The following pattern requires that power is followed optionally by "ed" and then by a word border (like a space or full stop), so will match "power" and "powered", but not "powerful".

```{r}
# test some examples to check the pattern
pattern <- "power(ed)?\\b"
yes <- c("power",
          "power.",
          "power analysis",
          "powered")
no  <- c("powerful",
          "powerful analysis")
grepl(pattern, yes)
grepl(pattern, no)
```

```{r}
text_3 <- search_text(papers, pattern)
```

Here we have `r nrow(text_3)` results. Inspect them for false positives again. 

```{r}
#| echo: false
mytable(text_3)
```

### Refining the search

You can repeat this process of skimming the results and refining the search term iteratively until you are happy that you have probably caught all of the relevant text and don't have too many false positives. 

One useful technique is to use `dplyr::anti_join()` to check which text was excluded when you make a search term more specific, to make sure there are no or few false negatives. 

```{r}
# rows in text_2 that were excluded in text_3
excluded <- anti_join(text_2, text_3, 
                      by = c("id", "div", "p", "s"))
```

```{r}
#| echo: false
mytable(excluded)
```

### Screening

Once you are happy that your search term includes all of the relevant text and not too much irrelevant text, the next step is to save this data frame so you can open it in a spreadsheet application and code each row for ground truth.

```{r}
readr::write_csv(text_3, "power/power_screening.csv")
```


Be careful opening files in spreadsheet apps like Excel. Sometimes they will garble special characters like `ü` or \beta, which will make the validation process below inaccurate, since the expected values from your spreadsheet will not exactly match the calculated values from the modules you're testing. One way to fix this if it has happened, is to read the excel file into R and replace the `text` column with the `text` column from the data frame above, and re-save it as a CSV file. 

## Validating a Module

### Module Creation

To validate a module, you need to write your search term into a module. See the [modules vignette](modules.html) for details. Creating a module for a text search is very straightforward. Just save the following text in a file called "power_0.mod". The `traffic_light` entry returns "green" for a paper if any sentences are found, and "red" if none are found. 

``` json
{
  "title": "Power Analysis",
  "description": "List all sentences that contain a power analysis.",
  "text": {
    "pattern": "power(ed)?\\b"
  },
  "traffic_light": {
    "found": "green",
    "not_found": "red"
  }
}
```

Now test your module by running it on the papers. The returned table should be identical to `text_3`. 

```{r}
mod_test <- module_run(papers, "power/power0.mod")
all.equal(mod_test$table, text_3)
```


### Set Up Validation Files

Once you have the ground truth coded from your best inclusive search term, you can validate your module and start trying to improve its performance.

First, let's use the over-inclusive search term. This will, by definition, have no false negatives, but further refining of your module will start to produce both false positives and negatives. 

You have to set up two files to match the module output. First, the sample of files to check.

```{r}
sample <- data.frame(
  id = names(papers)
)
```

Second, a table of the expected text matches. You can get this by filtering your ground truth table to just the rows that are true positives (hand-coded here as the column `power_computation`).

```{r}
ground_truth <- readxl::read_excel("power/power_screening_coded.xlsx")
ground_truth$text <- text_3$text # fix problem with excel and special chars

expected <- ground_truth |>
  filter(power_computation == 1) |>
  select(id, text)
```

Add the traffic light to the sample table by determining if there are any matches in the results table.

```{r}
sample$traffic_light <- ifelse(
  sample$id %in% expected$id, 
  "green", "red")
```

### Validate

```{r}
v0 <- validate(module = "power/power0.mod",
               sample = sample, 
               expected = expected, 
               path = "xml")
```


```{r}
v0
```

### Refine and Iterate

Refine your module to improve it based on your coding of the ground truth. For example, perhaps we decide that almost all instances of real power analyses contain both the strings "power" and "analys"

```{r}
pattern <- "(analys.*power|power.*analys)"
yes <- c("power analysis",
         "power analyses",
         "power has an analysis",
         "analyse power",
         "analysis is powered at")
no  <- c("powered",
         "power",
         "analysis")
grepl(pattern, yes)
grepl(pattern, no)
```

Duplicate the file "power.mod" as "power1.mod" and change the search pattern to this new one and re-run the validation.

```{r}
v1 <- validate(module = "power/power1.mod",
               sample = sample, 
               expected = expected, 
               path = "xml")
```


```{r}
v1
```

Now we are only matching `r round(v1$tables_matched *100)`% of the tables. There are a few ways to investigate this.

Traffic lights can be more complex than "green" and "red", since they can also return values like "info", or "na". But if your values map straightforwardly onto yes and no, you can calculate signal detection measures for your module.

```{r}
tl_accuracy(v1, yes = "green", no = "red") |> str()
```

You can plot the number of missing and extra results per paper to see if the problem is in false positives and/or false negatives.

```{r}
#| warning: false
ggplot(v1$sample) +
  geom_freqpoly(aes(x = misses), color = "red", binwidth = 1) +
  geom_freqpoly(aes(x = false_alarms), color = "blue", binwidth = 1) +
  scale_x_continuous(breaks = 0:10, limits = c(0, 10)) +
  labs(x = "Number of Results Missing (red) or Extra (blue) per Paper", 
       y = "Number of Papers")
```

Both seem to be happening here, so let's look more specifically at false alarms and misses.

```{r}
false_alarms <- anti_join(v1$table, expected, 
                          by = c("id", "text"))
```

```{r}
#| echo: false
mytable(false_alarms)
```


```{r}
misses <- anti_join(expected, v1$table, 
                          by = c("id", "text"))
```

```{r}
#| echo: false
mytable(misses)
```

### Multi-step Text Search

Sometime it makes more sense to filter a set down in steps, such as all sentences that contain the word "power" and then all of those that contain an equal sign and at least one number in the format ".#" or "#%".

```{r}
results <- papers |>
  search_text("power(ed)?\\b") |>
  search_text("(\\.[0-9]|[0-9]%)")
```

You can add chained text searches to the JSON module file like this:

``` json
{
  "title": "Power Analysis",
  "description": "List all sentences that contain the string 'power' and a number.",
  "text": {
    "pattern": "power(ed)?\\b"
  },
  "text": {
    "pattern": "(\\.[0-9]|[0-9]%)"
  },
  "traffic_light": {
    "found": "green",
    "not_found": "red"
  }
}
```



```{r}
v2 <- validate(module = "power/power2.mod",
               sample = sample, 
               expected = expected, 
               path = "xml")
```


```{r}
v2
```

```{r}
tl_accuracy(v2) |> str()
```

```{r}
false_alarms2 <- anti_join(v2$table, expected, 
                          by = c("id", "text"))
```

```{r}
#| echo: false
mytable(false_alarms2)
```


```{r}
misses2 <- anti_join(expected, v2$table, 
                          by = c("id", "text"))
```

```{r}
#| echo: false
mytable(misses2)
```

## Compare modules

```{r}
data.frame(
  module = c(v0$module, v1$module, v2$module),
  tables = c(v0$table_matched, v1$table_matched, v2$table_matched),
  traffic_lights = c(v0$tl_matched, v1$tl_matched, v2$tl_matched)
)
```

## Regression Classification

You can build a simple regression model to predict the classification of text strings.

### Get important words

You can use any method for finding the words you want to use in your model, but papercheck has a built-in function to find the words that are most distinctive in your two classification groups. 

```{r}
words <- distinctive_words(
  text = ground_truth$text,
  classification = ground_truth$power_computation,
  n = 10
)

words
```

You can get rid of words that you think will be irrelevant (even if they are predictive of classification in this data set). We also set the `numbers` argument to "specific" to see if there are any specific numbers associated with power analyses.

```{r}
words <- distinctive_words(
  text = ground_truth$text,
  classification = ground_truth$power_computation,
  n = 10,
  numbers = "specific",
  stop_words = c("the", "a", "of", "an", "and")
)

words
```

### Train a model

First, code the features of your ground truth text using `text_features()`. This will give you a data frame that codes 0 or 1 for the absence or presence of each word or feature.

```{r}
has_symbols <- c(has_equals = "=", 
                 has_percent = "%")

features <- text_features(
  text = ground_truth$text, 
  words = words,
  has_symbol = has_symbols 
)

# show the first row
features[1, ] |> str()
```

You can then use this coded data to train a model. Here, we're using a simple binomial logistic regression to predict the classification from all of the features.

```{r}
# Train logistic regression model
model <- glm(ground_truth$power_computation ~ .,
             data = features,
             family = "binomial")

summary(model)
```

You can use any model you like and any method to assess and choose the best model.

### Predict Classification

Now you can classify any text using this model. Here, we're going to predict the classification of the original training data. Use `text_features()` to get the feature data and `predict()` to return the model response, and compare this result to a threshold (here 0.5) to generate the predicted classification.

```{r}
ground_truth$model_response <- 
  predict(model, features)

ground_truth$power_computation_predict <-
  ground_truth$model_response > 0.5

count(ground_truth, power_computation, power_computation_predict)
```

<!--

### Create a module

To create a module for this, you will need to save your model and create a module with code.

```{r, eval = FALSE}
saveRDS(model, "power/power_log_model.Rds")
```


```
{
  "title": "Classify with Log Model",
  "description": "",
  "code": {
    "packages": ["papercheck"],
    "code": "power_log_model.R"
  }
}
```


```{r, eval = FALSE}

words <- c("size", "effect", "sample", 
           "participants", "detect", "analysis",
           "medium", "power", "faul", "respondents", 
           "α", "cohen's", "research", 
           "version", "experiment")

has_symbols <- c(has_equals = "=", 
                 has_percent = "%")

readRDS("power/power_log_model.Rds")

table <- paper |>
  papercheck::search_text("power(ed)?\\b") |>
  papercheck::search_text("(\\.[0-9]|[0-9]%)")

features <- papercheck::text_features(
  table$text, words, has_symbols = has_symbols
)

table$model_response <- predict(model, features)
table$model_classification <- table$model_response > 0.5

traffic_light <- "red"
if (any(table$model_classification)) traffic_light <- "green"

# return
list(
  table = table,
  traffic_light = traffic_light
)

```

```{r, eval = FALSE}
v4 <- module_run(psychsci, "power/power_log_model.mod")
```


-->



