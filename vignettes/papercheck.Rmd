---
title: "papercheck"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{papercheck}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>"
)
```


## Installation

You can install the development version of papercheck from [GitHub](https://github.com/scienceverse/papercheck) with:

``` r
# install.packages("devtools")
devtools::install_github("scienceverse/papercheck")
```

```{r}
library(papercheck)
```

You can launch an interactive shiny app version of the code below with:

``` r
papercheck_app()
```

### Load from PDF

The function `pdf2grobid()` can read PDF files and save them in the [TEI](https://tei-c.org/) format created by [grobid](https://grobid.readthedocs.io/). This requires an internet connection and takes a few seconds per paper, so should only be done once and the results saved for later use.

If the server is unavailable, you can [use a grobid web interface](https://huggingface.co/spaces/kermitt2/grobid).

```{r, eval = FALSE}
pdf_file <- demopdf()
xml_file <- pdf2grobid(pdf_file)
```

```{r, include = FALSE}
# doesn't require a call to the grobid server
xml_file <- demoxml()
```

You can set up your own local grobid server following instructions from <https://grobid.readthedocs.io/>. The easiest way is to use Docker.

``` bash
docker run --rm --init --ulimit core=0 -p 8070:8070 lfoppiano/grobid:0.8.1
```

Then you can set your grobid_url to the local path <http://localhost:8070>.

```{r, eval = FALSE}
xml_file <- pdf2grobid(pdf_file, grobid_url = "http://localhost:8070")
```


### Load from XML

The function `read_grobid()` can read XML files parsed by grobid.

```{r}
paper <- read_grobid(xml_file)
```

The function `read_cermine()` can read XML files parsed by [cermine](http://cermine.ceon.pl). This is not as good as grobid at parsing papers, and omits figure and table captions.

```{r}
#| eval: false
cermine_xml_file <- system.file("psychsci/0956797620955209.cermine.xml",
                                package = "papercheck")
paper <- read_cermine(cermine_xml_file)
```

### Load from non-PDF document

To take advantage of grobid's ability to parse references and other aspects of papers, for now the best way is to convert your papers to PDF. However, papercheck can read in plain text from a character object or text/docx file with `read_text()`.

```{r}
text <- "Abstract

This is my very short paper. It has two sentences."
shortpaper <- read_text(text, id = "shortpaper")
shortpaper$full_text
```


```{r}
#| eval: false
filename <- system.file("extdata/to_err_is_human.docx", 
                        package = "papercheck")
paper_from_doc <- read_text(filename)
```


### Batch Processing

The functions `pdf2grobid()` and `read_grobid()` also work on a folder of files, returning a list of XML file paths or paper objects, respectively. The functions `search_text()`, `expand_text()` and `llm()` also work on a list of paper objects.

```{r}
grobid_dir <- demodir()

papers <- read_grobid(grobid_dir)

hypotheses <- search_text(papers, "hypothesi", 
                          section = "intro", 
                          return = "paragraph")
```

## Paper Components

Paper objects contain a lot of structured information, including info, references, and citations.

### Info

```{r}
paper$info
```

### References

References are provided in a tabular format.

```{r, eval = FALSE}
paper$references
```

```{r, echo = FALSE}
knitr::kable(paper$references)
```

### Citations

Citations are also provided in a tabular format, with a bib_id to match the references.

```{r, eval = FALSE}
paper$citations
```

```{r, echo = FALSE}
knitr::kable(paper$citations)
```

### Batch

There are functions to combine the infomation from a list of papers, like the `psychsci` built-in dataset of 250 open access papers from Psychological Science.

```{r}
info_table(psychsci[1:5], c("title", "doi"))
```


```{r}
concat_tables(psychsci[1:5], "references") |>
  dplyr::filter(!is.na(doi))
```

```{r}
concat_tables(psychsci[1:40], "citations") |>
  dplyr::filter(grepl("replicat", text)) |>
  dplyr::count(id, text)
```

## Search Text

You can access a parsed table of the full text of the paper via `paper$full_text`, but you may find it more convenient to use the function `search_text()`. The defaults return a data table of each sentence, with the section type, header, div, paragraph and sentence numbers, and file name. (The section type is a best guess from the headers, so may not always be accurate.)

```{r}
text <- search_text(paper)
```

```{r, echo = FALSE}
dplyr::filter(text, p == 1, s == 1) |>
  knitr::kable()
```

### Pattern

You can search for a specific word or phrase by setting the `pattern` argument. The pattern is a regex string by default; set `fixed = TRUE` if you want to find exact text matches.

```{r}
text <- search_text(paper, pattern = "papercheck")
```

```{r, echo = FALSE}
knitr::kable(text)
```

### Section

Set `section` to a vector of the sections to search in.

```{r}
text <- search_text(paper, "papercheck", 
                    section = "abstract")
```

```{r, echo = FALSE}
knitr::kable(text)
```


### Return

Set `return` to one of "sentence", "paragraph", "section", or "match" to control what gets returned.

```{r}
text <- search_text(paper, "papercheck", 
                    section = "intro", 
                    return = "paragraph")
```

```{r, echo = FALSE}
knitr::kable(text)
```


### Regex matches

You can also return just the matched text from a regex search by setting `return = "match"`. The extra `...` arguments in `search_text()` are passed to `grep()`, so `perl = TRUE` allows you to use more complex regex, like below.

```{r}
pattern <- "[a-zA-Z]\\S*\\s*(=|<)\\s*[0-9\\.,-]*\\d"
text <- search_text(paper, pattern, return = "match", perl = TRUE)
```

```{r, echo = FALSE}
knitr::kable(text)
```

### Expand Text

You can expand the text returned by `search_text()` or a module with `expand_text()`.

```{r}
marginal <- search_text(paper, "marginal") |>
  expand_text(paper, plus = 1, minus = 1)

marginal[, c("text", "expanded")]
```



## Large Language Models

You can query the extracted text of papers with LLMs using [groq](https://console.groq.com/docs/).

<a href="https://groq.com" target="_blank" rel="noopener noreferrer">
  <img
    src="https://groq.com/wp-content/uploads/2024/03/PBG-mark1-color.svg"
    alt="Powered by Groq for fast inference."
  />
</a>

### Setup


You will need to get your own API key from <https://console.groq.com/keys>. To avoid having to type it out, add it to the .Renviron file in the following format (you can use `usethis::edit_r_environ()` to access the .Renviron file).

``` bash
GROQ_GPT_KEY="sk-proj-abcdefghijklmnopqrs0123456789ABCDEFGHIJKLMNOPQRS"
```

```{r, eval=FALSE}
# useful if you aren't sure where this file is
usethis::edit_r_environ()
```

You can get or set the default LLM model with `llm_model()` and access a list of the current available models using `llm_model_list()`. 

```{r, echo = FALSE}
data.frame(
  id = c("mistral-saba-24b", "qwen-2.5-coder-32b", 
"deepseek-r1-distill-qwen-32b", "qwen-2.5-32b", "deepseek-r1-distill-llama-70b", 
"llama-3.3-70b-specdec", "llama-3.3-70b-versatile", "llama-3.2-3b-preview", 
"llama-3.2-1b-preview", "llama-guard-3-8b", "mixtral-8x7b-32768", 
"llama-3.1-8b-instant", "llama3-8b-8192", "gemma2-9b-it", "llama3-70b-8192"
), 
  owned_by = c("Mistral AI", "Alibaba Cloud", "DeepSeek / Alibaba Cloud", 
"Alibaba Cloud", "DeepSeek / Meta", "Meta", "Meta", "Meta", "Meta", 
"Meta", "Mistral AI", "Meta", "Meta", "Google", "Meta"), 
  created = c("2025-02-19", 
"2025-02-14", "2025-02-07", "2025-02-05", "2025-01-26", "2024-12-06", 
"2024-12-06", "2024-09-25", "2024-09-25", "2023-09-03", "2023-09-03", 
"2023-09-03", "2023-09-03", "2023-09-03", "2023-09-03"), 
  context_window = c(32768L, 
131072L, 131072L, 131072L, 131072L, 8192L, 32768L, 8192L, 8192L, 
8192L, 32768L, 131072L, 8192L, 8192L, 8192L)
) |> knitr::kable()
```


### LLM Queries

You can query the extracted text of papers with LLMs using [groq](https://console.groq.com/docs/). See `?llm` for details of how to get and set up your API key, choose an LLM, and adjust settings. 

Use `search_text()` first to narrow down the text into what you want to query. Below, we limited search to the first ten papers' method sections, and returned sentences that contains the word "power" and at least one number. Then we asked an LLM to determine if this is an a priori power analysis, and if so, to return some relevant values in a JSON-structured format.

```{r, eval = FALSE}
power <- psychsci[1:10] |>
  # sentences containing the word power
  search_text("power", section = "method") |>
  # and containing at least one number
  search_text("[0-9]") 

# ask a specific question with specific response format
query <- 'Does this sentence report an a priori power analysis? If so, return the test, sample size, critical alpha criterion, power level, effect size and effect size metric plus any other relevant parameters, in JSON format like:

{
  "apriori": true, 
  "test": "paired samples t-test", 
  "sample": 20, 
  "alpha": 0.05, 
  "power": 0.8, 
  "es": 0.4, 
  "es_metric": "cohen\'s D"
}

If not, return {"apriori": false}

Answer only in valid JSON format, starting with { and ending with }.'

llm_power <- llm(power, query, seed = 8675309)
```


```{r, echo = FALSE}
# dput(llm_power)
llm_power <- structure(list(text = c("Sample size was calculated with an a priori power analysis, using the effect sizes reported by KÃ¼pper et al. (2014), who used identical procedures, materials, and dependent measures.", 
"We determined that a minimum sample size of 7 per group would be necessary for 95% power to detect an effect.", 
"For the first part of the task, 11 static visual images, one from each of the scenes in the film were presented once each on a black background for 2 s using Power-Point.", 
"A sample size of 26 per group was required to ensure 80% power to detect this difference at the 5% significance level.", 
"A sample size of 18 per condition was required in order to ensure an 80% power to detect this difference at the 5% significance level.", 
"The 13,500 selected loan requests conservatively achieved a power of .98 for an effect size of .07 at an alpha level of .05.", 
"On the basis of simulations over a range of expected effect sizes for contrasts of fMRI activity, we estimated that a sample size of 24 would provide .80 power at a conservative brainwide alpha threshold of .002 (although such thresholds ideally should be relaxed for detecting activity in regions where an effect is predicted).", 
"Stimulus sample size was determined via power analysis of the sole existing similar study, which used neural activity to predict Internet downloads of music (Berns & Moore, 2012).", 
"The effect size from that study implied that a sample size of 72 loan requests would be required to achieve .80 power at an alpha level of .05."
), section = c("method", "method", "method", "method", "method", 
"method", "method", "method", "method"), header = c("Participants", 
"Participants", "Tasks and measures", "Intrusion-provocation task (IPT).", 
"Method", "Internet study", "Neuroimaging study", "Neuroimaging study", 
"Neuroimaging study"), div = c(3, 3, 4, 5, 10, 3, 4, 4, 4), p = c(1, 
1, 2, 13, 3, 2, 5, 5, 5), s = c(2L, 3L, 3L, 3L, 3L, 8L, 3L, 7L, 
8L), id = c("0956797615569889", "0956797615569889", "0956797615583071", 
"0956797615583071", "0956797615583071", "0956797615588467", "0956797615588467", 
"0956797615588467", "0956797615588467"), answer = c("{\n  \"apriori\": true\n}", 
"{\n  \"apriori\": true, \n  \"test\": null, \n  \"sample\": 7, \n  \"alpha\": null, \n  \"power\": 0.95, \n  \"es\": null, \n  \"es_metric\": null\n}", 
"{\"apriori\": false}", "{\n  \"apriori\": true, \n  \"test\": \"two-sample t-test\", \n  \"sample\": 26, \n  \"alpha\": 0.05, \n  \"power\": 0.8\n}", 
"{\n  \"apriori\": true, \n  \"test\": \"t-test\", \n  \"sample\": 18, \n  \"alpha\": 0.05, \n  \"power\": 0.8\n}", 
"{\n  \"apriori\": true, \n  \"test\": null, \n  \"sample\": 13500, \n  \"alpha\": 0.05, \n  \"power\": 0.98, \n  \"es\": 0.07, \n  \"es_metric\": null\n}", 
"{\n  \"apriori\": true, \n  \"test\": \"unknown\", \n  \"sample\": 24, \n  \"alpha\": 0.002, \n  \"power\": 0.8, \n  \"es\": \"unknown\", \n  \"es_metric\": \"unknown\"\n}", 
"{\"apriori\": true}", "{\n  \"apriori\": true, \n  \"test\": null, \n  \"sample\": 72, \n  \"alpha\": 0.05, \n  \"power\": 0.8, \n  \"es\": null, \n  \"es_metric\": null\n}"
), time = c(0.054125226, 0.221003837, 0.065295072, 0.191989067, 
0.184921361, 0.248274644, 0.345732381, 0.046376265, 0.232392039
), tokens = c(225L, 259L, 224L, 254L, 255L, 272L, 302L, 221L, 
269L)), row.names = c(NA, -9L), class = c("ppchk_llm", "data.frame"
), llm = list(messages = list(list(role = "system", content = "Does this sentence report an a priori power analysis? If so, return the test, sample size, critical alpha criterion, power level, effect size and effect size metric plus any other relevant parameters, in JSON format like:\n{\n  \"apriori\": true, \n  \"test\": \"paired samples t-test\", \n  \"sample\": 20, \n  \"alpha\": 0.05, \n  \"power\": 0.8, \n  \"es\": 0.4, \n  \"es_metric\": \"cohen's D\"\n}\nIf not, return {\"apriori\": false}\nAnswer only in valid JSON format, starting with { and ending with }."), 
    list(role = "user", content = "")), model = "llama-3.3-70b-versatile", 
    temperature = 0.5, max_completion_tokens = 1024L, top_p = 0.95, 
    seed = 8675309, stream = FALSE, stop = NULL))

```

### Expand JSON

It is useful to ask an LLM to return data in JSON structured format, but can be frustrating to extract the data, especially where the LLM makes syntax mistakes. The function `json_expand()` tries to expand a column with a JSON-formatted response into columns and deals with it gracefully (sets an 'error' column to "parsing error") if there are errors. It also fixes column data types, if possible. 

```{r}
llm_response <- json_expand(llm_power, "answer") |>
  dplyr::select(text, apriori:es_metric)
```

```{r}
#| echo: false

knitr::kable(llm_response)
```

### Rate Limiting

Remember, the `llm()` function makes a separate query for each row in a data frame from `search_text()`. Free GROQ accounts are rate limited, so we set the default limits to 30 queries, but you can change this:

```{r}
llm_max_calls(30)
```

If you hit your rate limit, the `llm()` function will add a short delay between calls, so don't worry if you notice the query speed slowing down after the first 30 calls. 


## Modules

Papercheck is designed modularly, so you can add modules to check for anything. It comes with a set of pre-defined modules, and we hope people will share more modules.

### Module List

You can see the list of built-in modules with the function below.

```{r}
module_list()
```

### Running modules

To run a built-in module on a paper, you can reference it by name.

```{r}
p <- module_run(paper, "all_p_values")
```

```{r, echo = FALSE}
knitr::kable(p$table)
```

### Creating modules

You can create your own modules using R code. Modules can also contain instructions for reporting, to give "traffic lights" for whether a check passed or failed, and to include appropriate text feedback in a report. See the [modules vignette](modules.html) for more details. 


## Reports

You can generate a report from any set of modules. The default set is `c("exact_p", "marginal", "effect_size", "osf_check", "retractionwatch", "ref_consistency")`

```{r, eval = FALSE}
report(paper, output_format = "qmd")
```

See the [example report](report-example.html).




