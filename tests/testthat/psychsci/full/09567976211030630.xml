<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Naturalistic Face Learning in Infants and Adults</title>
				<funder ref="#_uznu7C3">
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xiaomei</forename><surname>Zhou</surname></persName>
							<email>x.zhou@ryerson.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Ryerson University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shruti</forename><surname>Vyas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Ryerson University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinbiao</forename><surname>Ning</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Margaret</forename><forename type="middle">C</forename><surname>Moulson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Ryerson University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Ryerson University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Naturalistic Face Learning in Infants and Adults</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A9BE82CB0F0F2AE0511B878F51A6D237</idno>
					<idno type="DOI">10.1177/09567976211030630</idno>
					<note type="submission">Received 12/10/20; Revision accepted 5/23/21</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-06-03T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>infancy</term>
					<term>visual development</term>
					<term>face recognition</term>
					<term>eye tracking</term>
					<term>within-person variability</term>
					<term>facial feature tracking</term>
					<term>open data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>On a daily basis, humans rely on the ability to process information from faces to effectively navigate their social world. Accurate face recognition provides an entry point into human social interactions and lays a foundation for subsequent complex cognition, such as trait perception, impression formation, discrimination, and prejudice. Successful face recognition involves two aspects: (a) Telling faces apart (i.e., detecting differences between identities) requires sensitivity to between-person variability <ref type="bibr" target="#b33">(Pascalis et al., 2002)</ref>, and (b) "telling faces together" (i.e., recognizing different images as the same person despite natural variations in the person's appearance) requires formation of a generalizable representation of that person <ref type="bibr" target="#b6">(Burton, 2013)</ref>.</p><p>Most studies investigating how people discriminate between faces use a limited number of highly controlled images to represent an individual's appearance. These studies greatly underestimate the real challenge of face identification. This is because faces viewed in the real world vary naturally in appearance as a result of changes in lighting, expression, viewing angle, hairstyle, and makeup; the pictorial cues (e.g., shadow on a face image) that participants rely on for face recognition in laboratory tests are no longer reliable in daily life. This oversight of within-person variability in the face-recognition literature leaves a significant gap in knowledge about exactly how we recognize faces in the real world ( Jenkins &amp; Burton, 2011; Laurence et al.,  2016).</p><p>It is only recently that the challenge of recognizing faces despite within-person variability has emerged at the forefront of face-recognition research. Withinperson variability, including both changes in the face itself (e.g., age, expression, makeup, stress level, fatigue) and changes in the capture conditions (e.g., lighting, filming distance, surrounding context), influences the 1030630P SSXXX10.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>apparent facial configuration, attractiveness, age, and distinctiveness of a given identity, making accurate face identification difficult <ref type="bibr" target="#b6">(Burton, 2013;</ref><ref type="bibr">Jenkins et al., 2011;</ref><ref type="bibr" target="#b31">Mileva et al., 2020)</ref>. It has been proposed that tolerance of within-person variability is a key difference between familiar and unfamiliar face recognition: Whereas familiar face recognition is robust across changes in images, unfamiliar face recognition is fragile and can be easily disrupted by superficial changes in appearance <ref type="bibr" target="#b24">( Johnston &amp; Edmonds, 2009;</ref><ref type="bibr" target="#b45">Young &amp; Burton, 2018)</ref>. When a face contains idiosyncratic variability in appearance, different photographs of the same person are often perceived as different people, unless that person is familiar to perceivers <ref type="bibr">( Jenkins et al., 2011;</ref><ref type="bibr">Zhou &amp; Mondloch, 2016)</ref>. Thus, understanding perception of within-person variability is thought to be key to understanding how unfamiliar faces become familiar, a ubiquitous task that guides humans' social interactions. Exposure to how faces vary enhances people's subsequent perceptual matching of the faces <ref type="bibr" target="#b0">(Andrews et al., 2015;</ref><ref type="bibr" target="#b15">Dowsett et al., 2016)</ref> and later recall of the faces from memory <ref type="bibr" target="#b2">(Baker et al., 2017;</ref><ref type="bibr" target="#b17">Hayward et al., 2017;</ref><ref type="bibr" target="#b38">Ritchie &amp; Burton, 2017;</ref><ref type="bibr" target="#b46">Zhou et al., 2018)</ref>. Researchers therefore argue that multiple exposures to within-person variability of a given face allow perceivers to separate transient changeable facial variations from the stable characteristics of the face, leading to reliable learning of that face, consistent with <ref type="bibr" target="#b4">Bruce's (1994)</ref> idea of "stability from variation."</p><p>Despite the important role of within-person variability in face recognition and learning, our current understanding is primarily based on behavioral tasks in adults <ref type="bibr">( Jenkins et al., 2011)</ref>. Few developmental studies have investigated how within-person variability affects identity perception in children. Laurence and colleagues found that whereas children older than 6 years can recognize familiar faces despite within-person facial variability, their recognition of unfamiliar faces continues to develop between 4 and 12 years of age <ref type="bibr">(Laurence &amp; Mondloch, 2016)</ref>. <ref type="bibr" target="#b30">Matthews et al. (2018)</ref> found that exposure to how faces vary facilitates children's perceptual matching of previously seen faces. <ref type="bibr" target="#b2">Baker et al. (2017)</ref> found that 6-to 13-year-old children require exposure to more within-person variability than adults to learn a newly encountered face. Therefore, children are less efficient than adults at learning faces from variability, although it is unclear why.</p><p>To our knowledge, no studies have examined how infants perceive faces when faces incorporate a wide range of natural variations in appearance. Most infants develop within a rich visual environment that requires them to cope with face variability to successfully recognize important faces (e.g., caregivers), yet it remains unclear how this ability develops. Using single face images, or images that vary across a single dimension (e.g., viewpoint or facial expression), past studies revealed that within days of birth, infants are able to recognize their mother's face <ref type="bibr" target="#b8">(Bushnell, 2001)</ref> and can recognize unfamiliar faces after short familiarization periods <ref type="bibr" target="#b34">(Pascalis &amp; de Schonen, 1994)</ref>. However, early identity recognition is fragile: Newborns do not recognize their mother's face when the hair is masked <ref type="bibr" target="#b35">(Pascalis et al., 1995)</ref>, young infants' face recognition is disrupted by the addition of paraphernalia <ref type="bibr" target="#b5">(Bulf et al., 2013)</ref>, and there is mixed evidence about whether young infants can recognize an unfamiliar face across transformation in viewpoint <ref type="bibr" target="#b10">(Cohen &amp; Strauss, 1979;</ref><ref type="bibr" target="#b42">Turati et al., 2008)</ref>. In general, researchers agree that the ability to recognize faces across single transformations (e.g., in viewpoint or facial expression) becomes increasingly robust by 6 to 7 months of age <ref type="bibr" target="#b3">(Bornstein &amp; Arterberry, 2003;</ref><ref type="bibr" target="#b26">Kobayashi et al., 2014)</ref>. Thus, infants show some limited capability to cope with variability in appearance. However, no studies have investigated identity recognition in infancy using face stimuli that approximate what infants see in daily life (i.e., faces that vary across multiple dimensions concurrently, including expressions, lighting, pose, hairstyle, and background context).</p><p>In the present study, we intentionally introduced a wide range of naturalistic within-person variability in facial appearance and conducted the first examination of the attentional mechanisms underlying adults' and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statement of Relevance</head><p>Face recognition in the real world is challenging. Faces vary naturally in appearance because of changes in the face itself (e.g., expression) and changes in the surrounding context (e.g., lighting). We know little about how humans develop the ability to learn faces despite naturalistic facial variability. The current study was the first examination of attentional mechanisms underlying adults' and infants' naturalistic face learning. Participants viewed dynamic videos of faces that varied in appearance. Using eye tracking and a novel facialtracking program, we performed spatial and temporal analyses of participants' face scanning. Infants showed adultlike prioritization of face over nonface regions, yet overall, they showed less ability to resist environmental distractions. These results highlight that attentional mechanisms critical for successful face recognition in the real world continue to develop beyond the first year. The innovative facial-tracking program introduced here shows significant promise for future research.</p><p>infants' naturalistic face learning using eye tracking. We familiarized adults and 6-to 12-month-old infants with naturalistic videos of models reading a storybook for 1 min; the videos captured either high or low variability in models' appearance. Later, participants viewed the learned model paired with a novel model to test recognition. To process large-scale video eye-tracking data (containing millions of data points), we developed a novel Python-based dynamic facial-tracking program. This program allowed us to automatically place dynamic areas of interest (AOIs) on models' facial features along with models' frame-by-frame movement in the video and to perform temporal analysis of adults' and infants' eye movements during dynamic face learning with extremely high efficiency. Applying this novel program in combination with eye tracking, we specifically asked how internal (i.e., eyes, nose, mouth) and external (i.e., hair, neck, background information) visual cues attracted visual attention during naturalistic face learning and whether this affected learning of faces differently in infants and adults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Forty-eight young adults (44 female; age: M = 21.55 years, SD = 4.90, range = 17-42) and forty-eight 6-to 12-month-old infants (20 female; age: M = 261.17 days, SD = 51.85, range = 180-363) were recruited from the Ryerson Psychology Research Pool and from the Ryerson Infant and Child Database, respectively. All adult participants had normal or corrected-to-normal vision. The adult sample comprised White (n = 16), East Asian (n = 11), South and Southeast Asian (n = 11), Black (n = 8), and mixed race (n = 2) participants. Data from eight infant participants were excluded from analysis because of fussiness (n = 2), failure to find the pupil (n = 2), calibration failure (n = 3), or experimental error (n = 1). The final infant sample comprised White (n = 24), East Asian (n = 5), South and Southeast Asian (n = 2), Black (n = 1), and mixed race (n = 8) participants. All infants were born between 37 and 42 weeks' gestational age. Parents reported that none of the infants had been diagnosed with any visual impairments or developmental delays.</p><p>An a priori power analysis indicated that our experimental design required 54 participants to uncover an effect size (f) of .25 (similar to the effect size found by <ref type="bibr">Xiao et al., 2015, who</ref> showed developmental changes in eye tracking of dynamic faces across infancy) in a 2 (within subjects) × 2 (between subjects) analysis of variance (ANOVA) given an α of .05 and 95% power (correlation among repeated measures = .5, no nonsphericity correction). We purposely oversampled with a goal of testing 48 participants per age group, anticipating that some participants would not be included in the final analysis, and data collection continued until this goal was reached. Our final sample size (N = 88), after we excluded data from participants as described above, exceeded the sample size required as indicated by the a priori power analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus</head><p>The EyeLink 1000 Plus (SR Research, Mississauga, Ontario, Canada), sampling at a rate of 500 Hz (16-mm lens), was used to track adults' and infants' monocular eye movements using a remote-desktop configuration. Under optimal conditions, the EyeLink has an accuracy of 0.5° and a tracking range of 32° × 25° (horizontal × vertical). Remote tracking allows head movements within a 22 cm × 18 cm × 20 cm (horizontal × vertical × depth) space without compromising accuracy. The tracker camera and infrared illuminator box were mounted on an adjustable arm below an attached 23-in. display monitor. A target sticker was placed on the participant's forehead to track head distance, position, and movement. A Dell monitor, with a screen resolution of 1,920 pixels × 1,200 pixels and a refresh rate of 60 Hz, was used to present the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>Video stimuli for familiarization. We made two dynamic videos of each of eight White female models reading the storybook Where the Wild Things Are <ref type="bibr" target="#b41">(Sendak, 1963)</ref>: one low-variability video and one high-variability video, each 1 min long. We chose 1 min for the length of the familiarization video because previous studies examining infants' processing of moving faces used comparable familiarization times <ref type="bibr" target="#b1">(Bahrick et al., 2002;</ref><ref type="bibr" target="#b12">Coulon et al., 2011;</ref><ref type="bibr" target="#b44">Xiao et al., 2015)</ref>. Additionally, previous studies examining face learning across variability demonstrated that adults show efficient face learning after 1 min of exposure to videos similar to those used in the current study <ref type="bibr" target="#b2">(Baker et al., 2017;</ref><ref type="bibr" target="#b46">Zhou et al., 2018)</ref>. Models were filmed with a Sony α NEX-5 camera with an 18-mm to 55-mm lens. Each model was filmed in three separate scenes that were used for both low-variability and high-variability conditions (see Fig. <ref type="figure" target="#fig_1">1</ref>). In each of the filming scenes, we systematically manipulated the filming distance, viewing angle, lighting conditions, models' hairstyle, and makeup intensity (see Table <ref type="table" target="#tab_0">1</ref>). For Scene 1, models were filmed from a moderately close low angle (i.e., from below) in soft lamplight with the light source out of the video frame. Models had their hair down and behind their shoulders and wore no makeup. Scene 2 was filmed at a straight angle, farther away from the model. A central overhead white light was used as a light source. Models tied their hair back loosely and wore a moderate amount of makeup (eye liner, light lipstick only). Scene 3 was filmed from a top angle at a medium distance from the model. The models were filmed in front of an image of a public park; a bright white light was directed from the front of the model and some natural light came from the window of the filming room. Models wore a high ponytail and heavy makeup, including liquid foundation, blush, eyebrow fillers, eye shadow, face highlighter, and heavy lipstick. For consistency, one research assistant applied makeup to all of the models for Scenes 2 and 3, and all models were draped with a black cloth around their shoulders and filmed for 1 min in each of the three scenes. For all of the scenes, the book was not included in the frame to ensure that the models would make eye contact with the camera.</p><p>Low-Variability Video High-Variability Video Se gm en t 1 (2 0 s) Se gm en t 1 Se gm en t 1 Se gm en t 2 (2 0 s) Se gm en t 3 (2 0 s) Se gm en t 1 (2 0 s) Se gm en t 2 (2 0 s) I M in I M in I M in I M in Se gm en t 3 (2 0 s) Model 1 Model 2 Familiarization Phase Model 1 Model 2 Learned Novel Learned Novel Novel Learned Novel Learned Testing Phase To create the high-variability video, for each model, we cut 20 s from each of the three scenes and combined them into one video that was approximately 1 min in length. The order of the three scenes was the same for all high-variability videos. The high-variability videos captured extensive variability in background and in the models' appearance associated with factors related to the models themselves (e.g., makeup) as well as external factors (e.g., lighting, distance). A soft 40-ms fade between cuts was applied to simulate a natural transition between sections. To make the low-variability videos, for each model, we spliced the entire Scene 1 video into three 20-s sections and then combined them. Therefore, each low-variability video was also 1 min in length and contained fade transitions between sections but captured only minor changes in models' appearance (e.g., expression, head movement). The other factors, including filming distance, angle, lighting, hairstyle, and makeup, remained constant. To eliminate the effect of speakers' voice on identity perception, we muted the sound in all videos.</p><p>Static stimuli for testing. The testing stimuli were pairs of static face images (see Fig. <ref type="figure" target="#fig_1">1</ref>), one of which was the learned face and the other a novel face that matched on physical similarities. The novel face was one of the eight models who was not seen in the familiarization phase (see the Supplemental Material available online). All models wore no makeup and made a neutral expression in photographs that were taken in our laboratory. The face images were then cropped into an oval (10.62 cm × 12.7 cm) so that the upper neck, ears, and a portion of the upper hairline were the only visible external features around the face. The two face images in each pair were separated by 20 cm and were presented on a black background (67.73 cm × 42.33 cm). The brightness of the two faces was adjusted to approximately the same level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>This study received clearance from the Ryerson University Research Ethics Board. Testing of each participant started with a 3-point calibration procedure to guarantee eye-tracking accuracy <ref type="bibr" target="#b16">(Farroni et al., 2007)</ref>. During the calibration, a colorful and animated cartoon character was presented with sound on the screen. Calibration was achieved when participants successfully fixated at three locations (top center, left, and right corner). If any validation points resulted in an error value greater than 1.0°, participants were recalibrated. Following successful calibration and validation, the task was immediately initiated.</p><p>The task was divided into two trials, each of which included a familiarization phase and a testing phase. The eight models were paired, and each participant was randomly assigned to one of the four learning-model pairs: Models <ref type="table" target="#tab_0">1</ref> and <ref type="table">2</ref>, <ref type="table">Models 3</ref> and <ref type="table">4</ref>, <ref type="table">Models 5</ref> and <ref type="table">6</ref>, and <ref type="table">Models 7</ref> and <ref type="table">8</ref>. The pair to which a participant was assigned determined which models served as the familiarization stimuli. In each of the model pairs, the low-variability video was presented for one, and the high-variability video was presented for the other (e.g., Model 1 in lowvariability video, Model 2 in high-variability video). In each particular pair, the model who served as the highvariability model was randomized across participants. Therefore, each participant was familiarized with both high-variability and low-variability videos (with different models), and the presentation order (high-variability video first vs. low-variability video first) was counterbalanced across participants. This resulted in a total of eight different conditions, and each participant was randomly assigned to one of the eight conditions, which determined model pair and variability order (see the Supplemental Material).</p><p>Following familiarization, a test phase displayed the static faces of the familiarized model and a novel model. The left/right position of the learned and the novel models in each face pair was randomized on the first test trial, and then their left/right position was reversed on the second test trial. As in numerous previous studies of infant face recognition, each trial was presented for 10 s, resulting in a total of 20 s for the testing phase. Five-to 10-s test trials are commonly used in studies of infant face perception (e.g., <ref type="bibr" target="#b18">Heron-Delaney et al., 2011;</ref><ref type="bibr" target="#b25">Kelly et al., 2007;</ref><ref type="bibr" target="#b36">Quinn et al., 2020)</ref>. The images were displayed in the vertical center of the screen, aligned to the left and right sides. An attention getter was displayed between test trials to orient the participants' attention to the center of the screen.</p><p>The task was presented in a dimly lit room with a screen brightness level of 60%. After providing consent, participants sat (infants were seated on their caregiver's lap) at a distance of approximately 50 cm from the display monitor so that each static image subtended 25.3° × 32.0° (width × height) of visual angle at the sides of the screen. After the task, adults filled out a demographic questionnaire, and infants' parents filled out an additional questionnaire assessing infants' daily exposure to facial within-person variability (see the Supplemental Material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data analysis</head><p>Following many past infant eye-tracking studies using the EyeLink 1000 Plus, we used the fixation events that were determined by the host software. Specifically, the EyeLink online-parsing program computes fixation events by using a saccade-picking algorithm that designates fixations as different from saccades and blinks on the basis of acceleration, instantaneous velocity, and gaze-motion thresholds. Saccades were defined as eye movements that exceeded 0.2° motion, 40°-per-second velocity, and 8,000°-per-second 2 acceleration. Blinks were defined as the time at which the pupil in the eyetracking camera was missing for a minimum of three consecutive samples. Fixations were defined as times between saccades not containing blinks. These saccadic thresholds are preset for a typical eye-tracking task and are commonly used for remote or head-free tracking with the EyeLink 1000 Plus system (for online-parsing details, see EyeLink 1000 Plus User Manual; SR Research, 2013). Fixation events with durations less than 80 ms were considered contaminated with noise and were removed <ref type="bibr" target="#b19">(Holmqvist et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic AOIs for learning faces</head><p>Traditional manual placements of AOIs used in past eyetracking studies primarily apply to static image stimuli. However, placing AOIs manually for the dynamic stimuli in the current study would have been a highly time-consuming procedure requiring enormous work and trained eye-tracking experts (i.e., 6 AOIs × 1,800 video frames × 2 models × 88 participants) and yet would have suffered from inevitable human errors (e.g., fatigue, lack of concentration). To overcome these limitations, we developed an automatic Python-based dynamic facial-tracking program to process large-scale video eye-tracking data applying a widely used facial-detection-and-mapping technique <ref type="bibr" target="#b40">(Sagonas et al., 2013</ref><ref type="bibr" target="#b39">(Sagonas et al., , 2016))</ref>. Crucially, this program can automatically place dynamic AOIs on models' facial features along with models' frame-by-frame movements in the learning videos and perform time-series analysis of the fixations that fall into specific AOIs regardless of the changes in models' appearance. Specifically, for each video frame, a facial-landmark detector was used to produce 68 (x, y) coordinates (reference points) that map to specific facial structures of learning models <ref type="bibr" target="#b37">(Ranjan, 2017)</ref>. These 68 reference-point mappings were generated by training a shape predictor on the labeled 300-W data set (designed by the Intelligent Behavior Understanding Group [iBUG] at Imperial College London; <ref type="bibr" target="#b40">Sagonas et al., 2013</ref><ref type="bibr" target="#b39">Sagonas et al., , 2016))</ref>.</p><p>The iBUG 300-W data set was chosen because it consists of a large number of naturalistic face images (more than 3,000 images for training set and 2 × 300 images for testing set) that were captured in totally unconstrained real-world settings (both indoor and outdoor scenes, such as at a party, a conference, and protests), and these in-the-wild face images show a variety of naturalistic facial variations such as different poses, spontaneous expressions, illumination, background, occlusion, and image quality. Critically, the annotation of the 68 reference facial landmarks used in the iBUG 300-W has been shown to be highly accurate despite the highly variable nature of the training face images, making the iBUG 300-W one of the most widely used data sets in the face-detection domain <ref type="bibr" target="#b39">(Sagonas et al., 2016)</ref>.</p><p>The generation of 68 reference points and the facemapping process were automatically repeated for each of the 1,800 video frames (1 min) for both high-and low-variability videos and for each of the eight learning models. This process enabled the dynamic facial-tracking program to automatically extract facial features, estimate head pose, and detect expressions and eyeblinks, making the processing of the large-scale video eyetracking data set (containing millions of data points) highly efficient. Indeed, as shown in the demonstration video (see Fig. <ref type="figure" target="#fig_3">2</ref> for link to video), the facial-landmark detection of the dynamic facial-tracking program is highly accurate. In addition, we compared the performance of the dynamic facial-tracking program and performance of manual AOI placement on three randomly selected face images; the mean overlapping areas between automatic and manual AOIs were all greater than 97.37%, indicating that the dynamic facial-tracking program performed reliably. An illustration of our dynamic AOIs using 68 reference-points mapping is shown in Figure <ref type="figure" target="#fig_3">2</ref>. Python codes for the facial-tracking program are freely available from the Corresponding Author, X. Zhou.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static AOIs for testing faces</head><p>For each model, we created six separate AOIs for the (a) entire head, (b) entire face (approximately 72.89% of the head), (c) left eye (3.09%), (d) right eye (3.09%), (e) nose (4.66%), and (f) mouth (5.07%; see Fig. <ref type="figure" target="#fig_5">3</ref>). Fixations on the left and right eyes were combined as fixation on the eyes region for data analysis. The size of the AOIs was adjusted across models to best fit the model's facial configuration. Each AOI was slightly</p><p>y (x min , y min ) (x min , y min ) (x min , y min ) x y x y x Facial-Landmark Detector Produces 68 (x, y) Coordinates Facial Images at Frame 157, Frame 1,091, and Frame 1,702 68 Reference Points Mapped to Facial Structure by Training a Shape Predictor From iBUG 300-W Data Set Results of Facial-Landmark Detection Dynamic AOI Tracking Program Range of Facial Reference Points Jawline: [1, 17] Left Eyebrow: [18, 22] Right Eyebrow: [23, 27] Nose: [28, 36] Left Eye: [37, 42] Right Eye: [43, 48] Mouth: [49, 68] bigger than the corresponding facial features to allow for the sampling variations in eye-tracking precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The average calibration accuracy across the adult and infant samples was 0.42° and 0.61° of error, respectively. Precise calibration values were unavailable for two infants; however, the task was never initiated if an infant had poor calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Familiarization phase (dynamic video)</head><p>First, we examined how much time infants and adults directed toward the screen during the familiarization phase as a measure of attentiveness during learning. On average, infants spent 63.79% of their time (M = 38.27 s, SE = 1.90) and adults spent 88.4% of their time (M = 53.05 s, SE = 1.63) looking at the video during the familiarization phase. This was true for both lowvariability videos (75.93% of time on screen; M = 45.56 s, SE = 1.19, across age groups) and high-variability videos (76.27% of time on screen; M = 45.76 s, SE = 1.84, across age groups). Although there was a significant difference in infants' and adults' attention during familiarization (p &lt; .001), both age groups were attentive during learning.</p><p>Next, we examined fixation to various aspects of the familiarization stimuli. The relative areas of facial features, the face, and the nonface region changed dynamically across videos because of models' facial movements; if participants directed their fixations randomly across stimuli, areas of the stimulus that are larger (e.g., nonface region) would receive more fixations than areas of the stimulus that are smaller (e.g., face region) by chance alone <ref type="bibr" target="#b9">(Carter &amp; Luke, 2020)</ref>. Thus, to reveal true patterns of visual attention, it is critically important to adjust fixation counts within AOIs by weighting them relative to the area of the screen. Considering this, we calculated the fixation counts within an AOI on the basis of its weighted area. For example, if a participant had 10 fixations to each of the face and nonface regions, and the sizes of the face and nonface regions in that specific video frame were 20% and 80% of the whole screen, respectively, then the fixation counts in that video frame were transformed by dividing the fixation count by the percentage area, resulting in area-weighted fixation counts of 50 (10/20%) and 12.5 (10/80%) for the face and nonface regions, respectively. Averages of areaweighted fixation counts across 1,800 video frames were then calculated for each AOI, each variability condition, and each model. Except where specified, all of the values that we report below reflect area-weighted fixations.</p><p>Preliminary analyses revealed no effect of model pair, F(3, 75) = 0.72, p = .544, η p 2 = .03, or learning order (high variability first or low variability first), F(3, 75) = 0.01, p = .957, η p 2 = .01, on the mean area-weighted fixations for high-variability and low-variability videos; therefore, data were collapsed across testing orders and model pairs for further analyses.</p><p>Face versus nonface area. To examine whether variability type influenced participants' attention to the face versus nonface regions (i.e., ears, neck, hair, and background area), we conducted a mixed ANOVA with AOI (face vs. nonface) and variability (high variability vs. low variability) as within-subjects factors and age (adults vs. infants) as a between-subjects factor. Significant effects were followed up with Bonferroni-corrected pairwise comparisons. We found significant main effects of AOI, F( <ref type="formula">1</ref>  Although the AOI × Variability × Age interaction was significant, F(1, 81) = 6.53, p = .012, η p 2 = .08, follow-up tests confirmed a similar effect in both age groups, whereby variability type influenced participants' fixations on the face region (ps &lt; .001) but not on the nonface region (ps &gt; .900). Specifically, area-weighted fixations were greater for faces in high-variability videos (M = 703.20, SE = 39.24) than low-variability videos (M = 523.20, SE = 27.30), t(82) = 6.43, p &lt; .001, Cohen's d = 0.71. However, fixations on the nonface region did not significantly differ for high-variability videos (M = 18.71, SE = 2.29) and low-variability videos (M = 18.61, SE = 2.40), t(82) = 0.04, p = .968, Cohen's d = 0.01, a pattern consistent across age groups (see Fig. <ref type="figure" target="#fig_6">4a</ref>). This suggests that, like adults, infants between 6 and 12 months of age were capable of directing their attention to highly variable faces despite the presence of increased variability in the background context in the high-variability condition. There was also a significant interaction between AOI and age, F(1, 81) = 97.84, p &lt; .001, η p 2 = .55. Adults (M = 788.80, SE = 32.85) fixated the face region significantly more than infants <ref type="bibr">(M 372.38,</ref><ref type="bibr">SE = 20.43)</ref>, t( <ref type="formula">81</ref> demonstrated a clear face over nonface scanning pattern during face learning (ps &lt; .001), and this pattern was similar for high-variability and low-variability videos.</p><p>Because the 1-min familiarization videos consisted of three segments, each lasting for 20 s, we explored whether the pattern of increased fixation on the face over nonface region in both age groups changes over time and whether it was affected by variability type. To do so, we calculated the difference score (d) between area-weighted fixations on face and nonface regions for each participant by subtracting fixations on nonfaces from fixations on faces. A greater d indicates a greater number of fixations on the face area and a decreased number of fixations on the nonface area, adjusting for the area of each region, reflecting increased attention to the face over the nonface region. We then conducted a mixed ANOVA with the video segment (first 20 s, second 20 s, third 20 s) and variability type as withinsubjects factors and age as a between-subjects factor. All of the main effects were significant (ps &lt; .001). There was a significant Age × Segment interaction, F(2, 162) = 6.39, p = .002, η p 2 = .07, and a significant Segment × Variability interaction, F(2, 162) = 24.89, p &lt; .001, η p 2 = .24. These effects were qualified by the significant Age × Segment × Variability interaction, F(2, 162) = 5.11, p = .007, η p 2 = .06. Follow-up tests revealed that the effect of segment and variability differed for infants and adults (ps &lt; .013). Specifically, when infants learned faces from high-variability videos, d was greater for Segment 2 than both Segments 1 and 3 (ps &lt; .021), and there was no difference between Segments 1 and 3 (p = .150); but when they learned faces from low-variability videos, there was a linear decrease of d from Segment 1 to Segment 3, and there were significant differences among the three segments (ps &lt; .013). For adults, patterns were similar to those in infants for high-variability videos: d was greater for Segment 2 than both Segments 1 and 3 (ps &lt; .001), and there was no difference between Segments 1 and 3 (p = .190); but when adults learned faces from low-variability videos, there was no decrease of d from Segment 1 to Segment 2 (p = .121) or from Segment 2 to Segment 3 (p = .076) but a decrease of d from Segment 1 to Segment 3 (p = .006).</p><p>Together, these results suggest that adults' and infants' increased attention to face over nonface regions remained relatively consistent throughout the learning phase when participants learned faces from highvariability videos (i.e., no decrease of d throughout the three segments; see Fig. <ref type="figure" target="#fig_6">4b</ref>). However, when participants learned faces from low-variability videos, where the facial features and background information remain relatively unchanged, infants demonstrated a clear decrease in their fixations on the face over the nonface region across time (i.e., significant decrease of d across the three segments). This pattern was less clear in adults (see Fig. <ref type="figure" target="#fig_6">4c</ref>). To better visualize how adults' and infants' visual attention evolved over time for highvariability and low-variability videos, we plotted adults' and infants' gaze changes to the face versus nonface regions frame by frame during the entire 1-min learning period (see Fig. <ref type="figure" target="#fig_7">5</ref>).</p><p>Facial features: eyes versus nose versus mouth. We next examined whether the spatial distribution of areaweighted fixations on facial features (eyes vs. nose vs. mouth) differed for high-variability and low-variability videos in adults and infants. We found significant main effects of AOI, F(1, 81) = 24.99, p &lt; .001, η p 2 = .24; variability type, F(1, 81) = 13.60, p &lt; .001, η p 2 = .14; and age group, F(1, 81) = 36.18, p &lt; .001, η p 2 = .31. Participants allocated more area-weighted fixations to the eye region (M = 5,943.88, SE = 853.27) and nose region (M = 5,690.79, SE = 465.17) than the mouth region (M = 1,210.98, SE = 120.56; there was no difference between eyes and nose, p = .761). Fixation counts were greater in high-variability videos (M = 4,810.89, SE = 447.68) than low-variability videos (M = 3,752.88, SE = 319.07) and in adults (M = 6,455.24, SE = 469.25) than in infants (M = 2,108.53, SE = 549.52). Importantly, we found a significant AOI × Variability interaction, F(2, 160) = 4.57, p = .012, η p 2 = .05; a significant AOI × Age interaction, F(2, 160) = 6.72, p = .002, η p 2 = .08; and a significant AOI × Variability × Age interaction, F(2, 160) = 5.22, p = .006, η p 2 = .06. Follow-up tests revealed that the AOI × Variability interaction was significant for adults, F(2, 94) = 8.10, p = .001, η p 2 = .15, but not for infants, F(2, 68) = 0.34, p = .710, η p 2 = .01. Specifically, adults' area-weighted fixation counts on the eyes and nose were greater in high-variability than low-variability videos (ps &lt; .003), but their fixation counts on the mouth were similar in high-variability and low-variability videos (p = .944). In contrast, infants' area-weighted fixation counts on all facial features were similar for high-variability and lowvariability videos (ps &gt; .190).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing phase (static face pairs)</head><p>The proportion of fixation durations within each AOI for each condition was analyzed for the testing phase. Analyses of fixation counts yielded identical results and therefore are not reported here. Here, we report analyses of fixation duration without adjusting for area because the familiar and novel stimuli were identical in size. A preliminary examination of the fixation duration to the learned and novel faces suggested increased looking to the familiar face overall. Therefore, we calculated a familiarity-preference score by dividing looking time to the learned face by total looking time to the learned and novel faces. There were no effects of testing order or learning models (ps &gt; .558). In addition to common frequentist statistics, Bayesian statistics were used in follow-up tests to help interpret the current pattern of results. An advantage of Bayesian statistics over standard null-hypothesis significance testing is that we are able to evaluate the evidence in favor of both the alternative hypothesis and the null hypothesis in terms of Bayes factors (BF 10 s).</p><p>In a 2 (age) × 2 (variability) mixed ANOVA, we found a significant main effect of age, F(1, 86) = 9.60, p = .003, η p 2 = .10, but no effect of variability or interactions with this factor (ps &gt; .457). Adults showed a significantly greater familiarity preference (M = .58, SE = .02) than infants (M = .49, SE = .01), t(86) = 3.11, p = .003, Cohen's d = 0.67, BF 10 = 13.77. 1 Follow-up tests were run to compare the familiarization-preference score with chance level (.5) in each age group. 2 Adults demonstrated a significant familiarity preference, t(47) = 3.32, p = .002, BF 10 = 17.49, whereas infants did not, t(39) = -0.72, p = .475, BF 10 = 0.22 (see Fig. <ref type="figure" target="#fig_9">6</ref>). In line with commonly used standards for evidence <ref type="bibr" target="#b43">(van Doorn et al., 2021)</ref>, our Bayesian results reveal strong evidence for a familiarity preference in adults, moderate evidence for no preference in infants, and strong evidence for a difference in familiarity preference between adults and infants.</p><p>Another metric of preference we compared was the length of the first fixations for the learned and novel models. Although overall, infants had longer first fixations than adults (p = .020), infants' first fixations to the learned and novel models did not differ in length (p = .142), which is consistent with their lack of preference across the entire trial.</p><p>We next examined whether adults' and infants' looking duration to specific facial features (eye, nose, and mouth) during the testing phase changed as a function of variability type. Participants spent a greater proportion of time looking at the eyes (M = .17, SE = .01) than both the nose (M = .06, SE = .01) and mouth (M = .05, SE = .01), a greater proportion of time looking at facial features of the learned face (M = .10, SE = .01) than the novel face (M = .08, SE = .01, p = .031), and a greater time looking at facial features of high-variability models (M = .09, SE = .01) than of low-variability models (M = .09, SE = .01, p = .031). A significant Novelty × Age interaction, F( <ref type="formula">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Age-related change in infancy</head><p>To test whether infants' face learning across withinperson variability changes with age, we ran multiple linear regressions to predict their area-weighted fixation counts in the familiarization phase and their familiaritypreference score in the testing phase on the basis of their age in days. The analyses revealed that infants' age in days did not predict their area-weighted fixation counts for faces learned in the high-variability videos, β = -0.51, t(33) = -1.03, p = .311, or low-variability videos, β = -0.07, t(33) = -0.15, p = .878, nor did it predict their familiarity preference in the testing phase for faces learned in the high-variability videos, β = 0.01, t(33) = 0.17, p = .854, or in the low-variability videos (see Fig. <ref type="figure" target="#fig_10">7</ref>), β = 0.01, t(33) = -1.51, p = .142. These results suggest that there was no age-related change in face learning from naturalistic variability in 6-to 12-month-old infants in the current study. Additional analyses found that there were no effects of participant race or gender on participants' visual behavior during either the familiarization or testing phase. Detailed results are reported in the Supplemental Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Applying eye tracking in combination with the dynamic facial-tracking program, the present study provided the first examination of how naturalistic within-person variability influences adults' and infants' visual attention during face learning. To our knowledge, this is the first explicit investigation of face learning across naturalistic within-person variability reported in an infant population.</p><p>Within-person variability affected adults' and infants' visual attention to faces during learning. Faces learned in the high-variability condition attracted significantly more attention than faces learned in the low-variability condition in both adults and infants. This finding dovetails with the results of recent behavioral studies showing that adults' and children's face learning benefits from high variability in facial appearance <ref type="bibr" target="#b0">(Andrews et al., 2015;</ref><ref type="bibr" target="#b2">Baker et al., 2017)</ref>. Our study provided the first evidence that, like adults, infants between 6 and 12 months of age are capable of directing visual attention to highly variable faces, despite the presence of increased variability in the background context. Like adults, infants demonstrated a face-over-nonface scanning pattern during face learning. However, their visual attention was not fully adultlike. Overall, infants fixated on the videos less than adults; they also fixated more than adults on the nonface region but less than adults on the face region. These findings together suggest that infants' visual attention is more likely to be attracted by variations in external features and environmental context than adults' visual attention during face learning. This might lead to inefficient learning of newly encountered faces, thereby causing the observed lack of discrimination between the learned and novel faces in the testing phase, a point we will return to below. Crucially, our time-course analyses provide further evidence that the extent to which external and environmental information attracts visual attention changes across learning in high-variability compared with lowvariability learning conditions and in adults compared with infants. Although adults showed higher difference scores than infants overall for fixations on faces relative to nonfaces, both age groups in the high-variability condition maintained relatively consistent attention on face over nonface regions across the three segments, despite extensive variability in external and environmental information. This was not true in the low-variability condition, where difference scores decreased significantly across the three segments in infants and, to a lesser extent, in adults. This finding is somewhat counterintuitive: In a context of increased background variability, both infants and adults maintain more consistent attention on the to-be-learned face. This suggests that the context in which a face is learned influences how both adults and infants direct their visual attention toward the face in potentially surprising ways. However, it is important to note that the order of the three segments in the high-variability condition was not randomized; thus, we cannot be certain that the pattern of looking in this condition would remain the same if the segments were presented in a different order. It is also unclear whether the maintained focus on the face in the high-variability condition is driven by increased variability in the face itself or whether the increased variability in the background serves to increase attention to the face. Future studies that vary the extent of variability in the face and background independently would help to disentangle these possibilities.</p><p>During the test phase, we found a clear familiarity preference in adults, but we found no preference in infants, suggesting that infants were unable to discriminate the learned face from the novel face despite being attentive during the learning phase. This finding is consistent with a small number of past studies showing that infants demonstrate no preference between newly learned and novel faces after being familiarized to moving faces <ref type="bibr" target="#b1">(Bahrick et al., 2002;</ref><ref type="bibr" target="#b12">Coulon et al., 2011;</ref><ref type="bibr" target="#b44">Xiao et al., 2015)</ref>. It is also broadly consistent with research demonstrating that young children show difficulty recognizing even familiar faces because of the various factors that can alter a person's appearance <ref type="bibr">(Laurence &amp; Mondloch, 2016)</ref>, and learning faces from variability is significantly less efficient in children than adults <ref type="bibr" target="#b2">(Baker et al., 2017)</ref>. Researchers generally agree that infant looking preferences following familiarization reflect the amount of familiarity and are influenced by task difficulty and stimulus complexity <ref type="bibr" target="#b20">(Hunter &amp; Ames, 1988;</ref><ref type="bibr" target="#b32">Pascalis &amp; de Haan, 2003)</ref>. Given that our face stimuli incorporated significant natural variability, and we tested infants' recognition using novel images of the learned face, a task more difficult than recognizing familiar images of learned faces, it is perhaps not surprising that we found no preference in infants. This lack of discrimination suggests that 1 min of learning was not sufficient for infants to build a robust face representation, potentially because of less efficient use of internal and external facial cues. This finding is important, especially when considered in light of the existing literature on face perception in infancy. Much of the traditional face-perception research in infancy uses static faces that minimize real-world variability; our results suggest that this approach may underestimate the true challenge of face recognition for infants in their daily life. It would be valuable for future studies to investigate how much exposure to dynamic, naturally varying faces is needed for infants to form a generalizable face representation by systematically manipulating the length of the familiarization period (e.g., 1 min vs. 3 min vs. 10 min).</p><p>Addressing exactly how facial variability facilitates face learning is beyond the scope of the current study. However, our study suggests that holding a stable focus on facial features and resisting distractions from the surrounding context are critical for efficient face learning. We argue that when face information and contextual information compete for attention, prioritizing the processing of faces while ignoring contextual distractions might particularly facilitate two aspects of face recognition. First, it might help the visual system to filter what information is critical for identification and</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Screenshots demonstrating the appearance of two models in low-variability (left) and high-variability (right) videos in the familiarization phase (top) and in the accompanying static photos used in the testing phase (bottom), in which the learned model was paired with a novel model. Note that each participant learned two models, but each model was learned in only one type of variability (e.g., Model 1 was learned in a low-variability video and then Model 1 was tested against a novel identity; Model 2 was learned in a high-variability video and then Model 2 was tested against a novel identity). For illustration purposes, both high-and low-variability versions are shown for both models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Procedure for mapping facial reference points (top) and dynamic areas of interest (AOIs; bottom) used in the familiarization phase. Sixty-eight facial reference points were mapped to each model's face at Video Frames 157 (from Scene 1), 1,091 (Scene 2), and 1,702 (Scene 3) using the dynamic facial-tracking program. The dynamic areas of interest (AOIs) generated via the mapping of the 68 reference points is shown at the bottom. An MP4 video showing the final facial-landmark-detection results can be accessed at https://osf.io/fwypd/. iBUG = Intelligent Behavior Understanding Group.</figDesc><graphic coords="7,148.61,380.94,237.62,191.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>, 81) = 671.33, p &lt; .001, η p 2 = .89; variability, F(1, 81) = 37.14, p &lt; .001, η p 2 = .31; and age, F(1, 81) = 94.61, p &lt; .001, η p 2 = .54. Participants allocated more area-weighted fixations to the face region (M = 580.59, SE = 21.14) than the nonface region (M = 19.59, SE = 1.90) and more areaweighted fixations in high-variability videos (M = 342.40, SE = 14.59) than low-variability videos (M = 257.78, SE = 9.98). Adults (M = 401.22, SE = 13.50) made more areaweighted fixations than infants (M = 198.96, SE = 15.81).Notably, we found a significant interaction between AOI and variability, F(1, 81) = 38.04, p &lt; .001, η p 2 = .32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the areas of interest (AOIs) used in the testing phase. AOIs were created for the (1) entire head region, (2) face region, (3) left eye, (4) right eye, (5) nose, and (6) mouth.</figDesc><graphic coords="8,62.24,68.99,216.02,259.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Area-weighted fixation counts and differences in fixation counts for the face and nonface regions. Adults' and infants' area-weighted fixation counts (a) are shown separately for face and nonface regions in high-variability videos (solid bars) and low-variability videos (patterned bars) during face learning. Adults' and infants' differences in fixation counts (ds) between the face and nonface regions during the first 20 s (Segment 1), second 20 s (Segment 2), and last 20 s (Segment 3) of learning are shown separately for high-variability videos (b) and low-variability videos (c). In all panels, the midline and X in each box represent the median and mean, respectively. The bottom and top of each box respectively indicate the first and third quartiles. Points displayed beyond the end of each whisker are outliers. Note that in (a) we use one of the background pictures (a park) as a demonstration of the nonface region. However, the nonface regions defined in our learning videos included both variations in models' external features (hair, neck, shoulders) and variations in the background.</figDesc><graphic coords="9,101.28,328.01,203.31,114.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Individual frame-by-frame gaze changes to the face region and the nonface region the 1-min familiarization phase in adults and infants. Each dot represents an areaweighted gaze count for a specific video frame, and results are shown separately for adults' and infants' gazes for the high-and low-variability videos. There were 30 frames per second, resulting in a total of 1,800 video frames in each 1-min learning video. Shading indicates the three 20-s segments of the video: The first segment is from 0 to 600 frames, the middle is from 601 to 1,200 frames, and the last is from 1,201 to 1,800 frames.</figDesc><graphic coords="11,169.18,588.21,97.22,57.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>, 86) = 8.20, p = .005, η p 2 = .09, suggests that whereas adults fixated longer on facial features of the learned model (M = .72, SE = .04) than of the novel model (M = .53, SE = .03), t(47) = 3.01, p = .004, Cohen's d = 0.43, infants demonstrated a similar fixation duration for facial features of the learned model (M = .45, SE = .03) and novel model (M = .48, SE = t(39) = -0.79, p = .433, Cohen's d = 0.13. All other interactions were found to be nonsignificant (ps &gt; .164).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Heat maps showing an example of two adults' (top) and two infants' (bottom) fixations on novel faces and faces that were learned from either high-variability (left column) or low-variability (right column) videos.</figDesc><graphic coords="12,327.02,205.71,168.26,105.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Scatterplot (with best-fitting regression lines) showing the relation between infants' age and visual behavior. For the familiarization phase, visual behavior is indexed by the number of area-weighted fixations toward faces and nonfaces (top), whereas for the testing phase, visual behavior is indexed by the familiaritypreference score for high-and low-variability videos (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Parameters Manipulated During Filming of Models to Create Low-Variability and High-Variability Videos</figDesc><table><row><cell></cell><cell>Filming</cell><cell>Viewing</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Scene</cell><cell>distance</cell><cell>angle</cell><cell>Lighting conditions</cell><cell>Hairstyle</cell><cell>Makeup intensity</cell></row><row><cell>1</cell><cell>20-30 cm</cell><cell>Low</cell><cell>Soft lamplight</cell><cell>Down and behind</cell><cell>No</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>shoulders</cell><cell></cell></row><row><cell>2</cell><cell>120-150 cm</cell><cell>Straight</cell><cell>Central overhead light</cell><cell>Tied back loosely</cell><cell>Moderate</cell></row><row><cell>3</cell><cell>50-80 cm</cell><cell>Top</cell><cell>Front white light and</cell><cell>High ponytail</cell><cell>Heavy</cell></row><row><cell></cell><cell></cell><cell></cell><cell>natural window light</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank our anonymous reviewers for their comments, <rs type="person">Marlene Ma</rs> and <rs type="person">Eunice Kim</rs> for their assistance in data collection, and our models for contributing to video filming.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This work was supported by a <rs type="funder">Natural Sciences and Engineering Research Council</rs> <rs type="grantName">Discovery Grant</rs> (<rs type="grantNumber">RGPIN-2019-05548</rs>) to <rs type="person">M. C. Moulson</rs>.</p></div>
<div><head>Open Practices</head><p>All data have been made publicly available via OSF and can be accessed at <ref type="url" target="https://osf.io/fwypd/">https://osf.io/fwypd/</ref>. The design and analysis plans for the study were not preregistered. This article has received the badge for Open Data. More information about the Open Practices badges can be found at <ref type="url" target="http://www.psychologicalscience.org/publications/badges">http://www.psychologicalscience.org/publications/  badges</ref>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uznu7C3">
					<idno type="grant-number">RGPIN-2019-05548</idno>
					<orgName type="grant-name">Discovery Grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>what is not so as to enhance the accurate abstraction of an average face representation across the various factors that can alter a person's appearance (i.e., ensemble coding; <ref type="bibr" target="#b7">Burton et al., 2005;</ref><ref type="bibr" target="#b13">Davis et al., 2021;</ref><ref type="bibr" target="#b27">Kramer et al., 2015)</ref>, consistent with Bruce's notion of learning "stability from variation" <ref type="bibr" target="#b4">(Bruce, 1994;</ref><ref type="bibr" target="#b45">Young &amp; Burton, 2018)</ref>. However, how ensemble coding is related to the use of internal and external cues requires further investigation. Second, given that visual attention orienting is linked to visual memory and learning <ref type="bibr" target="#b14">(de Haan, 2007)</ref>, it might help guide individuals to build facial representations from exemplar instances and update these instances in visual memory. Because the integration of attention and memory continues to develop between 6 and 15 months of age, as a result of the maturation of frontal circuitry <ref type="bibr" target="#b11">(Colombo &amp; Cheatham, 2006)</ref>, we would expect an improvement in consolidating faces into stable representations in memory as infants age. However, how such ability develops calls for future investigation.</p><p>In summary, our study was the first examination of the attentional mechanisms underlying adults' and infants' face learning across the extensive factors that can alter a person's appearance. It reveals that facial variability is an important factor shaping both adults' and infants' visual attention during face learning but that infants are overall less efficient than adults in maintaining attention on a face in naturalistic face-learning contexts, which may influence their ability to build a stable representation. Our study calls for the use of face stimuli that incorporate naturalistic within-person variability in future face-recognition research to understand how face recognition occurs in infants' daily life. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Additional supporting information can be found at <ref type="url" target="http://journals.sagepub.com/doi/suppl/10.1177/09567976211030630">http://  journals.sagepub.com/doi/suppl/10.1177/09567976211030630</ref> Notes 1. Using JASP (Version 0.14.1; JASP Team, 2020), we conducted a Bayesian independent-samples t test. For the prior, we used the default Cauchy prior width of r = 1/√2. 2. Using JASP (Version 0.14.1; JASP Team, 2020), we conducted Bayesian one-sample t tests (compared with a value of .5) separately for adults and infants. For the prior, we used the default Cauchy prior width of r = 1/√2.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Telling faces together: Learning new faces through exposure to multiple instances</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cursiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470218.2014.1003949</idno>
		<ptr target="https://doi.org/10.1080/17470218.2014.1003949" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2041" to="2050" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention and memory for faces and actions in infancy: The salience of actions over faces in dynamic events</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bahrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Gogate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ruiz</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-8624.00495</idno>
		<ptr target="https://doi.org/10.1111/1467-8624.00495" />
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1629" to="1643" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How does a newly encountered face become familiar? The effect of within-person variability on adults&apos; and children&apos;s perception of identity</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laurence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mondloch</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2016.12.012</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2016.12.012" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition, discrimination and categorization of smiling by 5-monthold infants</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Bornstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Arterberry</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-7687.00314</idno>
		<ptr target="https://doi.org/10.1111/1467-7687.00314" />
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="585" to="599" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stability from variation: The case of face recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
		<idno type="DOI">10.1080/14640749408401141</idno>
		<ptr target="https://doi.org/10.1080/14640749408401141" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology A: Human Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How a hat may affect 3-month-olds&apos; recognition of a face: An eye-tracking study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valenza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Turati</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0082839</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0082839" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">82839</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Why has research in face recognition progressed so slowly? The importance of variability</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470218.2013.800125</idno>
		<ptr target="https://doi.org/10.1080/17470218.2013.8001" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1467" to="1485" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust representations for face recognition: The power of averages</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J B</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogpsych.2005.06.003</idno>
		<ptr target="https://doi.org/10.1016/j.cogpsych.2005.06.003" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="256" to="284" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mother&apos;s face recognition in newborn infants: Learning and memory</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W R</forename><surname>Bushnell</surname></persName>
		</author>
		<idno type="DOI">10.1002/icd.248</idno>
		<ptr target="https://doi.org/10.1002/icd.248" />
	</analytic>
	<monogr>
		<title level="j">Infant and Child Development</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="67" to="74" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Best practices in eye tracking research</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Luke</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijpsycho.2020.05.010</idno>
		<ptr target="https://doi.org/10.1016/j.ijpsycho.2020.05.010" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="29" to="62" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Concept acquisition in the human infant</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Strauss</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8624.1979.tb04123.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-8624.1979.tb04123.x" />
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="419" to="424" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The emergence and basis of endogenous attention in infancy and early childhood</title>
		<author>
			<persName><forename type="first">J</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Cheatham</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0065-2407(06)80010-8</idno>
		<ptr target="https://doi.org/10.1016/S0065-2407(06)80010-8" />
	</analytic>
	<monogr>
		<title level="m">Advances in child development and behavior</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Kail</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="283" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognition of unfamiliar talking faces at birth</title>
		<author>
			<persName><forename type="first">M</forename><surname>Coulon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guellai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Streri</surname></persName>
		</author>
		<idno type="DOI">10.1177/0165025410396765</idno>
		<ptr target="https://doi.org/10.1177/0165025410396765" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Behavioral Development</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="282" to="287" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ensemble coding of facial identity is not refined by experience: Evidence from other-race and inverted faces</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mondloch</surname></persName>
		</author>
		<idno type="DOI">10.1111/bjop.12457</idno>
		<ptr target="https://doi.org/10.1111/bjop.12457" />
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="265" to="281" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual attention and recognition memory in infancy</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in developmental psychology. Infant EEG and event-related potentials</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Haan</surname></persName>
		</editor>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="101" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face learning with multiple images leads to fast acquisition of familiarity for specific individuals</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Dowsett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sandford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470218.2015.1017513</idno>
		<ptr target="https://doi.org/10.1080/17470218.2015.1017513" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Direct gaze modulates face recognition in young infants</title>
		<author>
			<persName><forename type="first">T</forename><surname>Farroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaccesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2006.01.007</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2006.01.007" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="396" to="404" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The other-race effect in face learning: Using naturalistic images to investigate face ethnicity effects in a learning paradigm</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Hayward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Favelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oxner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470218.2016.1146781</idno>
		<ptr target="https://doi.org/10.1080/17470218.2016.1146781" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="890" to="896" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual training prevents the emergence of the otherrace effect during infancy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heron-Delaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anzures</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Slater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pascalis</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0019858</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0019858" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Article e19858</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Holmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nyström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dewhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jarodzka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<title level="m">Eye tracking: A comprehensive guide to methods and measures</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multifactor model of infant preferences for novel and familiar stimuli</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Ames</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in infancy research</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Rovee-Collier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Lipsitt</surname></persName>
		</editor>
		<imprint>
			<publisher>Ablex</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="69" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<orgName type="collaboration">JASP Team</orgName>
		</author>
		<ptr target="https://jasp-stats.org/" />
	</analytic>
	<monogr>
		<title level="j">JASP</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Computer software</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stable face representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2010.0379</idno>
		<ptr target="https://doi.org/10.1098/rstb.2010.0379" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="issue">1571</biblScope>
			<biblScope unit="page" from="1671" to="1683" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variability in photos of the same face</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Van Montfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2011.08.001</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2011.08.001" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="323" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Familiar and unfamiliar face recognition: A review</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Edmonds</surname></persName>
		</author>
		<idno type="DOI">10.1080/09658210902976969</idno>
		<ptr target="https://doi.org/10.1080/09658210902976969" />
	</analytic>
	<monogr>
		<title level="j">Memory</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="577" to="596" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The other-race effect develops during infancy</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Slater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pascalis</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9280.2007.02029.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-9280.2007.02029.x" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1084" to="1089" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The processing of faces across nonrigid facial transformation develops at 7 month of age: A fNIRS-adaptation study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kakigi</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2202-15-81</idno>
		<ptr target="https://doi.org/10.1186/1471-2202-15-81" />
	</analytic>
	<monogr>
		<title level="j">BMC Neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Viewers extract the mean from images of the same person: A route to face learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<idno type="DOI">10.1167/15.4.1</idno>
		<ptr target="https://doi.org/10.1167/15.4" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">That&apos;s my teacher! Children&apos;s ability to recognize personally familiar and unfamiliar faces improves with age</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laurence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mondloch</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jecp.2015.09.030</idno>
		<ptr target="https://doi.org/10.1016/j.jecp.2015.09.030" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Child Psychology</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="123" to="138" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The flip side of the other-race coin: They all look different to me</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laurence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mondloch</surname></persName>
		</author>
		<idno type="DOI">10.1111/bjop.12147</idno>
		<ptr target="https://doi.org/10.1111/bjop.12147" />
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="374" to="388" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Getting to know you: The development of mechanisms underlying face learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mondloch</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jecp.2017.10.012</idno>
		<ptr target="https://doi.org/10.1016/j.jecp.2017.10.012" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Child Psychology</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="295" to="313" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facial identity across the lifespan</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mileva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogpsych.2019.101260</idno>
		<ptr target="https://doi.org/10.1016/j.cogpsych.2019.101260" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">101260</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognition memory and novelty preference: What model</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pascalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in infancy research</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Hayne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Fagen</surname></persName>
		</editor>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="95" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Is face processing species-specific during the first year of life?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pascalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Nelson</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1070223</idno>
		<ptr target="https://doi.org/10.1126/science.1070223" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="issue">5571</biblScope>
			<biblScope unit="page" from="1321" to="1323" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recognition memory in 3-to 4-day-old human neonates</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pascalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De Schonen</surname></persName>
		</author>
		<idno type="DOI">10.1097/00001756-199409080-00008</idno>
		<ptr target="https://doi.org/10.1097/00001756-199409080-00008" />
	</analytic>
	<monogr>
		<title level="j">NeuroReport</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1721" to="1724" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mother&apos;s face recognition by neonates: A replication and an extension</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pascalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De Schonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deruelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fabre-Grenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infant Behavior and Development</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Emotional expressions reinstate recognition of other-race faces in infants following perceptual narrowing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pascalis</surname></persName>
		</author>
		<idno type="DOI">10.1037/dev0000858</idno>
		<ptr target="https://doi.org/10.1037/dev0000858" />
	</analytic>
	<monogr>
		<title level="j">Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="27" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<ptr target="https://github.com/raviranjan0309/Detect-Facial-Features" />
		<title level="m">Detect-facial-features</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">GitHub</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning faces from variability</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470218.2015.1136656</idno>
		<ptr target="https://doi.org/10.1080/17470218.2015.1136656" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="897" to="905" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">300 Faces In-The-Wild Challenge: Database and results</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imavis.2016.01.002</idno>
		<ptr target="https://doi.org/10.1016/j.imavis.2016.01.002" />
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">300 Faces In-The-Wild Challenge: The first facial landmark localization challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCVW.2013.59</idno>
		<ptr target="https://doi.org/10.1109/ICCVW.2013" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</editor>
		<editor>
			<persName><surname>Chairs</surname></persName>
		</editor>
		<meeting>the 2013 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sendak</surname></persName>
		</author>
		<title level="m">EyeLink 1000 Plus User Manual</title>
		<imprint>
			<publisher>Author</publisher>
			<date type="published" when="1963">1963. 2013</date>
		</imprint>
	</monogr>
	<note>Where the wild things are</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Newborns&apos; face recognition over changes in viewpoint</title>
		<author>
			<persName><forename type="first">C</forename><surname>Turati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Simion</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2007.06.005</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2007.06.005" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1300" to="1321" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The JASP guidelines for conducting and reporting a Bayesian analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Doorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dablander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Derks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Draws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Etz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">F</forename><surname>Gronau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Haaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hinne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Š</forename><surname>Kucharský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matzke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R K N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarafoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Voelkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-020-01798-5</idno>
		<ptr target="https://doi.org/10.3758/s13423-020-01798-5" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="813" to="826" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Eye tracking reveals a crucial role for facial motion in recognition of faces by infants</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pascalis</surname></persName>
		</author>
		<idno type="DOI">10.1037/dev0000019</idno>
		<ptr target="https://doi.org/10.1037/dev0000019" />
	</analytic>
	<monogr>
		<title level="j">Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="744" to="757" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Are we face experts?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2017.11.007</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2017.11.007" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="100" to="110" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Becoming familiar with a newly encountered face: Evidence of an own-race advantage</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mondloch</surname></persName>
		</author>
		<idno type="DOI">10.1177/0301006618783915</idno>
		<ptr target="https://doi.org/10.1177/0301006618783915" />
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="807" to="820" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recognizing &quot;Bella Swan&quot; and &quot;Hermione Granger&quot;: No own-race advantage in recognizing photos of famous faces</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mondloch</surname></persName>
		</author>
		<idno type="DOI">10.1177/0301006616662046</idno>
		<ptr target="https://doi.org/10.1177/0301006616662046" />
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1426" to="1429" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
