<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">005767P SSXXX10.1177/09567976211005767SchafmeisterThe Effect of Replications on Citation Patterns</title>
				<funder ref="#_P4A82kV #_yXWHJtw">
					<orgName type="full">Svenska Handelsbanken Forskningsstiftelser</orgName>
				</funder>
				<funder>
					<orgName type="full">Marianne and Marcus Wallenberg Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Jan Wallander and Tom Hedelius Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Knut and Alice Wallenberg Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Felix</forename><surname>Schafmeister</surname></persName>
							<email>felix.schafmeister@phdstudent.hhs.se</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Stockholm School of Economics</orgName>
								<orgName type="department" key="dep2">Department of Economics</orgName>
								<orgName type="department" key="dep3">Department of Economics</orgName>
								<orgName type="institution">Stockholm School of Economics</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Stockholm School of Economics</orgName>
								<orgName type="department" key="dep2">Department of Economics</orgName>
								<orgName type="department" key="dep3">Department of Economics</orgName>
								<orgName type="institution">Stockholm School of Economics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">005767P SSXXX10.1177/09567976211005767SchafmeisterThe Effect of Replications on Citation Patterns</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AB940698DA8709F057E61D18BF0BC2E3</idno>
					<idno type="DOI">10.1177/09567976211005767</idno>
					<note type="submission">Received 8/17/20; Revision accepted 3/8/21</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-06-03T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>scientific communication</term>
					<term>statistical analysis</term>
					<term>open data</term>
					<term>preregistered</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Replication of existing research is often referred to as one of the cornerstones of modern science. In this study, I tested whether the publication of independent replication attempts affects the citation patterns of the original studies. Investigating 95 replications conducted in the context of the Reproducibility Project: Psychology, I found little evidence for an adjustment of citation patterns in response to the publication of these independent replication attempts. This finding was robust to the choice of replication criterion, various model specifications, and the composition of the contrast group. I further present some suggestive evidence that shifts in the underlying composition of supporting and disputing citations have likely been small. I conclude with a review of the evidence in favor of the remaining explanations and discuss the potential consequences of these findings for the workings of the scientific process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Replication of existing research is often referred to as one of the cornerstones of modern science. However, direct replications, defined as the same analyses being conducted on newly collected data using original materials, have historically been published infrequently. 1  Recently, a number of systematic, large-scale replication attempts have been published in prominent scientific journals (e.g., see <ref type="bibr" target="#b4">Camerer et al., 2016</ref><ref type="bibr" target="#b5">Camerer et al., , 2018;;</ref><ref type="bibr" target="#b18">Klein et al., 2018;</ref><ref type="bibr">Open Science Collaboration, 2015)</ref>, and the question of replicability has received a substantial amount of attention in subsequent academic publications and media outlets.</p><p>The replication projects published thus far have undoubtedly succeeded in raising awareness of methodological shortcomings with regard to the power of research designs and the pitfalls of postdiction, culminating in the advent of Registered Reports <ref type="bibr" target="#b24">(Nosek &amp; Lakens, 2014)</ref> and a widespread adoption of preregistration of research designs in psychology <ref type="bibr" target="#b25">(Nosek &amp; Lindsay, 2018)</ref>. However, little is known about how these replication attempts have shaped the way specific findings are perceived in the literature. In this study, I attempted to fill this gap by analyzing changes in yearly citation patterns of articles replicated in the context of the Reproducibility Project: Psychology (RP:P; Open Science <ref type="bibr">Collaboration, 2015)</ref> after the publication of its results in Science in 2015.</p><p>A single replication attempt, especially if insufficiently powered, provides only limited information about whether a prior research result is robust. However, I worked under the assumption that replication attempts can shift beliefs about the validity of an existing research finding <ref type="bibr" target="#b23">(McDiarmid et al., 2021)</ref>. In particular, replications that produce evidence in line with the original findings should strengthen beliefs in their validity, whereas nonsupportive or contradicting replications should weaken these beliefs <ref type="bibr" target="#b8">(Earp &amp; Trafimow, 2015)</ref>. This motivated my main research question: Does the publication of supportive versus nonsupportive replication attempts affect the frequency with which the underlying studies are cited? 2 I believe that the answer to this question could provide important insights into the workings of the scientific process by illuminating the extent to which new insights replace or strengthen existing knowledge and thereby shape the way new research is conducted.</p><p>I found that neither supportive nor nonsupportive replications have had a statistically significant effect on the number of times articles are cited. This null result is robust to different replication criteria and various model specifications as well as to alternative compositions of the contrast group. In investigating potential explanations for my findings, I present some suggestive evidence that shifts in the underlying composition of supporting and disputing citations have likely been small and discuss how the remaining explanations fit with the evidence.</p><p>Although I acknowledge that a number of forces are at the heart of my results, some of the contending explanations have particularly daunting consequences and therefore deserve additional attention: If researchers either were unaware of replication results or, in spite of their awareness, chose to discount them and continued to cite the original results at face value, this could considerably limit the self-corrective ability of the scientific process because it would reduce the likelihood that research results that have been called into question will be phased out and replaced by new insights. This could be particularly problematic in light of evidence by <ref type="bibr" target="#b15">Gneezy and Serra-Garcia (2021)</ref>, who report that studies that failed to replicate within three large-scale replication projects (and for which prediction markets were conducted) have higher citation counts than studies that replicated successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head><p>My empirical approach built on the sampling design of the RP:P (Open Science <ref type="bibr">Collaboration, 2012</ref><ref type="bibr">Collaboration, , 2015))</ref>. This project was a coordinated effort by the Center for Open Science that brought together a large and diverse group of more than 200 researchers and aimed to test the reproducibility of psychological science. Participating research teams chose studies to replicate from a predefined sampling frame and subsequently conducted independent replication attempts "using high-powered designs and original materials when available" <ref type="bibr">(Open Science Collaboration, 2015, abstract)</ref>. Ultimately, the results of 100 such replication attempts were included in the publication, which concluded that merely 36% of the replications had resulted in statistically significant results compared with 97% of the original studies.</p><p>Although a number of drawbacks in the design of the RP:P have been identified, it is one of the most comprehensive attempts to study replicability and followed a strict sampling protocol designed to minimize selection effects. This particular feature made it an appealing setting to study the present research question because it allowed for the definition of a contrast group against which the effect of replications can be evaluated.</p><p>Specifically, the sample of eligible studies for the RP:P was selected from the 2008 issues of the Journal of Experimental Psychology: Learning, Memory, and Cognition (JEP), Journal of Personality and Social Psychology (JPSP), and Psychological Science (PS). Replication teams chose studies from among the first 30 articles published in each of these journals beginning with the first 2008 issue; additional articles were made available in sets of 10 in case of additional demand. 3  This sampling strategy limits concerns regarding the selection of articles on the basis of observable or unobservable characteristics, which could complicate an analysis of citation patterns if those characteristics were correlated with both the likelihood of successful replication and citation patterns. Further, I expected that if the articles included in the RP:P sampling frame had not been selected for replication, they would have had similar citation patterns over time as studies published immediately prior or shortly after in the same journals.</p><p>The resulting variation therefore lends itself to be exploited in a generalized differences-in-differences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statement of Relevance</head><p>A crucial feature of the scientific process is its ability to self-correct by identifying and promoting robust results and phasing out those that fail to hold up to further scrutiny. This process ensures that through the cumulative generation of research and systematic replications, a reliable body of knowledge is created. This article provides a test of this self-corrective ability "in the medium run" by analyzing articles replicated in the context of the Reproducibility Project: Psychology. I investigated changes in citation patterns around the time the replication results were published, and my findings do not support the hypothesis that citation rates changed in response to independent replications. These results thus emphasize the need for replication on a large scale and the effective communication of replication results. design. This design compares changes in yearly citations after the publication of the replication results between the replicated and the contrast sample. Under the assumption of parallel trends-in the absence of the RP:P replications, the different sets of studies would have followed similar citation trajectories-this allowed me to identify the effect of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample selection</head><p>The definition of a contrast group is complicated by two notable deviations of the RP:P from its intended sampling frame. First, because of constraints regarding available instruments, samples, and knowledge among replicators, not all studies in the sampling frame could be replicated. Second, the sampling frame was not always followed, resulting in some studies being replicated despite being published in an issue that had not been made available for selection.</p><p>This introduces some degree of selection into the sample of replicated studies. The first deviation implies that studies with certain features were infrequently subjected to replications. This becomes an issue if these types of studies are also subject to different citation trends, thus potentially violating the common-trends assumption that underlies my identification of the effect of interest. Similarly, the second deviation suggests that some studies might have been selected by groups of researchers particularly interested in their results. Although the exact drivers of these choices are unclear, this interest might correlate with factors that affect citation trends and could therefore present a challenge to the identifying assumption.</p><p>To deal with these concerns, I implemented the following sample-selection protocol. I collected data for the 95 unique articles that had significant original findings and for which a replication was reported in the RP:P, thus excluding the three studies with insignificant original findings as well as the two duplicate replications. These data were combined with information from articles that were published in the same journals in adjacent months. This initial selection of eligible articles is detailed in Table <ref type="table" target="#tab_0">1</ref>. Within the sampling frame, all original research articles were considered eligible. Other material, such as editorials, commentaries, corrigenda, and book reviews, were excluded.</p><p>I further implemented a set of exclusion criteria intended to increase the comparability of the contrast and treatment groups. These exclusion criteria were designed to mimic the constraints faced by the replication teams in the RP:P, eliminating studies that use hard-to-access samples or that require specialized instruments that are not readily available in most laboratories. Further, I eliminated articles that did not include any experimental results that can be represented by a single statistical inference test or effect size. 4 I first coded the articles according to these criteria. To ensure the fidelity of the coding, a research assistant blind to the research question additionally coded a random subsample of 169 studies. The two classifications aligned in 97% of cases.</p><p>The sampling frame laid out in Table <ref type="table" target="#tab_0">1</ref> contains a total of 476 articles, of which 202 were published in PS, 144 in JPSP, and 130 in JEP. Applying the exclusion criteria left 329 articles, of which 117 were published in PS, 112 in JPSP, and 100 in JEP. This implies journal shares ranging from 30% for JEP to 36% for PS, which closely matches the percentages of articles replicated in the RP:P (29% for JEP to 41% for PS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable definitions</head><p>My main outcome variable was the number of citations that a publication received in a specific year. I collected these data from the Web of Science Core Collection between 2010 and 2019. This time range was chosen with the aim of constructing a balanced panel data set despite the fact that the sampling frame contains articles published between 2007 and 2009. The fact that the coverage of the Web of Science is limited within the social sciences <ref type="bibr" target="#b19">(Kousha &amp; Thelwall, 2007)</ref> was unlikely to represent a problem for my empirical strategy because there should not be any reason to expect that coverage of citing articles would have changed over time for different studies in my sample. In the main analyses, I applied an inverse hyperbolic sine (IHS) transformation to account for the nonnormality of the data. This transformation is conceptually similar to the approach of adding 1 unit to each observation and taking the natural logarithm recommended by <ref type="bibr" target="#b31">Thelwall and Wilson (2014)</ref> but is generally considered less arbitrary <ref type="bibr" target="#b1">(Bellemare &amp; Wichman, 2020)</ref>. Further, to address the research question, I required a measure to evaluate the degree to which a replication attempt supports the original finding. An intuitive criterion relies on the binary decision of whether the replication produces evidence that is statistically significant at the 5% level in the same direction as the original finding. The main point of contention in using this criterion is its inability to quantify the strength of the evidence contained in a replication coded as failed or successful. For example, a failed replication attempt might not be able to detect a true effect if it is itself underpowered, whereas a high-powered replication might uncover an effect that is orders of magnitude smaller than the original effect. The "small-telescopes" approach of Simonsohn ( <ref type="formula">2015</ref>) is based on the idea of detectability in that it tests whether the effect obtained in a replication study is smaller than an effect size that the original study had only 33% power to detect. In comparison, the Bayesian approach applied by <ref type="bibr" target="#b10">Etz (2015)</ref> is based on the calculation of Bayes factors (BFs) comparing the hypothesis of no effect with an alternative of the effect size found in the original publication, originally proposed by <ref type="bibr" target="#b32">Verhagen and Wagenmakers (2014)</ref>.</p><p>In the main specification, I employed the intuitive binary criterion, which has been used as the main replication criterion in the RP:P and has been widely communicated. In a secondary analysis, I instead used an alternative definition of failed replications based on the Bayesian approach outlined above. To ensure the comparability of my analyses across the different criteria, I collapsed the continuous BF into three categories. Specifically, I coded a replication as successful if it yielded a BF of 3 or more and as unsuccessful if it yielded a BF of one third or less; all remaining replications were coded as inconclusive. In Section S1 in the Supplemental Material available online, I further implemented similar analyses on the basis of the small-telescopes approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary statistics</head><p>The main criterion classified 35 replication attempts as successful and the remaining 60 as unsuccessful. The Bayesian criterion considered 31 replications successful, 34 unsuccessful, and 26 inconclusive. 5 The discrepancy in the number of articles coded is due to three studies missing effect-size estimates for either the original analysis or the replication attempt in the RP:P files. For one further study, the BF could not be calculated, resulting in a total of 91 replicated studies for which I had a valid classification.</p><p>Regarding the distribution of yearly citations, the average study in my estimation sample received 7.34 (SD = 7.91) citations, and the share of article years with no citations was 0.068. Further, yearly citations have been on an increasing trajectory even several years after the initial publication: On average, there were 6.46 (SD = 5.37) citations per year between 2010 and 2014 and 8.22 (SD = 8.81) yearly citations from 2015 to 2019. This reflects the fact that the articles under consideration are among the most influential in the field and thus have a long citation half-life <ref type="bibr" target="#b33">(Walters, 2011)</ref>. 6   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical strategy</head><p>The core of the estimation strategy lies in the assumption that in the absence of the RP:P replications, the studies included in the contrast group would have been subject to the same citation trends as those in the RP:P sample. Under this assumption, I could identify the effect of interest in a generalized differences-in-differences estimation framework by comparing changes in citation trends. I employed two closely related operationalizations: The first assumed that treatment effects are constant over time, whereas the second allowed for time-varying treatment effects.</p><p>The first model was</p><formula xml:id="formula_0">Y D D i t i t i t i t i t i t , ,<label>, , , , =</label></formula><formula xml:id="formula_1">+ + + + + α β ρ Successful Failed δ X Γ  (1)</formula><p>where Y i t , is the IHS-transformed number of citations for article i in year t, with t running from 2010 to 2019, and α i and β t indicate article and year fixed effects, respectively. D i t , Successful and D i t , Failed are indicator variables taking a value of 1 for observations from 2015 or later for which the replication attempt has been successful or unsuccessful, respectively. Finally, the matrix X i,t indicates time-varying, article-specific controls that are described in more detail below. The model is estimated with ordinary least squares (OLS), and standard errors allow for clustering at the article level.</p><p>The two sets of fixed effects α β i t and control for time-invariant, article-specific variation as well as articleinvariant, year-specific effects. In particular, assuming that beliefs about an article's replicability as well as their effect on citations are time invariant, the article fixed effects account for initial heterogeneity in beliefs about the likelihood of replication success. This is important because considerable heterogeneity likely exists in beliefs about the likelihood of replication success in the contrast group, and no survey measures are available to control for these beliefs. <ref type="bibr" target="#b7">Dreber et al. (2015)</ref> have shown that prediction markets, in which experts bet on the outcome of the replications, can be successful at predicting the results of replications, as can machine-learning algorithms <ref type="bibr" target="#b0">(Altmejd et al., 2019;</ref><ref type="bibr" target="#b35">Yang et al., 2020)</ref>. This suggests that researchers have meaningful priors about the likelihood of replication, which should correlate with the number of citations <ref type="bibr" target="#b15">(Gneezy &amp; Serra-Garcia, 2021)</ref>. Hence, these fixed effects alleviate concerns about articles that were replicated successfully receiving on average higher (or lower) citations than those in the contrast group because the latter is a mixture of studies that would replicate successfully and others that would not.</p><p>The second model differs from the first only in that it allows for time-varying treatment coefficients. It is given by</p><formula xml:id="formula_2">Y D D i t i t j S i t j j it j j it i t , ,<label>, , , , = +</label></formula><formula xml:id="formula_3">+ + + + ∈ α β Σ δ ρ Successful Failed X γ  , ,<label>(2)</label></formula><p>with S equal to {2010, . . ., 2019}\{2014}. The variables</p><formula xml:id="formula_4">D i t j ,</formula><p>, Successful take a value of 1 if an article was replicated successfully and the observation is from year j, and equivalently for D i t j , , Failed . The 2014 coefficients are normalized to 0 and thus serve as the reference categories against which the posttreatment coefficients will be compared.</p><p>This specification has two advantages. First, it allows one to gauge whether any effects of replications on citation patterns arise immediately or only with a lag as well as whether these effects persist over time. Second, the pre-2014 coefficients allow for an investigation of the identifying assumption: If the estimated coefficients prior to the publication of the RP:P results were significantly different from zero, this would call into question the assumption that replicated and nonreplicated articles would have followed similar citation trends in the absence of the RP:P.</p><p>It is important to note that the above exposition deviates from my preregistered analysis plan in a number of ways. 7 Most importantly, I specified Equation 2 as my main model and proposed to test my main hypothesis by testing for the joint significance of the post-2014 coefficients. I chose to deviate from this strategy because it did not take into account the direction of the estimated effects and depends strongly on the choice of base year. The results of this test and further explanations are provided in Section S1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Table <ref type="table" target="#tab_2">2</ref> presents four specifications of Equation 1 that differ in terms of which set of time-varying controls were included in the model. Row 2 presents the prespecified model equation including fixed effects for the number of years since publication. Rows 3 and 4 represent further attempts at accounting for potential deviations from the common-trends assumption by including separate year-fixed effects for each journal (Row 3) or allowing for issue-specific linear trends (Row 4). In all specifications, successful replications were positively associated with citation rates, δ  = 0 053</p><p>. , 95% confidence interval (CI) = [ . , . ] -0 09 0 195 , t( ) . 423 0 73 = , p = .468 , and failed replications were negatively associated with citation rates, ρ  = -0 035 . , 95% CI = [ -0 159 0 089 . , . ], t( ) . 423 0 56 = -, p = .576, but neither of these effects for large values of y, the coefficient estimates obtained in the main specification, taken at face value, suggest that successful replications led to an increase in yearly citations of around 5% and that unsuccessful replications led to a decrease in yearly citations of around 4%. For the average article in my sample, which has roughly eight citations per year, this would imply a change of ±1 citation every 2 to 3 years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Potential explanations</head><p>An obvious question is whether my inability to reject the null hypothesis is the result of the study being inadequately powered. On the basis of the estimated standard errors in the main specification, I had 80% power to detect an effect of around 0.2 in IHS units for successful replications and around 0.18 IHS units for failed replications at an α of .05, corresponding to 0.19 and 0.17 standard deviations, respectively. I thus conclude that my results are informative about effect sizes in this range but cannot make meaningful statements about the presence or absence of subtler effects. Although this important qualification should be taken into account in interpreting my results, I nonetheless emphasize that, as of yet, the RP:P represents the largest available sample to test my main hypothesis. It is my hope that future systematic replications will allow for the effect to be analyzed in even larger samples, thus enabling the investigation of smaller effect sizes.</p><p>Another possible explanation lies in the relatively short postreplication time window I had at my disposal. The citation count in the Web of Science is largely based on articles published in peer-reviewed journals. Hence, in order to be represented in the citation count, an article has to be written and pass through peer review, two processes that take a considerable amount of time. This could call into question my assumption of time-invariant treatment effects, a violation that could bias my estimates toward zero and hence drive the null result.</p><p>To investigate this concern, I present OLS estimates of Equation 2 in Figure <ref type="figure" target="#fig_0">1</ref>, including the additional set of fixed effects for years since publication. The plot reveals two main patterns. First, the estimated coefficients in the years up to the publication of the RP:P do not differ substantially from zero for either the successful or the unsuccessful replications, which increases confidence in the common-trends assumption underlying my previous estimation results. Second, the plot provides scant evidence for treatment effects arising in later years. Although some of the estimated yearly coefficients are significantly different from zero at the 5% level on the basis of a two-sided t test, no consistent pattern arises. Further, these effects are dependent on the normalization with regard to the base year 2014. This is reflected in the fact that already prior to 2014, all estimated coefficients for successful replications are positive, and all estimated coefficients for unsuccessful replications are negative. As a result, the coefficients would appear considerably more muted if compared with those of the average prereplication year. Overall, the figure provides little evidence that time-varying effects were driving the null result.</p><p>A third candidate explanation for the absence of a statistically significant effect is that the replication criterion did not account for the strength of the evidence obtained in the replication. This issue is particularly important because the power calculations underlying the RP:P replications were based on the original effect sizes. In the presence of a type-M error <ref type="bibr" target="#b13">(Gelman &amp; Carlin, 2014)</ref>, this practice is likely to result in replication attempts that are themselves not sufficiently powered to detect the effects of interest and hence could render some of the evidence too weak to shift researchers' beliefs. In line with this argument, the Bayesian analysis by <ref type="bibr" target="#b11">Etz and Vandekerckhove (2016)</ref> suggests that many original studies and their replications did not provide strong evidence for either the null or the alternative hypothesis, implying a limited need for belief updating in many cases. Thus, my measure of the effects of failed replications represents a weighted average of inconclusive and refuting information, which might shift the estimated coefficient toward zero.</p><p>I addressed this concern by classifying replication attempts according to the Bayesian criterion, thereby introducing an explicit distinction between failed and inconclusive replication attempts (see Table <ref type="table" target="#tab_3">3</ref>). The results were largely in line with those obtained , p = . , 98 replication attempts displayed a statistically significant effect. Although one has to be cautious in interpreting insignificant differences between coefficients, it is worth noting that the estimates in Table <ref type="table" target="#tab_3">3</ref> do not support my previous conjecture. If anything, the coefficient on inconclusive replications was more negative than that on failed replications. The corresponding time-varying coefficients are presented in Figure <ref type="figure">2</ref> and closely mirror the coefficients obtained in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness</head><p>In this subsection, I address some remaining concerns regarding the validity of my estimates along two dimensions.</p><p>First, as described in more detail in Section S2 in the Supplemental Material, I acknowledge that the inclusion criteria at best resulted in a rough approximation of the selection process of studies to be replicated in the RP:P. To ensure that my sample selection did not drive the results, I included all nonretracted studies in the contrast group in Row 1 of Table <ref type="table" target="#tab_5">4</ref>. This expansion of my estimation sample resulted in small gains in the precision of estimates and both successful, δ  = 0 064 . , 95% CI = [ . , . ] -0 074 0 202 , t( ) . 567 0 91 = , p = . , 362 and unsuccessful, ρ  = -0 024 . , 95% CI = [ . , . ] -0 143 0 095 , t(</p><p>) . 567 0 39 = -, p = .695, replications continued to yield insignificant coefficient estimates.</p><p>Second, I considered whether my results were sensitive to the choice of transformation applied to the outcome variable. Thus far, all regressions employed the IHS transformation. An alternative approach that has frequently been implemented when working with count data is adding a value of 1 to each observation and subsequently taking the logarithm. Although this approach has been criticized by some researchers for being inherently arbitrary <ref type="bibr" target="#b3">(Burbidge et al., 1988;</ref><ref type="bibr" target="#b6">Campbell &amp; Mau, 2020)</ref>, in Table <ref type="table" target="#tab_5">4</ref>, I present estimates of Equation <ref type="formula">1</ref>for both untransformed citation counts (Row 2) and the logarithmic transformation (Row 3). Although the coefficients are not directly comparable across models, the broad patterns are very similar, with both successful and unsuccessful replications resulting in small and statistically insignificant coefficient estimates-succesful untrans formed:</p><formula xml:id="formula_5">δ  = 0 056 . , 95% CI = [ -</formula><p>1 158 1 27 . , . ], t( ) . 423 0 09 = , p = .928; successful logarithmic: δ  = 0 044 . , -1 -.5 0 .5 1 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 Successful Replication Inconclusive Replication Failed Replication Estimated b Fig. 2. Estimated coefficients for successful, inconclusive, and failed replications across time. A replication is considered successful if it results in a Bayes factor of more than 3 and as unsuccessful if it yields a Bayes factor of less than 1/3. Error bars indicate 95% confidence intervals. Estimates were obtained from an ordinary least squares regression of Equation 2, in which the dependent variable is the inverse hyperbolic sine of yearly citations. The regression includes fixed effects for article, calendar year, and years since publication. Standard errors are clustered at the article level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The failure of my analyses to reject the null hypothesis that there was no effect of RP:P replications on yearly citation counts ran counter to my hypothesis that citation patterns should change as researchers adjust their beliefs about the validity of an existing research result. In the following, I outline a number of contending explanations for this null result and discuss the extent to which they are in line with the data.</p><p>First, a necessary condition for belief updating in response to replication attempts is researchers' awareness of the replication results. Previous findings of <ref type="bibr" target="#b28">Simkin and Roychowdhury (2005)</ref> suggest that a large number of citations are merely copied from existing reference lists and not actually read by the citing authors, making it likely that at least some researchers remain unaware of existing replications for the studies they cite.</p><p>Such inattention is likely exacerbated by the general difficulty of acquiring information about replication results. Unpublished replications are often difficult to find, but even if replication results are published, finding and evaluating them requires a substantial time investment from citing researchers. This concern carries particular weight in my setting because the RP:P was designed with the intention to draw conclusions about replicability on an aggregate level rather than to scrutinize individual research results. As a consequence, the outcomes of individual replication attempts were neither discussed in detail by the Open Science <ref type="bibr">Collaboration (2015)</ref>, nor were citations to the original studies included in their article, requiring researchers interested in the results of individual replication attempts to delve into the supplemental materials. This factor substantially qualifies the external validity of my findings because other replication studies might discuss individual replication outcomes in more detail and are more easily picked up by search engines if they are similar in title and include direct references to the original study. This increased visibility has the potential to alter the citation impact of a replication attempt compared with the effects that I uncovered in the context of the RP:P; indeed, the case studies by <ref type="bibr" target="#b16">Hardwicke et al. (2021)</ref> suggest that somewhat more marked effects might arise in other settings.</p><p>Second, even among researchers aware of the replication attempts, belief updating might have been limited. Although <ref type="bibr" target="#b23">McDiarmid et al. (2021)</ref> show that researchers updated their beliefs about the strength of a research finding in reaction to replications conducted in a number of large-scale replication projects (not including the RP:P), it is unclear to what extent these findings can be extrapolated to my setting. In particular, the authors note the possibility that experimenter demand and observer effects could have resulted in inflated estimates of researchers' true belief updating. Moreover, some authors of original studies that were replicated in the RP:P voiced concerns regarding the fidelity of the replication attempts (e.g., <ref type="bibr" target="#b2">Bressan, 2019;</ref><ref type="bibr" target="#b14">Gilbert et al., 2016</ref>; and replies to the RP:P published on OSF by the original authors). Although <ref type="bibr" target="#b9">Ebersole et al. (2020)</ref> show that the results of the RP:P replications were not sensitive to using peer-reviewed protocols, if citing researchers were nonetheless convinced that the replication attempts were not true to the original study, this might have weakened belief updating.</p><p>Other potential explanations could lie in articles gaining additional citations by being cited in the context of replications rather than for their content or in the citation count not taking into account citation content. Regarding the first argument, if this factor were to play a large role, one would expect to find an increase in citation rates for successful and inconclusive replications. In particular, because inconclusive replications were largely considered failures by the main criterion, these replications were likely among the most controversial and thus should have received the largest number of citations through this channel, a hypothesis that is not borne out by the present results.</p><p>Further, the second concern suggests that even if one cannot detect changes in total citation counts, the composition of supporting and disputing citations might have shifted. The analyses above are unable to directly shed light on the importance of this explanation because I am missing a reliable measure of citation content. Recently, a large-scale source of citation content classifications has become available through the website scite.ai, which uses deep learning to determine whether a citation supports, disputes, or merely mentions an existing research result. However, at the time of writing, the service is still in its beta stage and has only limited coverage. Hence, rather than subjecting these noisy measures to a formal statistical analysis, I present some suggestive evidence on the role of this channel.</p><p>According to the scite.ai classifications, only a small minority of citations are disputing or supporting existing findings. In the 10 years between 2010 and 2019, the average article in the RP:P sample has been subject to merely 0.83 disputing and 4.39 supporting citations, and 46% of the sample was never disputed. Moreover, investigating the timing of citations, I found little evidence that the frequency of disputing citations has been affected by the replication results. When the main replication criterion was used, studies that were replicated successfully received on average 0.4 disputing citations between 2015 and 2019, compared with 0.66 in the 5 years prior to replication, and studies that were replicated unsuccessfully received on average 0.38 disputing citations between 2015 and 2019, compared with 0.32 in the 5 years prior to replication. These numbers suggest that even if the RP:P replications shifted citation content, the size of these effects would likely be small.</p><p>In conclusion, my analyses fail to support the hypothesis that citation patterns adjust in response to the release of replication results. Among the potential reasons underlying these findings, a lack of attention to and the limited communication of replication results stand out as particularly important. These factors therefore have the potential to slow down the self-corrective ability of the scientific process and addressing them could represent an important step in maximizing the impact of recent advances to improve the quality and reliability of academic research. I am hopeful that technological advances such as scite.ai, with their potential to greatly improve the accessibility of the body of knowledge, can help to alleviate these issues in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency</head><p>Action Editor: Marc J. Buehner Editor: Patricia J. Bauer Author Contributions F. Schafmeister is the sole author of this article and is responsible for its content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Additional supporting information can be found at <ref type="url" target="http://journals.sagepub.com/doi/suppl/10.1177/09567976211005767">http://  journals.sagepub.com/doi/suppl/10.1177/0956797621100</ref> 5767 Notes 1. <ref type="bibr" target="#b22">Makel et al. (2012)</ref> showed that among all articles published in the top 100 psychology journals since 1900, roughly 1% contain a direct replication of an original result. <ref type="bibr" target="#b20">Makel and Plucker (2014)</ref>, <ref type="bibr" target="#b21">Makel et al. (2016)</ref>, <ref type="bibr" target="#b12">Evanschitzky et al. (2007)</ref>, and <ref type="bibr" target="#b17">Hubbard and Armstrong (1994)</ref> present similar results for educational sciences, special education, and marketing.</p><p>2. <ref type="bibr" target="#b30">Tahamtan and Bornmann (2018)</ref> review the manifold reasons underlying citations that prior research has identified. My research design assumed that at least some citations accrue from sources with the intent to build on the prior study.</p><p>3. A more detailed description of the sampling frame can be found in the article by the Open Science Collaboration <ref type="bibr">(2012,</ref><ref type="bibr">2015)</ref>. 4. A detailed description of the exclusions is presented in Section S2 in the Supplemental Material available online. 5. For one of the two articles that were replicated twice, the replication outcome according to the Bayesian criterion was ambiguous; one replication attempt was coded as failed and one as inconclusive. I coded this replication as inconclusive. 6. Section S2 presents more detailed summary statistics. 7. Because the observational nature of the data led to some unexpected complications, a number of deviations from the preregistration protocol were made. The Supplemental Material presents an explicit mapping of all changes from the preregistered analysis plan to the final article. The Supplemental Material further contains all analyses that were preregistered but not included in the present article, as well as some of the robustness tests. The main article and Supplemental Material in combination contain all preregistered analyses. The most important change was to the main estimation model. I prespecified the use of a model with time-varying coefficients but chose to present the coefficients from a timeinvariant model as my main analysis instead. The reason for this was that (a) it was noticed that the coefficient estimates were highly dependent on the year that was chosen as the base period, rendering the use of the time-invariant model more robust, and (b) the test I specified based on the timevarying model did not take into account the direction of the effect, which was crucial for the purpose of my study. The timevarying estimates are nonetheless presented in Figures <ref type="figure" target="#fig_0">1</ref> and <ref type="figure">2</ref>.</p><p>Further deviations from the preregistration were due to unanticipated reductions in sample size as a result of missing information. For example, I had not anticipated that my sampling frame contained studies that were subsequently retracted and hence did not have citation data available; in addition, the RP:P files contained a few missing values. 8. For concision, only the coefficients and p values from the preregistered specifications are noted in the text. Unless otherwise stated, reported p values are based on two-sided t tests. Following the recommendation of <ref type="bibr" target="#b34">Wilkinson (1999)</ref>, I report effect sizes as unstandardized regression coefficients.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Estimated coefficients for successful and failed replications across time. A replication is considered successful if it yields an effect in the same direction as the original and the replication p value is below .05. Error bars indicate 95% confidence intervals. Estimates were obtained from an ordinary least squares regression of Equation2, in which the dependent variable is the inverse hyperbolic sine of yearly citations. The regression includes fixed effects for article, calendar year, and years since publication. Standard errors are clustered at the article level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Sampling Frame</figDesc><table><row><cell>Journal and year</cell><cell>Volume</cell><cell>Issue</cell></row><row><cell>Psychological Science</cell><cell></cell><cell></cell></row><row><cell>2007</cell><cell>18</cell><cell>10, 11, 12</cell></row><row><cell>2008</cell><cell>19</cell><cell>5, 6, 8, 9, 10, 11</cell></row><row><cell>2009</cell><cell>20</cell><cell>1, 2, 3</cell></row><row><cell>Journal of Experimental</cell><cell></cell><cell></cell></row><row><cell>Psychology: Learning,</cell><cell></cell><cell></cell></row><row><cell>Memory, and Cognition</cell><cell></cell><cell></cell></row><row><cell>2007</cell><cell>33</cell><cell>5, 6</cell></row><row><cell>2008</cell><cell>34</cell><cell>4, 5, 6</cell></row><row><cell>2009</cell><cell>35</cell><cell>1, 2</cell></row><row><cell>Journal of Personality and</cell><cell></cell><cell></cell></row><row><cell>Social Psychology</cell><cell></cell><cell></cell></row><row><cell>2007</cell><cell>93</cell><cell>4, 5, 6</cell></row><row><cell>2008</cell><cell>94</cell><cell>6</cell></row><row><cell>2008</cell><cell>95</cell><cell>4, 5, 6</cell></row><row><cell>2009</cell><cell>96</cell><cell>1, 2, 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Differences-in-Differences Estimates of the Effect of Replications on Yearly Citations: Main CriterionThe table presents ordinary least squares estimates of Equation 1 with different sets of time-varying control variables. The outcome variable (yearly citations) has been transformed using an inverse hyperbolic sine to account for the nonnormality of the data. Successful replication is an indicator variable that takes the value of 1 if the article has been successfully replicated and t is greater than 2014. A replication is considered successful if it yields an effect in the same direction as the original effect and the replication p value is below .05. Standard errors allow for clustering at the article level and are presented in parentheses; p values were obtained from two-sided t tests.</figDesc><table><row><cell></cell><cell cols="2">Successful replication</cell><cell cols="2">Failed replication</cell><cell></cell><cell cols="2">Fixed effects</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Years since</cell><cell>Number of</cell></row><row><cell>Model</cell><cell>b</cell><cell>p</cell><cell>b</cell><cell>p</cell><cell>Article</cell><cell>Year</cell><cell>publication</cell><cell>observations</cell></row><row><cell>Baseline</cell><cell>0.037</cell><cell>.609</cell><cell>-0.051</cell><cell>.412</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>4,240</cell></row><row><cell></cell><cell>(0.072)</cell><cell></cell><cell>(0.062)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline + fixed effects</cell><cell>0.053</cell><cell>.468</cell><cell>-0.035</cell><cell>.576</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>4,240</cell></row><row><cell>for number of years</cell><cell>(0.073)</cell><cell></cell><cell>(0.063)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>since publication</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline + separate year</cell><cell>0.082</cell><cell>.260</cell><cell>-0.050</cell><cell>.424</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>4,240</cell></row><row><cell>fixed effects for each</cell><cell>(0.073)</cell><cell></cell><cell>(0.063)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>journal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline allowing for</cell><cell>0.102</cell><cell>.319</cell><cell>-0.010</cell><cell>.906</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>4,240</cell></row><row><cell>issue-specific linear</cell><cell>(0.102)</cell><cell></cell><cell>(0.082)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>trends</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Note:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Differences-in-Differences Estimates of the Effect of Replications on Yearly Citations: Bayesian CriterionThe table presents ordinary least squares estimates of Equation 1, further allowing for replications to be categorized as inconclusive. The outcome variable (yearly citations) has been transformed using an inverse hyperbolic sine to account for the nonnormality of the data. Successful replication is an indicator variable that takes the value of 1 if the article has been successfully replicated and t is greater than 2014. A replication is considered successful if it results in a Bayes factor of more than 3 and as unsuccessful if it yields a Bayes factor of less than 1/3. Standard errors allow for clustering at the article level and are presented in parentheses; p values were obtained from two-sided t tests.</figDesc><table><row><cell></cell><cell cols="2">Successful</cell><cell cols="2">Inconclusive</cell><cell>Failed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">replication</cell><cell cols="2">replication</cell><cell cols="3">replication</cell><cell></cell><cell></cell><cell></cell><cell>Fixed effects</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Number</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Years since</cell><cell>of</cell></row><row><cell>Model</cell><cell>b</cell><cell>p</cell><cell>b</cell><cell>p</cell><cell>b</cell><cell></cell><cell cols="2">p</cell><cell></cell><cell cols="2">Article</cell><cell>Year</cell><cell>publication</cell><cell>observations</cell></row><row><cell>Baseline</cell><cell>0.040</cell><cell>.585</cell><cell>-0.143</cell><cell>.144</cell><cell>-0.018</cell><cell cols="3">.807</cell><cell></cell><cell cols="2">Yes</cell><cell>Yes</cell><cell>No</cell><cell>4,200</cell></row><row><cell></cell><cell>(0.074)</cell><cell></cell><cell>(0.097)</cell><cell></cell><cell>(0.073)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline + fixed effects</cell><cell>0.057</cell><cell>.450</cell><cell>-0.127</cell><cell>.196</cell><cell>-0.002</cell><cell cols="3">.980</cell><cell></cell><cell cols="2">Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>4,200</cell></row><row><cell>for number of years</cell><cell>(0.075)</cell><cell></cell><cell>(0.098)</cell><cell></cell><cell>(0.073)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>since publication</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline + separate year</cell><cell>0.117</cell><cell>.131</cell><cell>-0.139</cell><cell>.162</cell><cell>-0.028</cell><cell cols="3">.700</cell><cell></cell><cell cols="2">Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>4,200</cell></row><row><cell>fixed effects for each</cell><cell>(0.077)</cell><cell></cell><cell>(0.099)</cell><cell></cell><cell>(0.072)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>journal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline allowing for</cell><cell>0.096</cell><cell>.375</cell><cell>-0.101</cell><cell>.342</cell><cell>0.032</cell><cell cols="3">.738</cell><cell></cell><cell cols="2">Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>4,200</cell></row><row><cell>issue-specific linear</cell><cell>(0.108)</cell><cell></cell><cell>(0.106)</cell><cell></cell><cell>(0.095)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>trends</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell> = 0 057 .</cell><cell>, 95% CI =</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">[ . , . -0 091 0 204 , t( ]</cell><cell>419</cell><cell>)</cell><cell>=</cell><cell>. 0 76</cell><cell>, p = .45 ; inconclusive,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">φ  = -0 127 .</cell><cell cols="4">, 95% CI = [ . , . -0 319 0 066 , t( ]</cell><cell>419</cell><cell>)</cell><cell>= -</cell><cell>. 1 30</cell><cell>,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">p = .196 ; nor failed, ρ  = -0 002 .</cell><cell>, 95% CI = [-0.146,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.143], t(</cell><cell cols="2">419</cell><cell>)</cell><cell>= -</cell><cell>. 0 02</cell></row></table><note><p>Note:</p><p>previously: Neither successful, δ</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Robustness Checks: Sample and Outcome Variables the values in Row 2 were obtained using raw citation counts, and the values in Row 3 were obtained by adding a value of 1 to each observation and taking the natural logarithm. Successful replication is an indicator variable that takes the value of 1 if the article has been successfully replicated and t is greater than 2014. A replication is considered successful if it yields an effect in the same direction as the original effect and the replication p value is below .05. Standard errors allow for clustering at the article level and are presented in parentheses; p values were obtained from two-sided t tests.</figDesc><table><row><cell></cell><cell cols="2">Successful</cell><cell>Failed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">replication</cell><cell cols="2">replication</cell><cell></cell><cell cols="2">Fixed effects</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Years since</cell><cell>Number of</cell></row><row><cell>Specification</cell><cell>b</cell><cell>p</cell><cell>b</cell><cell>p</cell><cell>Article</cell><cell>Year</cell><cell>publication</cell><cell>observations</cell></row><row><cell>All articles using IHS-</cell><cell>0.064</cell><cell>.362</cell><cell>-0.024</cell><cell>.695</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>5,680</cell></row><row><cell>transformed citation</cell><cell>(0.070)</cell><cell></cell><cell>(0.061)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>counts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Main sample using raw</cell><cell>0.056</cell><cell>.928</cell><cell>-0.480</cell><cell>.393</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>4,240</cell></row><row><cell>citation counts</cell><cell>(0.618)</cell><cell></cell><cell>(0.562)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Main sample using</cell><cell>0.044</cell><cell>.475</cell><cell>-0.032</cell><cell>.548</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>4,240</cell></row><row><cell>logarithmically</cell><cell>(0.061)</cell><cell></cell><cell>(0.053)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>transformed citation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>counts: ln(citations + 1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Note: The table presents ordinary least squares estimates of Equation1</p><p>. Row 1 uses the full sample for the contrast group, whereas Row 2 and Row 3 use the sample subjected to the exclusion criteria outlined in the text. The outcome variable (yearly citations) in Row 1 has been transformed using the inverse hyperbolic sine (IHS), whereas</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>I am thankful to <rs type="person">Anna Dreber Almenberg</rs>, <rs type="person">Oliver Engist</rs>, <rs type="person">Magnus Johannesson</rs>, <rs type="person">Brian Nosek</rs>, <rs type="person">Robert Östling</rs>, <rs type="person">Joakim Semb</rs>, and <rs type="person">Domenico Viganola</rs> for their helpful comments. <rs type="person">Vera Wellander Lindén</rs> provided excellent research assistance.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This research was supported by the <rs type="funder">Jan Wallander and Tom Hedelius Foundation</rs> (<rs type="funder">Svenska Handelsbanken Forskningsstiftelser</rs>; Grant Nos. <rs type="grantNumber">P18 0073</rs> and <rs type="grantNumber">P21 0091</rs>) and the <rs type="funder">Knut and Alice Wallenberg Foundation</rs>. Further support for this research was provided by a grant to <rs type="person">F. Schafmeister</rs>'s PhD supervisor, <rs type="person">Anna Dreber Almenberg</rs>, from the <rs type="funder">Marianne and Marcus Wallenberg Foundation</rs>.</p></div>
<div><head>Open Practices</head><p>All data and analysis code have been made publicly available via OSF and can be accessed at <ref type="url" target="https://osf.io/8vgm2">https://osf.io/8vgm2</ref>. The design and analysis plan for this study were preregistered at <ref type="url" target="https://osf.io/gct5x">https://osf.io/gct5x</ref>. A number of deviations were made from the preregistration (for more information, see Note 7). This article has received the badges for Open Data and Preregistration. More information about the Open Practices badges can be found at <ref type="url" target="http://www.psychologicalscience.org/publications/badges">http://www.psychologi  calscience.org/publications/badges</ref>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_P4A82kV">
					<idno type="grant-number">P18 0073</idno>
				</org>
				<org type="funding" xml:id="_yXWHJtw">
					<idno type="grant-number">P21 0091</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting the replicability of social science lab experiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Altmejd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dreber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Forsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johannesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Camerer</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0225826</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0225826" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">225826</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Elasticities and the inverse hyperbolic sine transformation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wichman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oxford Bulletin of Economics and Statistics</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="61" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Confounds in &quot;failed&quot; replications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bressan</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2019.01884</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2019.01884" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1884</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Alternative transformations to handle extreme values of the dependent variable</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Burbidge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Robb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">401</biblScope>
			<biblScope unit="page" from="123" to="127" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating replicability of laboratory experiments in economics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Camerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dreber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Forsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johannesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Almenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Altmejd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Heikensten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Holzmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Isaksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<biblScope unit="issue">6280</biblScope>
			<biblScope unit="page" from="1433" to="1436" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Camerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dreber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Holzmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johannesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Altmejd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Buttrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Forsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gampa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Heikensten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="637" to="644" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mau</surname></persName>
		</author>
		<title level="m">On trade induced technical change: The impact of Chinese imports on innovation, IT and productivity</title>
		<imprint>
			<publisher>New Economic School</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Working Paper w0264</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using prediction markets to estimate the reproducibility of scientific research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dreber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Almenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Isaksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johannesson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page" from="15343" to="15347" />
			<date type="published" when="2015">2015</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Replication, falsification, and the crisis of confidence in social psychology</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Earp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Trafimow</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.00621</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2015.00621" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">621</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Many Labs 5: Testing pre-data collection peer review as an intervention to increase replicability</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ebersole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Kidwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Buttrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Baranski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hartshorne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="331" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Etz</surname></persName>
		</author>
		<ptr target="https://alexanderetz.com/2015/08/30/the-bayesian-reproducibility-project/" />
		<title level="m">The Bayesian reproducibility project</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Bayesian perspective on the Reproducibility Project: Psychology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Etz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandekerckhove</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0149794</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0149794" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">149794</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Replication research&apos;s disturbing trend</title>
		<author>
			<persName><forename type="first">H</forename><surname>Evanschitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baumgarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business Research</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="411" to="415" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond power calculations: Assessing type S (sign) and type M (magnitude) errors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="651" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comment on &quot;Estimating the reproducibility of psychological science</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pettigrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<biblScope unit="issue">6277</biblScope>
			<biblScope unit="page" from="1037" to="1037" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonreplicable publications are cited more than replicable ones</title>
		<author>
			<persName><forename type="first">U</forename><surname>Gneezy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Serra-Garcia</surname></persName>
		</author>
		<idno type="DOI">10.1126/sciadv.abd1705</idno>
		<ptr target="https://doi.org/10.1126/sciadv.abd1705" />
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">1705</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Citation patterns following a strongly contradictory replication result: Four case studies from psychology</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Hardwicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szücs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Thibault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Crüwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Van Den Akker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ioannidis</surname></persName>
		</author>
		<idno type="DOI">10.31222/osf.io/wt5ny</idno>
		<ptr target="https://doi.org/10.31222/osf.io/wt5ny" />
	</analytic>
	<monogr>
		<title level="j">MetaArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Replications and extensions in marketing: Rarely published but quite contrary</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Research in Marketing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="248" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Many Labs 2: Investigating variation in replicability across samples and settings</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vianello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hasselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aveyard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Axt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Babalola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Š</forename><surname>Bahník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berkics</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bialobrzeska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Binan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bocian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Busching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="490" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Google Scholar citations and Google Web/URL citations: A multi-discipline exploratory analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kousha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1055" to="1065" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facts are more important than novelty: Replication in the education sciences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Makel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Plucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Researcher</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="304" to="316" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Replication of special education research: Necessary but far too rare</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Makel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Plucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coyne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remedial and Special Education</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="205" to="212" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Replications in psychology research: How often do they really occur?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Makel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Plucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hegarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="537" to="542" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Self-correction in psychological science: How do psychologists update their beliefs in response to replications? PsyArXiv</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Mcdiarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tullett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Whitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vazire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Smaldino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Stephens</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/hjcm4</idno>
		<ptr target="https://doi.org/10.31234/osf.io/hjcm4" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Registered reports: A method to increase the credibility of published results</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="137" to="141" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Preregistration becoming the norm in psychological science</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lindsay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APS Observer</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="19" to="21" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An open, large-scale, collaborative effort to estimate the reproducibility of psychological science</title>
		<author>
			<orgName type="collaboration">Open Science Collaboration</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="657" to="660" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Estimating the reproducibility of psychological science</title>
		<author>
			<orgName type="collaboration">Open Science Collaboration</orgName>
		</author>
		<idno type="DOI">10.1126/science.aac4716</idno>
		<idno>Article aac4716</idno>
		<ptr target="https://doi.org/10.1126/science.aac4716" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="issue">6251</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic modeling of citation slips</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Simkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Roychowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="367" to="384" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Small telescopes: Detectability and the evaluation of replication results</title>
		<author>
			<persName><forename type="first">U</forename><surname>Simonsohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="559" to="569" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Core elements in the process of citing publications: Conceptual overview of the literature</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tahamtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bornmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Informetrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="203" to="216" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regression for citation data: An evaluation of different methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Informetrics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="963" to="971" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian tests to quantify the result of a replication attempt</title>
		<author>
			<persName><forename type="first">J</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1457" to="1475" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The citation life cycle of articles published in 13 American Psychological Association journals: A 25-year longitudinal analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1629" to="1636" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Statistical methods in psychology journals: Guidelines and explanations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="594" to="604" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimating the deep replicability of scientific findings using human and artificial intelligence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Youyou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="10762" to="10768" />
			<date type="published" when="2020">2020</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
