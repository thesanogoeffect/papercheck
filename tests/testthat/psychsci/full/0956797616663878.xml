<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Creating Body Shapes From Verbal Descriptions by Linking Similarity Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Matthew</forename><forename type="middle">Q</forename><surname>Hill</surname></persName>
							<email>matthew.hill@utdallas.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Behavioral and Brain Sciences</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephan</forename><surname>Streuber</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carina</forename><forename type="middle">A</forename><surname>Hahn</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Behavioral and Brain Sciences</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alice</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Behavioral and Brain Sciences</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Behavioral and Brain Sciences</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
								<address>
									<addrLine>GR4.1 800 W. Campbell Road</addrLine>
									<postCode>75080-3021</postCode>
									<settlement>Richardson</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Creating Body Shapes From Verbal Descriptions by Linking Similarity Spaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6BAEAE1256ACD9C923FC12D2B22DF6C7</idno>
					<idno type="DOI">10.1177/0956797616663878pss.sagepub.</idno>
					<note type="submission">Received 1/6/16; Revision accepted 7/20/16</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-06-03T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual perception</term>
					<term>human body</term>
					<term>face perception</term>
					<term>open data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the importance of the body for recognition and social interaction, it is not surprising that language provides a rich corpus of colorful body-descriptor terms that allow people to communicate information about body shape to others. These terms can be combined in ways that give rise to vivid mental images. The "shapely, hourglass woman"; the "muscular, athletic young man"; and the "stout, portly gentleman" are imagined easily as complete human body shapes. The images conjured by these descriptions come from combinations of features that reference global shapes (e.g., pear-shaped) and local parts (e.g., long legs), as well as biologically relevant body structure (e.g., masculine) and mobility or strength potential (e.g., athletic, fit). These linguistic descriptions can provide a flexible framework for representing qualitatively diverse visual features about bodies that can exist 663878P SSXXX10.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>at different spatial scales and across multiple levels of abstraction.</p><p>How does one translate linguistic descriptions of people into visual images and visual images into descriptions? We explored the relationship between the physical dimensions of body shape and the words commonly used to describe bodies. We approached this question by constructing a multidimensional similarity-space representation of bodies using people's word-based descriptions of full-body photographs of clothed women. The proximity of two bodies in this language space represents the similarity of the descriptions applied to them. To link the language space to the physical variability of human body shapes, we collected verbal descriptions of three-dimensional graphics models of bodies <ref type="bibr" target="#b1">(Allen, Curless, &amp; Popovic´, 2003)</ref> and projected these descriptions into the language space, which provided physicalshape anchors to connect the linguistic descriptions to the three-dimensional shapes.</p><p>This approach was facilitated by an unexpected observation: The first five axes of the language space seemed to capture body-description "features" that could be used to label the axes of variability from a geometric shape space derived from three-dimensional laser scans of 2,094 bodies (cf. <ref type="bibr" target="#b2">Anguelov et al., 2005)</ref>. The order of the axes in the two spaces differed, but the potentially analogous structure of these multidimensional spaces offered a direct route for testing the relationship between body shapes and descriptions. Specifically, if the axes from the language and shape spaces coarsely correspond, it should be possible to generate a three-dimensional physical body shape of an individual from the verbal description of a photograph of that person.</p><p>In the context of the similarity space, a person's description takes the form of a point in the multidimensional language space. This point is specified by its coordinates in the space, which indicate where the person's description stands relative to the descriptions of other people on each axis in the space. Thus, it was possible to select individual points from the language space (i.e., based on descriptions of photographs), and after transposing axes to account for rank-order differences between the two spaces, place the points into the bodyshape space. We could then create three-dimensional graphics models of bodies at these locations in the shape space. We hypothesized that if sparse language captures the complex physical variability of bodies, descriptions of the three-dimensional bodies in the shape space should correspond to-or approximately "match"-the descriptions from which those bodies were generated.</p><p>To summarize our methodology and findings, we first created a mapping from semantics (i.e., word-based body descriptions) to three-dimensional body shape by linking five multivariate axes across two independently derived similarity spaces: One space was based on body descriptions, and the other space was based on body shapes. Second, we projected semantic descriptions of pictures into the body-shape space and generated three-dimensional models. Third, we showed that participants' descriptions of the bodies in the original pictures closely matched the descriptions of their corresponding three-dimensional models. These results indicate that language can act as a surprisingly effective code for body shape, capturing perceptually salient global and local body features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Participants were recruited from the subject pool at The University of Texas at Dallas (UTD) through an online sign-up system and were compensated with one research credit for a psychology course. The UTD Institutional Review Board approved all experimental procedures. Twelve people (all female) participated in a pilot study to validate the description terms. In the main experiment, 60 volunteers (30 female, 30 male) rated the body photographs, and another 60 volunteers (30 female, 30 male) rated the three-dimensional body reconstructions. There were no overlapping participants in the three groups. Because the study did not employ a traditional experimental design with manipulated variables, the goal was to include enough participants to achieve stable ratings of the bodies for the multivariate analysis. We conducted the multivariate analysis when we had tested 15 participants per block (45 total) and repeated the analysis with 20 per block (60 total). The interpreted axes remained stable with the additional participants. The data analysis reported includes the full set of participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>Screen captures of 164 women were taken from videos in the Human ID Database <ref type="bibr" target="#b19">(O'Toole et al., 2005)</ref> that show people's full bodies as they walked toward a camera. Two frames were selected from each video: a standing image and a midstride image. Standing images were captured from the first frame of the video (~13.6 m from the camera) or from the earliest frame showing the person at rest with hands at her sides. Midstride images were captured from the last full-stride frame in which the entire body was visible. Standing images were somewhat blurrier than midstride images because of the difference in the distance from the camera.</p><p>The image frames were cropped to remove excess background and resized to a uniform height of 900 pixels, with each image's original aspect ratio preserved. The Adobe Photoshop CS5 (Version 12.0) sponge filter was applied to the whole image to obscure facial detail. This filter also preserves and sharpens body contours. Figure <ref type="figure" target="#fig_0">1a</ref> shows an example of the processed body photographs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptor terms</head><p>Descriptor terms were sourced first from online dating profiles and clothing store fit recommendations to produce an initial list. Next, a pilot study was conducted to refine this list. Participants in this pilot study freely described the bodies in the images by typing a short description of each person's body type in a small text box. They were told to ignore the face, clothing, hair, and race. The initial list was augmented using some of the words that appeared commonly in the pilot study. The final list of terms captured global shape features, such as "round" and "rectangular"; local features, such as "long legs" and "short torso"; gender-related terms, such as "curvy" and "masculine"; and health-related terms, such as "muscular" and "fit." Table <ref type="table" target="#tab_0">1</ref> lists the 27 body descriptor terms used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>On each trial, participants simultaneously viewed a person's standing and midstride image, along with the list of 27 descriptor terms (see Fig. <ref type="figure" target="#fig_0">1</ref>). For each body, participants clicked one of three radio buttons to indicate whether that descriptor did not apply, applied somewhat, or applied perfectly to the body. At the outset of each trial, the radio buttons were all set to "does not apply." This made the task less tedious because participants usually selected only a small number of terms that applied somewhat or perfectly to the person. Trials were self-paced. Data ratings were carried out on 165 females and 60 males. 1 Because of the length of the task, photos were counterbalanced such that each participant saw a different subset of 75 of the total 225 people. Across all participants, a full set of ratings was obtained from 20 participants for each body.</p><p>Language-based body-description space. The language space was constructed using correspondence analysis <ref type="bibr" target="#b4">(Benzécri, 1976;</ref><ref type="bibr" target="#b12">Greenacre, 2007)</ref>-a multivariate analysis method for categorical data, similar in form to principal component analysis. Correspondence analysis was applied to ratings of 164 of the 165 female body photographs using only ratings of "applies perfectly." 2 One female, who was perceived consistently as a male, 3 was omitted from the analysis. The input to the correspondence analysis was an I × J matrix, X, of counts tallied across raters, where I was the number of bodies, and J was the number of descriptors. X ij contained the number of participants who rated the jth descriptor as applying perfectly to the ith body. The categorical nature of the data, expressed in a χ 2 contingency table, supports a biplot visualization of the rows (bodies) and columns (descriptors) in the same space. Individual axes were interpreted using the contribution scores of the descriptor terms. These indicate the importance of a term for establishing a component and are defined formally as the ratio of the squared factor score to the axis eigenvalue <ref type="bibr" target="#b0">(Abdi &amp; Williams, 2010)</ref>. Contributing descriptors were selected using a rule of thumb from <ref type="bibr" target="#b0">Abdi and Williams (2010)</ref> that assigns importance to contribution scores greater than 1/n, where n is the number of descriptors.</p><p>As applied here, the correspondence analysis produced a multidimensional representation that enabled visualization of the bodies and descriptor terms in a common space. The distance between bodies in this space is a measure of the similarity of the linguistic descriptions applied to them. Because the terms and bodies coexist in the same space, each axis was interpreted by finding the descriptor terms with the highest axis-contribution scores. Interpretations were made by comparing terms with large contribution scores that projected to opposite (i.e., positive and negative) sides of an axis.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the first four axes of the space and provides a qualitative interpretation of each. The first two axes roughly correspond to weight and height. The next two axes are related to different aspects of feminine appearance: Axis 3 spans classically feminine shapes (pear-shaped and curvy) versus women with various "other," not classically feminine shapes, and Axis 4 spans masculine women versus curvy women. The fifth axis (not plotted in Figs. <ref type="figure" target="#fig_1">2a</ref> and <ref type="figure" target="#fig_1">2b</ref>) was interpretable as waist-height or torso-to-leg-length ratio (see Figs. <ref type="figure" target="#fig_1">2c</ref> and <ref type="figure" target="#fig_1">2d</ref>).</p><p>Body-shape space. The geometric shape space was an extended version of the Shape Completion and Animation of People (SCAPE) model of body pose and shape variation applied to data from laser scans of people <ref type="bibr" target="#b2">(Anguelov et al., 2005)</ref>. In the SCAPE model, body shape is represented in terms of 3 × 3 deformation matrices <ref type="bibr" target="#b30">(Sumner &amp; Popovic ´, 2004</ref>) consisting of transformations of triangles in a template mesh into triangles in an instance mesh (see <ref type="bibr" target="#b2">Anguelov et al., 2005</ref>, for full details). A template mesh with 86,200 triangles was aligned (registered) to 2,094 laser scans of women from the Civilian American and European Anthropometry Resource Project (CAESAR) data set (cf. <ref type="bibr" target="#b22">Piryankova et al., 2014)</ref>. CAESAR contains full-body laser scans of American and European volunteers between the ages of 18 and 65 years; all are wearing bicycle-style shorts, and women are also wearing a sports bra. The alignment process <ref type="bibr" target="#b15">(Hirshberg, Loper, Rachlin, &amp; Black, 2012</ref>) puts all the shapes into correspondence, which enables statistical analysis. Further, the SCAPE representation separates pose and body shape, which allows analysis of only body shape. A low-dimensional shape space was created by applying principal component analysis to the "shapes" of these bodies, defined as the triangle deformations of the 2,094 aligned template bodies. This gave a three-dimensional-morphable representation of bodies that allowed for smooth transitions of body shape across arbitrary multivariate trajectories in the space.</p><p>Although the principal component axes are purely geometric, they have approximate linguistic interpretations (Fig. <ref type="figure" target="#fig_2">3a</ref>). Axes of the shape space were interpreted by visual inspection of bodies produced by adding and subtracting three standard deviations to and from each principal component (Fig. <ref type="figure" target="#fig_2">3a</ref>). We tentatively interpreted the shape space axes as representing (a) weight, (b) masculine versus curvy, (c) height, (d) waist height, and (e) classically feminine shapes versus other shapes. These labels were chosen from the labels we applied to the first five axes of the language space, but with the rank order  of the axes shifted between the two spaces. Specifically, we hypothesized parity between the following axes: (a) the first axis in both the language and shape space (weight), (b) Axis 2 in the language space and Axis 3 in the shape space (height), (c) Axis 3 in the language space and Axis 5 in the shape space (classically feminine shapes vs. other), (d) Axis 4 in the language space and Axis 2 in the shape space (masculine vs. curvy), and (e) Axis 5 in the language space and Axis 4 in the shape space (waist height).</p><p>Generating three-dimensional body models from descriptions. Given the potentially analogous interpretation of the first five axes of the language and shape spaces, we aligned the two spaces by reordering the axes in the shape space to match the analogously interpreted axes in the language space. Next, we approximated the shapes of the 164 photographed individuals by synthesizing their bodies in the shape space at positions specified by their locations (coordinates) in the language space. Figure <ref type="figure" target="#fig_2">3b</ref> shows a schematic of this process. We began</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linguistic Subspace</head><p>(5-space )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape Subspace</head><p>(5-space ) shows the process whereby three-dimensional body models of people in photographs were generated from the descriptions of the photographs. These descriptions were used to generate a five-dimensional language subspace that was aligned with the five-dimensional shape subspace on the basis of analogously interpreted axes. Corresponding points in the two subspaces were then used to generate the body models. Example pairs of photographed bodies and their corresponding approximated three-dimensional bodies (c) are also shown.</p><p>with the coordinates of each person on the first five (interpreted) dimensions of the language space and constructed a corresponding three-dimensional body using the first five dimensions of the shape space. We did this for all of the 164 photographed bodies. For each body, synthesis in the shape space was accomplished by a linear combination of the principal components, where the weight applied to each principal component was the projection coefficient or "coordinate" specified by its position in the language space on the analogous dimension. More concretely, the principal component shapespace model allows for a low-dimensional representation of body shape in the subspace U, defined by the first five "interpreted" dimensions. An individual shape S j is represented by a set of five linear coefficients, β j , that represent a body's coordinates with respect to the principal components. Thus, a body at a position in this fivedimensional subspace is approximated as S j = U j β j + u, where u is the average body (cf. <ref type="bibr" target="#b22">Piryankova et al., 2014)</ref>. To create body models of the 164 photographs using their coordinates in the language space, we standardized their factor scores on the first five axes. Next, we reordered the coordinates of the photographed bodies in the language space to match the analogously interpreted axes in the shape space. These new standardized coordinates were used to synthesize bodies at these positions in the shape space. Thus, the modeled body was created as a weighted sum of the first five principal components, with the coordinate vectors serving as the weights. This produced 164 synthetic geometric body models, which we rendered in two poses (standing and midstride) to match the photographic bodies. Figure <ref type="figure" target="#fig_2">3c</ref> shows example photographs and their corresponding synthesized three-dimensional bodies. A casual visual inspection shows that these appear quite similar to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human descriptions of three-dimensional bodies.</head><p>To formally test the resemblance between the two-dimensional photographs and their three-dimensional synthesized body approximations, a new set of participants rated the synthesized bodies using a procedure identical to that used for the body photographs but with the rendered models replacing the photographs (see Fig. <ref type="figure" target="#fig_0">1b</ref>). For the analysis, we represented these data as a 27-element description vector that contained the frequency with which these descriptors were judged by participants to apply perfectly to the body, as was done for the photographs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>To test the perceptual similarity between the photographed bodies and the three-dimensionally rendered bodies, we projected the description vectors for the three-dimensional body models as supplementary points onto axes of the language space. If the models resemble the photographs, we would expect the description vector projections (points) to be close in the multivariate language space to the descriptions (points) of the photographed bodies used to create them. For brevity, we use the term true-match pair to refer to a pair of points in the language space representing, respectively, a three-dimensional body's description point and the description point of the photographed body from whose coefficients it was generated. Nonmatched pairs refer to a three-dimensional body's description point and the description point of an unrelated photographed body. Figure <ref type="figure">4</ref> shows a schematic of the method used for determining the similarity of descriptions for the photographed and three-dimensional body models.</p><p>A bootstrap hypothesis test was used to determine whether the descriptions of true-match pairs were closer in the language space than descriptions of randomly sampled nonmatched pairs. For 1,000 iterations, we selected random samples of 164 nonmatched pairs and computed the mean Euclidian distance between pair descriptions in the five-dimensional language space. This yielded a distribution of nonmatched sample means that we compared with the average true-match distance. For inferential purposes, we selected a two-tailed cutoff value of p &lt; .05. In the top left graph in Figure <ref type="figure" target="#fig_4">5</ref>, the matched mean is shown along with the histogram of the 1,000 nonmatched samples. There is a wide separation between the matched mean and the nonmatched bootstrap distribution, with no overlap between the mean of the true-match pairs (M = 0.5626) and the distribution of 1,000 nonmatched sample means (M = 1.1449, 95% confidence interval, or CI <ref type="bibr">= [1.1428, 1.1469]</ref>). This indicates that the descriptions of the true matches were more similar to each other than were descriptions of random nonmatched pairs. Therefore, the language-based descriptions of photographed bodies were sufficient to synthesize three-dimensional reconstructions of body shapes that matched these descriptions. This synthesis was accomplished by linking a language space, derived from a handful of descriptor terms, to a shape space derived from a large and independent sample of human bodies.</p><p>Next, we asked whether the resemblance was based on the pattern of variation captured by the combination of all five dimensions in the language subspace or by one or two perceptually salient dimensions. To dissect the role of individual axes of variation in resemblance, the distances between the projected and original points were recomputed along single dimensions in the language space corresponding to (a) weight, (b) height, (c) classically feminine versus other, (d) masculine versus curvy, and (e) waist height. Figure <ref type="figure" target="#fig_4">5</ref>  For the masculine-versus-curvy axis (Axis 4), the true pairs were marginally more similar than the descriptions made to random pairs (true-match: M = 0.2693; nonmatched: M = 0.3018, 95% CI = [0.3007, 0.3029], p = .067). These results indicate that four of five dimensions contributed significantly to the overall resemblance between the photographs and three-dimensional models. The remaining dimension was related marginally (p = .067) to the shape variation. Given that Axis 4 was not significant, we recomputed the bootstrap on a fourdimensional subspace, eliminating this axis. As was the case for the full five-dimensional subspace, there was no overlap between the mean of the matched identity distances and the histogram of means for sampled nonmatched identities (subspace of Axes 1, 2, 3, and 5; truematch: M = 0.3963; nonmatched: M = 1.0734, 95% CI = [1.0712, 1.0756]).</p><p>A closer look at the data also suggested that this resemblance was sufficient to support body-shape categorization but not identification as a unique individual. This finding is consistent with humans' preferential reliance on faces for identification <ref type="bibr" target="#b5">(Burton, Wilson, Cowan, &amp; Bruce, 1999;</ref><ref type="bibr" target="#b14">Hahn et al., 2016;</ref><ref type="bibr" target="#b27">Robbins &amp; Coltheart,</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptor Space</head><p>Descriptor Space Fig. <ref type="figure">4</ref>. Schematic illustration of the method for measuring resemblance in the language space between photographed bodies (blue circles) and three-dimensional body models (red circles). As illustrated, each three-dimensional body model is projected as a point into the language space using its description vector. The dotted purple lines indicate true-match pairs (i.e., a photo and its corresponding model body). An example of a true-match pair is circled at the upper left of the language space. The oval at the bottom highlights a nonmatched pair, illustrating the distance between descriptions of a photographed body and an unrelated threedimensional body model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2012</head><p>). Because categorical information supports social and affective judgments that rely on coarse shape (e.g., femininity, athleticism, obesity), particular body descriptions apply accurately to many people. Therefore, the projection of a description of a three-dimensional body reconstruction should be close, not only to the body whose description was used to create it but also to other categorically similar bodies. Evidence for this categorical role can be seen by looking at the rank of the proximity of the true match with respect to all other nonmatched pairs (Fig. <ref type="figure" target="#fig_5">6</ref>). The description of the three-dimensional body was the closest point (i.e., body description) to matched photograph's description in only 8% of the 164 cases, but it was among the 10 closest points in 40% of cases and among the 30 closest points in 80% of the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Language can act as a concise code for complex body shapes, which thereby provides a support structure for the social and interpersonal judgments people make about others on the basis of their bodies. Linguistic body descriptors are compact but can operate as powerful perceptual discriminators. Here, the primary axes of variation that emerged from short, language-based descriptions</p><p>Five-Dimensional Language Subspace Mean Distance 0.4 0.6 0.8 1.0 1.2 1.4 0.0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6 Axis 1: Weight Random Pairs Matched Pairs 95th Percentile Axis 2: Height Axis 3: Pear-Shaped vs. Other 0.20 0.30 0.40 0.50 Axis 4: Masculine vs. Curvy 0.20 0.25 0.30 0.35 0.40 Axis 5: Waist Height 0.15 0.20 0.25 0.30 Mean Distance Mean Distance Mean Distance Mean Distance Mean Distance of a modest number of people mirrored those extracted from high-resolution three-dimensional shape measurements captured from a far larger number of people. This provided a simple and direct method for translating between verbal descriptions of people and complete human body shapes. It is worth noting that an easily visible parity of axis interpretation (e.g., weight, height, waist-height) for the language and shape space is not necessary to establish a computationally valid mapping between descriptions and body shapes. This is because standard machine-learning techniques can be applied to learn these mappings using a sample of descriptions and their associated body shapes (cf. <ref type="bibr" target="#b29">Streuber et al., 2016)</ref>. The direct machine-learning approach bypasses the underlying psychology of the language description space that ultimately makes it possible to generate bodies using the capacity of language to encapsulate a diverse set of visual features. These complex features underlie such concepts as "stout, portly gentlemen" and "shapely, hourglass women." Language is fundamentally a tool for conveying meaning, with "meaning" in this case referring to body shape. A fundamental task of the visual system is to convert retinal images into representations of meaningful objects. Early visual processing extracts generic image-based features by filtering retinotopic data. High-level visual cortex, however, is organized around categories of objects <ref type="bibr" target="#b13">(Grill-Spector &amp; Weiner, 2014)</ref>. Indeed, body-selective cortical areas in the ventral visual stream <ref type="bibr" target="#b10">(Downing, Jiang, Shuman, &amp; Kanwisher, 2001;</ref><ref type="bibr" target="#b20">Peelen &amp; Downing, 2005)</ref> are anatomically distinct, even from presumably "related" face-selective areas <ref type="bibr" target="#b28">(Schwarzlose, Baker, &amp; Kanwisher, 2005)</ref>.</p><p>Remarkably little is known about the visual features that operate within category-specific cortex. Using bodies as a microcosm, the present results suggest that high-level visual features might be better modeled with linguistic constructs than with multipurpose shape primitives. By contrast to features in early visual processing, these highlevel features would be category specific and would provide a flexible framework for representing qualitatively diverse, category-specific visual information (local, global, gender-specific, strength, and movement potential information) about bodies. This information can exist at different spatial scales and across multiple levels of abstraction. The perceptual relevance of these kinds of features provides psychological grounding for computational vision work that has harnessed human-sourced language-based descriptions of images to train machine-learning algorithms. This approach has made impressive progress on long-standing challenges in classifying objects <ref type="bibr" target="#b11">(Farhadi, Endres, Hoiem, &amp; Forsyth, 2009;</ref><ref type="bibr" target="#b17">Lampert, Nickisch, &amp; Harmeling, 2014;</ref><ref type="bibr" target="#b31">Yumer, Chaudhuri, Hodgins, &amp; Kara, 2015)</ref>, faces <ref type="bibr" target="#b16">(Kumar, Berg, Belhumeur, &amp; Nayar, 2011)</ref>, human activities <ref type="bibr" target="#b32">(Ziaeefar &amp; Bergevin, 2015)</ref>, and scenes <ref type="bibr" target="#b18">(Li &amp; Li, 2007)</ref>.</p><p>In applied terms, eyewitnesses describe suspects' bodies as commonly as their faces <ref type="bibr" target="#b24">(Reid, Nixon, &amp; Stevenage, 2014)</ref>, which indicates a pressing need for mechanisms that translate between body descriptions and shapes. Comparative judgments of even single-word body descriptors (i.e., taller, heavier) can be used in computational applications to improve body descriptors for identification <ref type="bibr" target="#b24">(Reid et al., 2014)</ref>. Our approach goes beyond comparative descriptions but is constrained by demographics, language, and culture. Language and body-shape mappings will vary across ethnicity and demographics (e.g., age, sex), and particular mappings will apply only among people sharing the same language and cultural views of body shape. Therefore, tuning these mappings to subpopulations is needed to optimize them for forensic applications. Notwithstanding, it is possible to engineer the general principles uncovered here with a classical machine-learning approach to make flexible and potentially customizable systems for mapping between descriptions and body shapes (see <ref type="bibr" target="#b29">Streuber et al., 2016)</ref>. Although customized mappings that work for particular languages and bodies from specific ethnic and racial groups remain a topic for future research, these tools could be useful for studying cultural and health-related aspects of person perception.</p><p>From a broader perspective, the effective use of language to guide body-shape analysis highlights the need for high-level visual representations to stand at the interface between understanding complex shapes and making this understanding available to others through language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Editor</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Example rating screen for (a) photographed bodies and (b) three-dimensional body models. For each stimulus, participants viewed one standing image and one midstride image simultaneously, and they clicked one of three radio buttons to indicate whether each of 27 descriptor terms applied perfectly, applied somewhat, or did not apply to that particular stimulus. Photographed bodies were shown with a filter applied to the whole image to obscure facial detail.</figDesc><graphic coords="4,61.50,69.00,480.00,582.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FemaleFig. 2 .</head><label>2</label><figDesc>Fig.2. Axes of the language space describing female body types. Plots of (a) Axes 1 and 2 and (b) Axes 3 and 4 illustrate the relationship between body images (gray circles) and descriptor words (blue circles). The size of the blue circles reflects the relative magnitude of their contribution scores. The percentage of explained variance is indicated next to all axis labels. The descriptor terms with the highest contribution scores for each of the five axes are shown in (c). Terms with negative projection scores appear on the left, and terms with positive scores appear on the right. The descriptor words with higher contribution scores appear in darker fonts. Semantic interpretations of the axes (d) are based on the descriptors with the highest contribution scores at the opposite ends of each axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Shape-space bodies and their relationship to photographed bodies. The renderings in (a) illustrate the first five axes of the shape space. For each component, a body 3 standard deviations above the origin and 3 standard deviations below the origin is shown. The schematic (b) shows the process whereby three-dimensional body models of people in photographs were generated from the descriptions of the photographs. These descriptions were used to generate a five-dimensional language subspace that was aligned with the five-dimensional shape subspace on the basis of analogously interpreted axes. Corresponding points in the two subspaces were then used to generate the body models. Example pairs of photographed bodies and their corresponding approximated three-dimensional bodies (c) are also shown.</figDesc><graphic coords="6,340.43,84.36,203.06,411.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>shows the results of a bootstrap test of the sample means of the nonmatched pairs along the individual axes. There was no overlap (p &lt; .001) between the mean of the true-match pairs and the bootstrap histogram for the axes corresponding to weight (Axis 1; true-match: M = 0.1710; nonmatched: M = 0.6961, 95% CI = [0.6935, 0.6988]), height (Axis 2; true-match: M = 0.1890; nonmatched: M = 0.4214, 95% CI = [0.4199, 0.4229]), and classically feminine shapes versus other shapes (Axis 3; true-match: M = 0.2187; nonmatched: M = 0.3750, 95% CI = [0.3737, 0.3763]). For the waist-height axis, the descriptions of the true-match pairs were significantly more similar to each other than the bootstrapped nonmatched pairs (Axis 5; true-match: M = 0.1976; nonmatched: M = 0.2329, 95% CI = [0.2320, 0.2338], p = .013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results of the bootstrap test of pair distances in the language space. Each panel shows the placement of the mean of distances between members of true-match pairs, along with the histogram of mean distances of nonmatched identity pairs. Results are shown separately for the five-dimensional language subspace and each individual axis within that subspace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Cumulative percentage of matched pairs as a function of ranked proximity in the five-dimensional language subspace. The dashed lines serve as a visual aid to show the percentage of matched identity pairs at ranks 1, 10, and 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Body Descriptor Terms Used in the Experiment</figDesc><table><row><cell>proportioned</cell><cell>short legs</cell><cell>pear-shaped</cell></row><row><cell>rectangular</cell><cell>long legs</cell><cell>long</cell></row><row><cell>stocky</cell><cell>long torso</cell><cell>curvy</cell></row><row><cell>big</cell><cell>short torso</cell><cell>masculine</cell></row><row><cell>muscular</cell><cell>small</cell><cell>tall</cell></row><row><cell>average</cell><cell>lean</cell><cell>sturdy</cell></row><row><cell>skinny</cell><cell>round (apple)</cell><cell>built</cell></row><row><cell>fit</cell><cell>petite</cell><cell>heavyset</cell></row><row><cell>short</cell><cell>broad shoulders</cell><cell>feminine</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Jake Swindle</rs> and <rs type="person">Linda Nguyen</rs> for assisting in data collection.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>M. Q. Hill, C. A. Hahn, and A. J. O'Toole designed the experiment. M. Q. Hill programmed the experiment and collected and analyzed the data. S. Streuber and M. J. Black provided geometric body stimuli and assisted in creating the sampling methodology. C. A. Hahn assisted in data analysis. All authors wrote the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Practices</head><p>All data have been made publicly available via the Open Science Framework and can be accessed at <ref type="url" target="https://osf.io/32tsq/">https://osf.io/32tsq/</ref>. The complete Open Practices Disclosure for this article can be found at <ref type="url" target="http://pss.sagepub.com/content/by/supplemental-data">http://pss.sagepub.com/content/by/supplemental-data</ref>. This article has received the badge for Open Data. More information about the Open Practices badges can be found at <ref type="url" target="http://pss.sagepub.com/content/25/1/3.full">https://  osf.io/tvyxz/wiki/1.%20View%20the%20Badges/</ref> and <ref type="url" target="http://pss.sagepub.com/content/25/1/3.full">http://pss  .sagepub.com/content/25/1/3.full</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notes</head><p>1. We report only the analysis of female bodies to focus more on the language-vision relationship than on the actual emergent features for a particular demographic. We assume that features will differ for sex, race, and age groups. We chose to report the analysis for females rather than males, because the data set contained fewer males (n = 60) than females (n = 165). See <ref type="bibr" target="#b29">Streuber et al. (2016)</ref> for a related machine-learning approach applied to male bodies. 2. The "applies perfectly" term was used for the analysis to declutter the correspondence-analysis visualization. The interpretation of axes was the same when all response choices were input to the correspondence analysis. 3. In a space with male and female bodies, this body clustered with males on Axis 1, which otherwise separated bodies by sex. We agreed that this body appeared male.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Correspondence analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.4135/9781412961288.n83</idno>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Research Design</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Salkind</surname></persName>
		</editor>
		<meeting><address><addrLine>Thousand Oaks, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Sage</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="267" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The space of human body shapes: Reconstruction and parameterization from range scans</title>
		<author>
			<persName><forename type="first">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<idno type="DOI">10.1145/882262.882311</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="587" to="594" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SCAPE: Shape completion and animation of people</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1145/1073204.1073207</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="408" to="416" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Body cues, not facial expressions, discriminate between intense positive and negative emotions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1224313</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="1225" to="1229" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Benzécri</surname></persName>
		</author>
		<title level="m">L&apos;analyse des données</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>Dunod</publisher>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
	<note>Data analysis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition in poor-quality video: Evidence from security surveillance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="243" to="248" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Is obesity stigmatizing? Body weight, perceived discrimination, and psychological wellbeing in the United States</title>
		<author>
			<persName><forename type="first">D</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.1177/002214650504600303</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Health and Social Behavior</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="244" to="259" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The relative importance of the face and body in judgments of human physical attractiveness</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Currie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Little</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.evolhumbehav.2009.06.005</idno>
	</analytic>
	<monogr>
		<title level="j">Evolution &amp; Human Behavior</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="409" to="416" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The perception of emotion in body expressions</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Gelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>De Borst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Watson</surname></persName>
		</author>
		<idno type="DOI">10.1002/wcs.1335</idno>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="149" to="158" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-conscious recognition of emotional body language</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Gelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hadjikhani</surname></persName>
		</author>
		<idno type="DOI">10.1097/00001756-200604240-00006</idno>
	</analytic>
	<monogr>
		<title level="j">NeuroReport</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="583" to="586" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A cortical area selective for visual processing of the human body</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Downing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanwisher</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1063414</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="2470" to="2473" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206772</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2009 IEEE International Conference on Computer Vision and Pattern Recognition<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Correspondence analysis in practice</title>
		<author>
			<persName><forename type="first">M</forename><surname>Greenacre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The functional architecture of the ventral temporal cortex and its role in categorization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grill-Spector</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Weiner</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn3747</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="536" to="548" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dissecting the time course of person recognition in natural viewing environments</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<idno type="DOI">10.1111/bjop.12125</idno>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="117" to="134" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coregistration: Simultaneous alignment and modeling of articulated 3D shape</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hirshberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rachlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33783-3_18</idno>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="242" to="255" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Describable visual attributes for face verification and image search</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2011</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1962" to="1977" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE: Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What, where and who? Classifying events by scene and object recognition</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2007.4408872</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE International Conference on Computer Vision</title>
		<meeting>the 2007 IEEE International Conference on Computer Vision<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A video database of moving faces and people</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2005</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="812" to="816" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Selectivity for the human body in the fusiform gyrus</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Peelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Downing</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.00513.2004</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="603" to="608" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Does attractiveness in men provide clues to semen quality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Simmons</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1420-9101.2007.01477.x</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Evolutionary Biology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="572" to="579" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Can I recognize my body&apos;s weight? The influence of shape and texture on the perception of self</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Piryankova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Stefanucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De La Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Mohler</surname></persName>
		</author>
		<idno type="DOI">10.1145/2641568</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The stigma of obesity: A review and update</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Puhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Heuer</surname></persName>
		</author>
		<idno type="DOI">10.1038/oby.2008.636</idno>
	</analytic>
	<monogr>
		<title level="j">Obesity</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="941" to="964" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Soft biometrics: Human identification using comparative descriptions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Stevenage</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2013.219</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1216" to="1228" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unaware person recognition from the body when face identification fails</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Natu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797613492986</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2235" to="2243" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The role of the face and body in unfamiliar person identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>O'toole</surname></persName>
		</author>
		<idno type="DOI">10.1002/acp.2969</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="761" to="768" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The effects of inversion and familiarity on face versus body cues to person recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coltheart</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0028584</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1098" to="1104" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Separate face and body selectivity on the fusiform gyrus</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Schwarzlose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanwisher</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2621-05.2005</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="11055" to="11059" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Body talk: Crowdshaping realistic 3D avatars with words</title>
		<author>
			<persName><forename type="first">S</forename><surname>Streuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Quiros-Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.1145/2897824.2925981</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">54</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deformation transfer for triangle meshes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1145/1015706.1015736</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="399" to="405" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic shape editing using deformation handles</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Kara</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766908</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic human activity recognition: A literature review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ziaeefar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2015.03.006</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2329" to="2345" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
